% Ideas
% - Define 
% - univariate, multivariate rules
% - rule checking, summarizing, plotting
% - soft rules and outlier detection
% - deterministic corrections with deducorrect
% - 
%

\section{From technically correct data to consistent data}
\label{sect:inputtomicro}

\emph{Consistent data} is \emph{technically correct data} that is fit for 
statistical analysis. This means that it should not contain obvious errors, 
or have special values such as infinity or missing that obstruct statistical analysis.

The checking for  errors is based on the knowledge you
have on subject matter. For example a persons age cannot be negative. If the 
data contains a negative age it will corrupt your age statistics.
This subject matter knowledge is usually formalized in terms of rules which may 
be interpreted as being obligatory (\emph{hard rules}), or indicative 
(\emph{soft rules}).

Observations (i.e. rows of a \code{data.frame}) that violate hard rules, have 
invalid values or are incomplete can be left out of your analysis. 
This may seem to be the right procedure, but can introduce a bias or can leave 
you with very few data to be analyzed. 

In many cases an invalid or incomplete observation can be corrected and imputed,
based on background information or using the data of the other observations.

The detection, correction and imputation of data are all operations that 
influence the statistical analysis (positively).
They should be seen as statistical methods.
\R{} makes it easy to be explicit on the detection rules, and the corrections on
 the data. This makes your data cleaning process transparent and reproducible.

We will first describe detection techniques that you will need to find records 
and values that are erroneous. After that we go into deterministic correction 
techniques that can repair obvious errors based on formalized or heuristic
knowledge. We end with an overview of imputation techniques, that can generate
plausible values for partially missing data.

\subsection{Detection}

\subsubsection{Detecting extraordinary values}

\R{} contains several function to deal with special values. We encountered
a number of them in section \ref{sec:}.
 
\begin{tip}{Tip}
Note that in sometimes a special value in your data may encode an 
actual value. For example for some columns ``empty''(\code{NA}) means \code{0}.
Always read the meta data of the data!
\end{tip}

Many times data will contain empty values. Rows (cases) containing \code{NA} can be
detected and removed easily in \R{}, with the functions \code{complete.cases} 
which returns a \code{logical} with rows having no \code{NA} values.
The logical can be used to select a \code{data.frame}, but for convenience \R{}
contains the \code{na.omit} function, which does exactly that.
<<>>=
person 
complete.cases(person)
na.omit(person)
@
The result of the \code{na.omit} function is a data.frame with the incomplete rows
removed, but also contains the names of the removed rows in the attribute 
\code{na.action}.

Numeric columns can have more formalized special values: \code{Inf}, \code{-Inf},
\code{NA} and \code{NaN}. Checking for these values can be done with \code{is.finite}.
For example:
<<>>=
is.finite(c(1, Inf, NaN, NA))
@
\code{is.finite} works on a vector. We can write a utility function \code{is.special}
that checks a column for the (non)occurrence of special values

<<>>=
is.special <- function(x){
  if (is.numeric(x)) is.finite(x) else is.na(x)
}
person
sapply(person, is.special)
@

The result of \code{special} show which data have special values. Note that 
detecting them is good, but the data should be fixed.

\subsubsection{Outliers?}
\code{extremevalues}????

\subsection{Checking record-wise rules with the \code{editrules} package.} 
With \R{} you can write scripts that do error detection, by writing code that 
checks each hard rule. However if you have many rules this soon gives many problems
Rules may interact, so the rules should be not seen as separate rules, but as a
set of connected rules. Furthermore your \R{} code quickly obscures the rules you 
try to implement. It would be easier and more clear to define a rule set that all
records must obey, and to check if a record obeys the rules.

The \code{editrules} package allows one to define rules on categorical,
numerical or mixed data sets which each record must obey. Furthermore it can 
use these rules to check data for errors and find erroneous values. It also allows
for manipulating the rule set, for example checking if the set of rules is 
consistent or not. We will briefly discuss the features of \code{editrules}, it
has many options and functions, but are not needed in this introduction.  

The rules you specify in \code{editrules} are valid \R{} syntax. They can 
be specified in a \code{character} vector directly with \code{editset} or stored 
separately in a text file and read in with \code{editfile}. 
The last option makes it a lot easier to maintain a large set of rules.

<<echo=FALSE>>=
library(editrules)
library(xtable)
person <- read.csv("files/person.txt")
@

<<echo=FALSE,results="asis">>=
xtable(person, caption="Person data, with lots of errors")
@

Lets first start with a small example with restrictions on age.
<<>>=
(E <- editset(c("age >=0", "age <= 150")))
@
The data can be checked with \code{violatedEdits}.
<<>>=
violatedEdits(E, person)
@
\code{violatedEdits} shows for each row of the data, which rule (edit) is violated.

An example of a more complex set of rules that can
be defined by typing 

<<cache=FALSE,echo=FALSE>>=
E <- editfile("edits.txt")
read_chunk('edits.txt')
@
<<rules, eval=FALSE>>=
@

Note that with a more complex rule set restrictions on variables are connected.
They form a web of interconnected restrictions.
<<width=7>>=
plot(E)
@
Because the restrictions on the variables. the result of \code{violatedEdits} shows
which rows are invalid according to what rules. Note that if a multivariate rule 
is violated, it is not clear which variable is erroneous.
<<width=7>>=
(ve <- violatedEdits(E, person))
@
It is also possible to do some statistics on the violated edits. For large 
dataset this can be very revealing: it may show problems in the collection 
of the data. It may also show that some rules are never violated.
The result of violatedEdits can be plotted.

<<>>=
plot(ve)
@

\code{editrules} makes it easy to specify rules that data must obey. It allows 
for easy checking if observations/cases/rows contain valid data or not.

\subsubsection{Error localization}
When using \code{is.finite}, \code{special} and univariate rules, it is clear
what values are invalid. However in case that there are many restrictions, this 
may not be clear. e.g. if an observation has the variables age, aged, status, 
yearsmarried are all connect.
\code{editrules} can localize values that need to be adjusted. It takes the 
Fellig-Holt principle which says: change the values of a minimal (weighted) 
number of variables so that it complies to the rules specified. The function
in \code{editrules} that does this, is called \code{localizeErrors}. 
It takes a set of rules and a \code{data.frame} and returns the found solutions.
<<>>=
person[2,]
le <- localizeErrors(E, person[2,])
le$status
le$adapt
@

\code{\$adapt} is a \code{logical} that signifies which variables are considered
erroneous. It can be used in the correction and imputation procedures for filling in valid
values. \code{localizeErrors} can be tuned with various options. It is possible
to supply confidence weight for variables allowing for fine grained control on
which values should be adapted. It is also possible to use a mixed integer programming
solver, which is typically very fast. For more information see the reference manual
of \code{editrules}.

\subsection{Correction}
\subsubsection{Deterministic and deductive methods}
In some cases, the cause of errors in data can be determined with enough
certainty so that the solution is almost automatically known. In recent years,
several such methods have been developed and implemented in the
\code{deducorrect} package\cite{loo:2011}. In this section we give a quick
overview of the possibilities of \code{deducorrect}. 

To follow the examples load the deducorrect package.
<<result=FALSE>>=
library(deducorrect)
@


\subsubsection{Automated correction using deterministic rules}
Data cleaning often involves applying a lot of \emph{ad-hoc} transformations
and derivations of new variables. This may lead to large scripts, where
you selects parts of the data, change some variables, select another part,
change some more variables, etc. When such scripts are neatly written and
commented, they can almost be treated as a log of the actions performed
by the analyst. However, as scripts get longer it is nicer to 
store the transformation rules separately and log which rule is executed
on what record. The \code{deducorrect} package offers functionality for 
this. Consider as an example the following (fantasized) dataset listing
the body length of some brothers.
<<>>=
(marx <- read.csv("files/marx.csv",stringsAsFactors=FALSE))
@
The task here is to standardize the lengths and express all of them in
meters. The obvious way would be to use the indexing techniques summarized
in Section \ref{sect:indexing}, which would look something like this.
<<>>=
marx_m <- marx
I <- marx$unit == "cm"
marx_m[I,'height'] <- marx$height[I]/100
I <- marx$unit == "inch"
marx_m[I,'inch'] <- marx$height[I]/39.37
I <- marx$unit == "ft"
marx_m[I,'ft'] <- marx$height[I]/3.28
marx_m$unit <- "m"
@
Such operations quickly become cumbersome. Of course, in this case one could
write a for-loop but that would hardly save any code. Moreover, if you want to
check afterwards which values have been converted and for what reason, there
will be a significant administrative overhead.

The deducorrect package takes all this overhead of your hands with the 
\code{correctionRules} functionality. For example, to perform the above 
task, one first specifies a file with simple correction rules as follows.
<<echo=FALSE>>=
read_chunk("files/conversions.txt")
@
<<conversions,eval=FALSE,tidy=FALSE>>=
@
With \code{deducorrect} we can read these rules, apply them to the data and
obtain a log of all actual changes as follows.
<<>>=
# read the conversion rules.
R <- correctionRules("files/conversions.txt")
R
@
\code{correctionRules} has parsed the rules and stored them in a \code{correctionRules}
object. We may now apply them to our data
<<>>=
cor <- correctWithRules(R,marx)
@
as simple as that. The returned value, \code{cor}, is a \code{list} containing the
corrected data
<<>>=
cor$corrected
@
but also a log of applied corrections.
<<>>=
cor$corrections[1:4]
@
The log lists for each row, what variable was changed, what the old value was
and what the new value is. Furthermore, the fifth column of
\code{cor\$corrections} shows the corrections that were applied (not shown
above for formatting reasons)
<<>>=
cor$corrections[5]
@
So here, with just two commands, the data is processed and all actions logged
in a \code{data.frame} which you can store or analyze. The rules that may be
applied with \code{deducorrect} are rules that can be executed
record-by-record. 

By design, there are some limitations to which rules can be applied with \code{correctWithRules}.
The processing rules should be executable record-by-record. That is, it is not permitted
to use functions line \code{mean} or \code{sd}. The symbols that may be used can be listed
as follows.
<<>>=
getOption('allowedSymbols')
@
When the rules are read by \code{correctionRules}, it checks whether any symbol
occurs that is not in the list of allowed symbols and returns an error message
when such a symbol is found. For example.
<<>>=
correctionRules(expression(x <- mean(x)))
@
%
Finally, it is currently not possible to add new variables using correctionrules
although such a feature will likely be added in the future.


\subsubsection{Correction of cases where the cause of error can be deduced}
When the data you are analyzing is generated by people rather than machines or
measurement devices, certain typical human-generated errors are likely to
occur. Given that data has to obey certain edit rules, the occurrence of such
errors can sometimes be detected from raw data with (almost) certainty.
Examples of errors that can be detected are typing errors in numbers (under linear
restrictions) rounding errors in numbers and sign errors or variable swaps\cite{scholtus:2011}.
The \code{deducorrect} package has a number of functions available that can correct
such errors. Below we give some examples, every time with just a single edit rule.
The functions can handle larger sets of edits however.

With \code{correctRoundings} deviations of the size of one or two measurement
units can be repaired. The function chooses randomly one variable to alter such that
the rule violation(s) are nullified while no new violations are generated.
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 100,y=101,z=200)
cor <- correctRounding(e,d)
cor$corrected
cor$corrections
@

The function \code{correctSigns} is able to detect and repair sign errors. 
It does this by trying combinations of variable swaps on variables that
occur in violated edits.
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 100, y = -100, z=200)
cor <- correctSigns(e,d)
cor$corrected
cor$corrections
@

Finally, the function \code{correctTypos} is capable of detecting and correcting
typographic errors in numbers. It does this by computing candidate solutions and
checking whether those candidates are less than a certain string distance (see Section 
\ref{sect:appstringmatching}) removed from the original.
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 123, y = 132, z = 246)
cor <- correctTypos(e,d)
cor$corrected
cor$corrections
@
Indeed, swapping the $3$ and the $2$ in $y=132$ solves the edit violation.


\begin{tip}{Tip}
Every \code{correct-} function in \code{deducorrect} is an object of class
\code{deducorrect}. When printed, it doesn't show the whole contents (the
corrected data and logging information) but a summary of what happened with
your data. A \code{deducorrect} object also contains data on timing, user,
and so on. See \code{?"deducorrect-object"} for a full explanation.
\end{tip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Deterministic imputation methods}

Under some circumstances, the rules that your data have to obey are
so restrictive that there's just a single solution for a missing value.
As an example, consider a record with variables listing the costs
for \emph{staff} \emph{cleaning}, \emph{housing} and the total \emph{total}.
We have the following rules.
\begin{equation}
\begin{array}{l}
  \textrm{\em staff} + \textrm{\em cleaning} + \textrm{\em housing} = \textrm{\em total}\\
  \textrm{\em staff} \geq 0\\
  \textrm{\em housing} \geq 0\\
  \textrm{\em cleaning} \geq 0
\end{array}
\end{equation}
In general, if one of the variables is missing the value can clearly be derived
by solving it for the first rule (providing that the solution doesn't violate
the last rule). However, there are other cases where unique solutions exist.
Suppose that we have $staff = total$. Assuming that these values are correct,
the only possible values for the other two variables  is $housing=cleaning=0$.
The \code{deducorrect} function \code{deduImpute} is able to recognize such
cases and compute the unique imputations.
<<tidy=FALSE>>=
E <- editmatrix(expression(
  staff + cleaning + housing == total,
  staff    >= 0,
  housing  >= 0,
  cleaning >= 0
))
dat <- data.frame(
  staff = c(100,100,100),
  housing = c(NA,50,NA),
  cleaning = c(NA,NA,NA),
  total = c(100,180,NA)
)
dat
cor <- deduImpute(E,dat)
cor$corrected
@
Note that \code{deduImpute} only imputes those values that can be derived
with absolute certainty (uniquely) from the rules. In the example, there are
many possible solutions fo impute the last record and hence \code{deduImpute}
leaves it untouched.

Similar situations exist for categorical data, and purely categorical data
is handled by \code{deduImpute} as well.
<<tidy=FALSE>>=
E <- editarray(expression(
  age %in% c("adult","under-aged"),
  driverslicense %in% c(TRUE, FALSE),
  if ( age == "under-aged" ) !driverslicense
))
dat <- data.frame(
  age = NA,
  driverslicense = TRUE
)
dat
cor <- deduImpute(E,dat)
cor$corrected
@
Here, \code{deduImpute} uses automated logic to derive from the conditional
rules that if someone has a drivers license, he has to be an adult.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model-based imputation}
\label{sect:mbimputation}
When deterministic or deductive corrections and imputations are not sufficient
to replace missing values, one may either correct for the missing data using
advances weight models or by estimating the actual missing values using
statistical models. There is a vast body of literature on imputation methods
and is goes beyond the scope of this tutorial to discuss them all. In stead, we
present in Table \ref{tab:imputation} an overview of packages that offer some
kind of imputation method and list them against a number of popular model-based
imputation methods. We note that the list of packages and methods are somewhat
arbitrary as they result from an investigation conducted at Statistics
Netherlands \cite{broek:2012} for internal purposes. Nevertheless we feel that
this overview is a quite useful place to start. See for example the paper by
Kalton and Kasprzyk\cite{kalton:1986} for an overview of established imputation
methods.  The packages \code{Amelia}, \code{deducorrect} and \code{mix} do not
implement any of the methods mentioned in the table. That is because
\code{Amelia} implements a multiple imputation method based on the assumption
of a conditional multinormal variable distribution. The \code{deducorrect}
package implements deductive and deterministic method, and we choose not to
call this model-based in this tutorial. The \code{mix} package implements a
Bayesian estimation method based on an Markov Chain Monte Carlo algorithm.

There is no one single best imputation method that works in all cases. The
imputation model of choice depends on what auxiliary information is available
and whether there are (multivariate) edit restrictions on the data to be
imputed. The availability of \R{} software for imputation under edit
restrictions is, to our best knowledge, limited. However, a viable strategy for
imputing numerical data is to first impute missing values without restrictions,
and then minimally adjust the imputed values so that the restrictions are
obeyed.  Separately, these methods are available in \R{}.

\begin{table}[!t]
\begin{adjustwidth}{-2cm}{}
\begin{threeparttable}
\caption{An overview of imputation functionality offered by some  \R{} packages.
reg: regression, rand: random, seq: sequential, NN: nearest neighbor, pmm:
predictive mean matching, kNN: $k$-nearest-neighbours, int: interpolation,
lo/no last observation carried forward / next observation carried backward,
LS: method of Little and Su.
}
\label{tab:imputation}
\begin{tabular}{|l|ccc|ccc|c|ccc|}
\hline
\multicolumn{1}{|l}{} & 
\multicolumn{3}{|c|}{Numeric} &
\multicolumn{3}{|c|}{Hot deck}&
\multicolumn{1}{|c|}{}&
\multicolumn{3}{|c|}{Longitudinal}\\

Package                             & 
\multicolumn{1}{|c}{mean}  & 
\multicolumn{1}{c}{ratio}  & 
\multicolumn{1}{c|}{reg.}   & 
\multicolumn{1}{c}{rand}  & 
\multicolumn{1}{c}{seq}   & 
\multicolumn{1}{c|}{pmm}   & 
\multicolumn{1}{c}{kNN}    & 
\multicolumn{1}{|c}{int}   & 
\multicolumn{1}{c}{lo/no}  & 
\multicolumn{1}{c|}{LS}     \\ \hline
\code{Amelia}\cite{honaker:2011}       &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{BaBoon}\cite{meinfelder:2011}    &\xmark & \xmark &\xmark           &\xmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{cat}\cite{harding:2012}          &\xmark & \xmark &\xmark           &\cmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\  
\code{deducorrect}\cite{loo:2011}      &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ \hline
\code{e1071}\cite{meyer:2012}          &\cmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{ForImp}                          &\cmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\cmark$^\ddagger$ &\xmark &\xmark & \xmark \\ 
\code{Hmisc}\cite{harrel:2013}         &\cmark & \xmark &\xmark           &\cmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{imputation}\cite{wong:2013}      &\cmark & \xmark &\cmark$^\dagger$ &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ \hline
\code{impute}\cite{hastie}             &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{mi}\cite{su:2011}                &\xmark & \xmark &\cmark$^*$       &\cmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{mice}\cite{buren:2011}           &\cmark & \xmark &\cmark$^*$       &\cmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{mix}\cite{schafer:2010}          &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ \hline
\code{norm}\cite{alvaro:2013}          &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{robCompositions}\cite{templ:2011}&\xmark & \xmark &\cmark$^*$       &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{rrcovNA}\cite{todorov:2012}      &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{StatMatch}\cite{orazio:2012}     &\xmark & \xmark &\xmark           &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ \hline
\code{VIM}\cite{templ:2012}            &\xmark & \xmark &\cmark$^*$       &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{yaImpute}\cite{crookston:2007}   &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{zoo}\cite{zeileis:2005}          &\cmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\cmark &\cmark & \xmark \\ 
\hline                                                                                                             
\end{tabular}
{\scriptsize ${^*}$Methods are ultimately based on some form of regression, but are more involved than simple linear regression.\\
${^\dagger}$Uses a non-informative auxiliary variable (row number).\\
${^\ddagger}$Uses nearest neighbor as part of a more involved imputation scheme.
}
\end{threeparttable}
\end{adjustwidth}
\end{table}
%
In the following subsections we discuss three types of imputation
models and give some pointers to how to implement them using \R{}.
The next section (\ref{sect:rspa}) is devoted to value adjustment.

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Basic numeric imputation models}
\label{sect:numericimputation}
Here, we distinguish three imputation models. The first is imputation of the mean:
\begin{equation}
\hat{x}_i = \bar{x},
\end{equation}
where the mean is taken over the observed values. The usability of this model
is limited since it obviously causes a bias in measures of spread, estimated
from the sample after imputation. However, in base \R{} it is implemented
simply enough.
<<eval=FALSE>>=
x[is.na(x)] <- mean(x,na.rm=TRUE)
@
In principle one can use other measures of centrality, and in fact, the \code{Hmisc}
package has a convenient wrapper function allowing you to specify what function is
used to compute imputed values from the non-missing. For example, imputation of 
the mean or median can be done as follows. 
<<eval=FALSE>>=
library(Hmisc)
x <- impute(x, fun=mean)     # mean imputation
x <- impute(x, fun=median)   # median imputation
@
An nice feature of the \code{impute} function is that the resulting
vector ``remembers'' what values were imputed. This information
may be requested with \code{is.imputed} as in the example below.
<<echo=FALSE,message=FALSE>>=
library(Hmisc)
@
<<>>=
x <- 1:5   # create a vector...
x[2] <- NA # ...with an empty value
x <- impute(x,mean)
x
is.imputed(x)
@
Note also that imputed values are printed with a post-fixed asterix.

The second imputation model we discuss is ratio imputation. Here, the
imputation estimate $\hat{x}_i$ is given by 
\begin{equation}
\hat{x}_i = \hat{R} y_i,
\end{equation}
Where $y_i$ is a covariate and $\hat{R}$ is an estimate of the average ratio
between $x$ and $y$. Often this will be given by the sum of observed $x$ values
divided by the sum of corresponding $y$ values although variants are possible.
Ratio imputation has the property that the estimated value equals $\hat{x}=0$
when $y=0$, which is in general not guaranteed in linear regression. Ratio
imputation may be a good model to use when restrictions like $x\geq 0$ and/or
$y\geq0$ apply. There is no package directly implementing ratio imputation,
unless it is regarded a special case of regression imputation. It is easily
implemented using plain \R{} though. Below, we suppose that \code{x} and \code{y}
are numeric vectors of equal length, \code{x} contains missing values and \code{y}
is complete.
<<eval=FALSE>>=
I <- is.na(x)
R <- sum(x[!I])/sum(y[!I])
x[I] <- R*y[I] 
@
Unfortunately, it is not possible to simply wrap the above in a function and
pass this to \code{HMisc}'s \code{impute} function. There seem to be no packages
that implement ratio imputation in a single function.


The third, and last numerical model we treat are (generalized) linear
regression models. In such models, missing values are imputed
using as follows
\begin{equation}
\hat{x}_i = \hat{\beta}_0 + \hat{\beta}_1y_{1,i} + \cdots + \hat{\beta}_ky_{k,i},
\end{equation}
where the $\hat{\beta}_0,\hat{\beta}_1\ldots\hat{\beta}_k$ are estimated linear
regression coefficients for each of the auxiliary variables
$y_1,y_2\ldots,y_k$. Estimating linear models is easy enough using \code{lm}
and \code{predict} in standard \R{}. Here, we use the built-in \code{iris}
data set as an example.
<<>>=
data(iris)
iris$Sepal.Length[1:10] <- NA
model <- lm(Sepal.Length ~ Sepal.Width + Petal.Width,data=iris)
I <- is.na(iris$Sepal.Length)
iris$Sepal.Length[I] <- predict(model, newdata=iris[I,])
@
Above, \code{lm} automatically omits rows with empty values and estimates the
model based on the remaining records. Next, we used \code{predict} to estimate
the missing values. 

It should be noted that although \code{Hmisc}, \code{VIM}, \code{mi} and
\code{mice} all implement imputation methods that ultimately use some form of
regression, they do not include a simple interface for the case described
above.  The more advanced methods in those packages are aimed to be more
precise and/or robust against outliers than standard (generalized) linear
regression.  Moreover, both \code{mice} and \code{mi} implement methods for
multiple imputation, which allows for estimation of the imputation variance.
Such methods are beyond the scope of the current text. The \code{imputation}
package has a function \code{lmImpute} but is uses a the row number as
auxiliary variable which limits the practical usability.


\subsubsection{Hot deck imputation}
\label{sect:hotdeck}
In hot deck imputation, missing values are imputed by copying values from
similar records in the same dataset. Or, in notation:
\begin{equation}
\hat{x}_i = x_j, 
\end{equation}
where $x_j$ is taken from the observed values.  Hot-deck imputation can be
applied to numerical as well as categorical data but is only viable when enough
donor records are available. 

The main question in hot-deck imputation is
how to choose the replacement value $x_j$ from the observed values.  Here, we
shortly discuss four well-known flavors.

In \emph{random} hot-deck imputation, a value is chosen randomly, and uniformly
from the same data set. When meaningful, random hot deck methods can be
applied per stratum. For example, one may apply random hot-deck imputation of
body height for male and female respondents separately. Random hot-deck on a
single vector can be applied with the \code{impute} function of the
\code{Hmisc} package.
<<>>=
data(women)
# add some missingess
height <- women$height
height[c(6,9)] <- NA
height
(height <- impute(height,"random"))
@
Note that the outcome of this imputation is very likely tot be different each
time it is executed. If you want the results to be random, but repeatable (e.g.
for testing purposes) you can use \code{set.seed(<a number>)} prior to calling
\code{impute}. 

In \emph{sequential} hot deck imputation, the vector containing missing values is
sorted according to one or more auxiliary variables so that records that have
similar auxiliaries occur sequentially in the \code{data.frame}.  Next, each
missing value is imputed with the value from the first following record that
has an observed value. There seems to be no package that implements the
sequential hot deck technique. However, given a vector that is sorted, the
following function offers some basic functionality.
<<tidy=FALSE>>=
# x    :  vector to be imputed
# last : value to use if last value of x is empty
seqImpute <- function(x,last){
  n <- length(x)
  x <- c(x,last)
  i <- is.na(x)
  while(any(i)){
    x[i] <- x[which(i) + 1]
    i <- is.na(x)
  }
  x[1:n]
}
@
We will use this function in exercise \ref{ex:seqimpute}. 


\emph{Predictive mean matching} (pmm) is a form of nearest-neighbor hot-deck
imputation with a specific distance function. In predictive mean matching one
first estimates values for empty fields, for example with a regression method.
Empty values are then imputed with observed values which are the most similar
to the predicted values. Predictive mean matching imputation has been
implemented in several packages (\code{mi}, \code{mice}, \code{BaBoon}) but
each time either for a specific type of data and/or using a specific prediction
model (\emph{e.g.} \code{Bayesglm}). Moreover, the packages known to us always
use pmm as a part of a more elaborate multiple imputation scheme, which we
deemed out of scope for this tutorial.

\subsubsection{kNN-imputation}
In $k$ \emph{nearest neighbor} imputation one defines a distance
function $d(i,j)$ that computes a measure of dissimilarity between records. A
missing value is then imputed by finding first the $k$ records nearest to the
record with one or more missing values. Next, a value is chosen from or
computed out of the $k$ nearest neighbors. In the case where a value is picked
from the $k$ nearest neighbors, kNN-imputation is a form of hot-deck imputation.

The \code{VIM} package contains a function called \code{kNN} that uses Gowers
distance\cite{gower:1971} to determine the $k$ nearest neighbors. Gower's
distance between two records labeled $i$ and $j$ is defined as
\begin{equation}
d_{\sf g}(i,j) \frac{\sum_k w_{ijk} d_k(i,j)}{\sum_{k}w_{ijk}},
\end{equation}
where the sum runs over all variables in the record and $d_k(i,j)$
is the distance between the value of variable $k$ in record $i$ and record $j$.
For categorical variables, $d_k(i,j) = 0$ when the value for the $k'th$ variable
is the same in record $i$ and $j$ and $1$ otherwise. For numerical variables
the distance is given by $1-(x_i-x_j)/(\max(x)-\min(x))$. The weight $w_{ijk}=0$
when the $k$'th variable is missing in record $i$ or record $j$ and otherwise
$1$.

Here is an example of \code{kNN}.
<<message=FALSE>>=
library(VIM)
data(iris)
n <- nrow(iris)
# provide some empty values (10 in each row, randomly)
for ( i in 1:ncol(iris) ){
  iris[sample(1:n,10,replace=FALSE),i] <- NA
}
iris2 <- kNN(iris)
@
The \code{kNN} function determines the $k$ (default: 5) nearest neighbors of a
record with missing values. For numerical variables the median of the $k$
nearest neighbors is used as imputation value, for categorical variables the
category that most often occurs in the $k$ nearest neighbors is used. Both the
these functions may be replaced by the user. Finally, \code{kNN} prints (negatively)
the time it took to complete the imputation. We encourage the reader to execute
the above code and experiment further. 



\subsection{Minimal value adjustment}
\label{sect:rspa}
Once missing numerical values have been adequately imputed, there is a good
chance that the resulting records violate a number of edit rules.  The obvious
reason is that there are not many methods that can perform imputation under
arbitrary edit restrictions. One viable option is therefore to minimally
adjust imputed values such that after adjustment the record passes every edit
check within a certain tolerance. 

We need to specify more clearly what \emph{minimal} adjustment means here.
The \code{rspa} package\cite{loo:2012} is able to take a numerical record
$\bs{x}^0$ and replace it with a record $\bs{x}$, such that
the weighted Euclidean distance
\begin{equation}
\sum_iw_i(x_i-x_i^{0})^2,
\end{equation}
is minimized and $\bs{x}$ obeys a given set of (in)equality restrictions
\begin{equation}
\bs{Ax} \leq \bs{b}.
\end{equation}

As practical example consider the following script.
<<>>=
library(editrules)
library(rspa)
E <- editmatrix(expression(
  x + y == z,
  x >= 0,
  y >= 0
))
d <- data.frame(x = 10, y=10, z=21)
d1 <- adjustRecords(E,d)
d1$adjusted
@
The function \code{adjustRecords} adjusts every record minimally to obey the restrictions
in \code{E}. 
Indeed, we may check that the adjusted data now obeys every rule within \code{adjustRecords}'
default tolerance of $0.01$.
<<>>=
violatedEdits(E,d1$adjusted,tol=0.01)
@
By default, all variables in the input \code{data.frame} are adjusted with
equal amounts. However, suppose that $z$ is an imputed value while $x$ and $y$
are observed, original values. We can tell \code{adjustRecords} to only adjust
\code{z} as follows.
<<>>=
A <- array(c(x=FALSE,y=FALSE,z=TRUE),dim=c(1,3))
A
d2 <- adjustRecords(E,d, adjust=A)
d2$adjusted
@



\subsection*{Exercises}
\begin{exercise}
\label{ex:seqimpute}

\end{exercise}


