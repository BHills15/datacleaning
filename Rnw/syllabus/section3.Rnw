% Ideas
% - Define 
% - univariate, multivariate rules
% - rule checking, summarizing, plotting
% - soft rules and outlier detection
% - deterministic corrections with deducorrect
% - 
%

\section{From technically correct data to consistent data}
\label{sect:inputtomicro}
With \emph{consistent data} we mean that the data obeys all expectations you
have based on subject matter knowledge. Such expectations are usually expressed
in terms of rules wich may be interpreted as being obligatory (\emph{hard
rules}), or indicative (\emph{soft rules}). 





\subsection{Checking record-wise rules with the \code{editrules} package.}
The \code{editrules} package allows one to define rules on categorical,
numerical or mixed data sets which each record must obey. Restrictions can
be defined by typing them in a file and reading them into \R{} with
the \code{editfile} function.

<<cache=FALSE,echo=FALSE>>=
read_chunk('edits.txt')
@
<<rules,eval=FALSE>>=
@

\subsection{Resolving errors with \code{deducorrect}}

\subsubsection{Automated correction using deterministic rules}
Data cleaning often involves applying a lot of \emph{ad-hoc} transformations
and derivations of new variables. This may lead to large scripts, where
you selects parts of the data, change some variables, select another part,
change some more variables, etc. When such scripts are neatly written and
commented, they can almost be treated as a log of the actions performed
by the analist. However, as scripts get longer it is nicer to 
store the transformation rules separately and log which rule is executed
on what record. The \code{deducorrect} package offers functionality for 
this. Consider as an example the following (fantasized) dataset listing
the body lengtgh of some brothers.
<<>>=
(marx <- read.csv("files/marx.csv",stringsAsFactors=FALSE))
@
The task here is to standardize the lengths and express all of them in
meters. The obvious way would be to use the indexing techniques summarized
in Section \ref{sect:indexing}, which would look something like this.
<<>>=
marx_m <- marx
I <- marx$unit == "cm"
marx_m[I,'height'] <- marx$height[I]/100
I <- marx$unit == "inch"
marx_m[I,'inch'] <- marx$height[I]/39.37
I <- marx$unit == "ft"
marx_m[I,'ft'] <- marx$height[I]/3.28
marx_m$unit <- "m"
@
Such operations quickly become cumbersome. Of course, in this case one could
write a for-loop but that would hardly save any code. Moreover, if you want to
check afterwards which values have been converted and for what reason, there
will be a significant administrative overhead.

The deducorrect package takes all this overhead of your hands with the 
\code{correctionRules} functionality. For example, to perform the above 
task, one first specifies a file with simple correction rules as follows.
<<echo=FALSE>>=
read_chunk("files/conversions.txt")
@
<<conversions,eval=FALSE,tidy=FALSE>>=
@
With \code{deducorrect} we can read these rules, apply them to the data and
obtain a log of all actual changes as follows.
<<>>=
# read the conversion rules.
R <- correctionRules("files/conversions.txt")
R
@
\code{correctionRules} has parsed the rules and stored them in a \code{correctionRules}
object. We may now apply them to our data
<<>>=
cor <- correctWithRules(R,marx)
@
as simple as that. The returned value, \code{cor}, is a \code{list} containing the
corrected data
<<>>=
cor$corrected
@
but also a log of applied corrections.
<<>>=
cor$corrections[1:4]
@
The log lists for each row, what variable was changed, what the old value was
and what the new value is. Furthermore, the fifth column of
\code{cor\$corrections} shows the corrections that were applied (not shown
above for formatting reasons)
<<>>=
cor$corrections[5]
@
So here, with just two commands, the data is processed and all actions logged
in a \code{data.frame} which you can store or analyze. The rules that may be
applied with \code{deducorrect} are rules that can be executed
record-by-record. At the moment variables can be transformed but not added or
removed with the rules applied by \code{correctionRules}, but this will
probably change in the future.



\subsubsection{Correction of cases where the cause of error can be deduced}
When the data you are analyzing is generated by people rather than machines or
measurement devices, certain typical human-generated errors are likely to
occur. For example, typing errors, swapping variables accross fields or sign
errors in numerical variables are 



\subsection{Imputation of missing data}

\subsubsection{Deriving imputation values with certainty}
Under some circumstances, the rules that your data have to obey are
so restrictive that there's just a single solution for a missing value.
As an example, consider a record with variables listing the costs
for \emph{staff} \emph{cleaning}, \emph{housing} and the total \emph{total}.
We have the following rules.
\begin{equation}
\begin{array}{l}
  \textrm{\em staff} + \textrm{\em cleaning} + \textrm{\em housing} = \textrm{\em total}\\
  \textrm{\em staff} \geq 0\\
  \textrm{\em housing} \geq 0\\
  \textrm{\em cleaning} \geq 0
\end{array}
\end{equation}
In general, if one of the variables is missing the value can clearly be derived
by solving it for the first rule (providing that the solution doesn't violate
the last rule). However, there are other cases where unique solutions exist.
Suppose that we have $staff = total$. Assuming that these values are correct,
the only possible values for the other two variables  is $housing=cleaning=0$.
The \code{deducorrect} function \code{deduImpute} is able to recognize such
cases and compute the unique imputations.
<<tidy=FALSE>>=
E <- editmatrix(expression(
  staff + cleaning + housing == total,
  staff    >= 0,
  housing  >= 0,
  cleaning >= 0
))
dat <- data.frame(
  staff = c(100,100,100),
  housing = c(NA,50,NA),
  cleaning = c(NA,NA,NA),
  total = c(100,180,NA)
)
dat
cor <- deduImpute(E,dat)
cor$corrected
@
Note that \code{deduImpute} only imputes those values that can be derived
with absolute certainty (uniquely) from the rules. In the example, there are
many possible solutions fo impute the last record and hence \code{deduImpute}
leaves it untouched.

Similar sitiations exist for categorical data, and purely categorical data
is handled by \code{deduImpute} as well.
<<tidy=FALSE>>=
E <- editarray(expression(
  age %in% c("adult","under-aged"),
  driverslicense %in% c(TRUE, FALSE),
  if ( age == "under-aged" ) !driverslicense
))
dat <- data.frame(
  age = NA,
  driverslicense = TRUE
)
dat
cor <- deduImpute(E,dat)
cor$corrected
@
Here, \code{deduImpute} uses automated logic to derive from the conditional
rules that if someone has a driverslicense, he has to be an adult.




\subsection{Adjusting values with \code{rspa}}


