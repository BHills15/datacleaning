% Ideas
% - Define 
% - univariate, multivariate rules
% - rule checking, summarizing, plotting
% - soft rules and outlier detection
% - deterministic corrections with deducorrect

\section{From technically correct data to consistent data}
\label{sect:inputtomicro}

\emph{Consistent data} are \emph{technically correct data} that are fit for
statistical analysis. They are data in which missing values, special values,
(obvious) errors and outliers are either removed, corrected or imputed. The
data are consistent with constraints based on real-world knowledge about the
subject that the data describe. 

Consistency can be understood to include \emph{in-record} consistency, meaning
that no contradictory information is stored in a single record, and
\emph{cross-record} consistency, meaning that statistical summaries of
different variables do not conflict with each other. Finally, one can include
cross-dataset consistency, meaning that the dataset that is currently analyzed
is consistent with other datasets pertaining to the same subject matter. In
this tutorial we mainly focus on methods dealing with in-record consistency,
with the exception of outlier handling which can be considered a cross-record
consistency issue.

The process towards consistent data involves always involves the following 
three steps.
\begin{enumerate}
\item \emph{Detection} of an inconsistency. That is, one establishes which
constraints are violated. For example, an \emph{age} variable is constraint to
non negative values.
\item \emph{Selection} of the field or fields causing the inconsistency. This
is trivial in the case of a univariate demand as in the previous step, but may
be more cumbersome when cross-variable relations are expected to hold. For
example the \emph{marital status} of a child must be \code{unmarried}.
In the case of a violation it is not immediately
clear whether \emph{age}, \emph{marital status} or both are wrong.
\item \emph{Correction} of the fields that are deemed erroneous by the
selection method.  This may be done through deterministic (model-based) or
stochastic methods.
\end{enumerate}
%
For many data correction methods these steps are not necessarily neatly
separated. For example, in the deductive correction methods described in
subsection \ref{sect:deductivecorrection} below, the three steps are performed
with a single mathematical operation. Nevertheless, it is useful to be able to
recognize these sub-steps in order to make clear what assumptions are made
during the data cleaning process. 

In the following subsection (\ref{sect:detectandlocalize}) we introduce a
number of techniques dedicated to the detection of errors and the selection of
erroneous fields. If the field selection procedure is performed separately from
the error detection procedure, it is generally referred to as \emph{error
localization}. The latter is described in subsection \ref{sect:errloc}.
Subsection  \ref{sect:correction} describes techniques that implement
correction methods based on `direct rules' or `deductive correction'.  In these
techniques, erroneous values are replaced by better ones by directly deriving
them from other values in the same record. Finally, subsection
\ref{sect:mbimputation} gives an overview of some commonly used imputation
techniques that are available in \R{}.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Detection and localization of errors}
\label{sect:detectandlocalize}
This section details a number of techniques to detect univariate and
multivariate constraint violations. Special attention is paid tot the error
localization problem in subsection \ref{sect:errloc}.

\subsubsection{Missing values}
A missing value, represented by \code{NA} in \R{}, is a placeholder for a datum
of which the type is known but its value isn't. Therefore, it is impossible to
perform statistical analysis on data where one or more values in the data are
missing. One may choose to either omit elements from a dataset that contain
missing values or to impute a value, but missingness is something to be dealt
with prior to any analysis.

In practice, analysts, but also commonly used numerical software may confuse a
missing value with a default value or category. For instance, in \code{Excel
2010}, the result of adding the contents of a field containing the number 1
with an empty field results in 1. This behavior is most definitely unwanted
since \code{Excel} silently imputes `0' where it should have said something
along the lines of `unable to compute'. It should be up to the analyst to
decide how empty values are handled, since a default imputation may yield
unexpected or erroneous results for reasons that are hard to trace. Another
commonly encountered mistake is to confuse an \code{NA} in categorical data
with the category \emph{unknown}. If \emph{unknown} is indeed a category, it
should be added as a \code{factor} level so it can be appropriately analyzed.
Consider as an example a categorical variable representing  \emph{place of
birth}. Here, the category \emph{unknown} means that we have no knowledge about
where a person is born. In contrast, \code{NA} indicates that we have no
information to determine whether the birth place is known or not.

The behavior of \R{}'s core functionality is completely consistent with the
idea that the analyst must decide what to do with missing data. A common choice,
namely `leave out records with missing data' is supported by many base functions
through the \code{na.rm} option.\\
<<>>=
age <- c(23, 16, NA)
mean(age)
mean(age, na.rm=TRUE)
@
Functions such as \code{sum}, \code{prod}, \code{quantile}, \code{sd} and so on all
have this option. Functions implementing bivariate statistics such as \code{cor}
and \code{cov} offer options to include complete or pairwise complete values.

Besides the \code{is.na} function, that was already mentioned in section
\ref{sect:specval}, \R{} comes with a few other functions facilitating
\code{NA} handling.  The \code{complete.cases} function detects rows in a
\code{data.frame} that do not contain \code{any} missing value. Recall the
\code{person} data set example from page \pageref{pp:person}.\\
<<>>=
print(person) 
complete.cases(person)
@
The resulting \code{logical} be used to remove incomplete records from the
\code{data.frame}.  Alternatively the \code{na.omit} function, does the same.\\
<<>>=
(persons_complete <- na.omit(person))
na.action(persons_complete)
@
The result of the \code{na.omit} function is a \code{data.frame} where
incomplete rows have been deleted. The \code{row.names} of the removed
records are stored in an attribute called \code{na.action}.

\begin{tip}{Note}
It may happen that a missing value in a data set means $0$ or \code{Not applicable}.
If that is the case, it should be explicitly imputed with 
that value, because it is not unknown, but was coded as empty.
\end{tip}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Special values}
As explained in section \ref{sect:specval}, numeric variables are endowed with
several formalized special values including $\pm$\code{Inf}, \code{NA} and
\code{NaN}. Calculations involving special values often result in special
values, and since a statistical statement about a real-world phenomenon
should never include a special value, it is desirable to handle special
values prior to analysis.

For numeric variables, special values indicate values that are not an element
of the mathematical set of real numbers ($\mathbb{R}$). The function \code{is.finite}
determines which values are `regular' values.\\
<<>>=
is.finite(c(1, Inf, NaN, NA))
@
This function accepts vectorial input. With little effort we can write a function
that may be used to check every numerical column in a data.frame\\
%
<<tidy=FALSE>>=
is.special <- function(x){
  if (is.numeric(x)) !is.finite(x) else is.na(x)
}
person
sapply(person, is.special)
@
Here, the \code{is.special} function is applied to each column of \code{person}
using \code{sapply}. \code{is.special} checks its input vector for numerical
special values if the type is numeric, otherwise it only checks for \code{NA}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Outliers}
There is a vast body of literature on outlier detection, and several
definitions of \emph{outlier} exist. A general definition by Barnett and
Lewis\cite{barnett:1994} defines an outlier in a data set as \emph{an
observation (or set of observations) which appear to be inconsistent with
that set of data}. Although more precise definitions exist (see e.g. the book
by Hawkins\cite{hawkins:1980}), this definition is sufficient for the current
tutorial. Below we mention a few fairly common graphical and computational
techniques for outlier detection in univariate numerical data.

\begin{tip}{Note}
Outliers do not equal errors. They should be detected, but not necessarily 
removed. Their inclusion in the analysis is a statistical decision.
\end{tip}

For more or less unimodal and symmetrically distributed data, Tukey's
box-and-whisker method\cite{tukey:1977} for outlier detection is often
appropriate. In this method, an observation is an outlier when it is larger
than the so-called ``whiskers'' of the set of observations.  The upper whisker
is computed by adding 1.5 times the interquartile range to the third quartile
and rounding to the nearest lower observation.  The lower whisker is computed
likewise.

The base \R{}, installation comes with function \code{boxplot.stats},
which, amongst other things, list the outliers.\\
<<>>=
x <- c(1:10,20, 30)
boxplot.stats(x)$out
@
Here, $20$ and $50$ are detected as outliers since they are above the upper
whisker of the observations in \code{x}. The factor $1.5$ used to compute the
whisker is to an extent arbitrary and it can be altered by setting the \code{coef}
option of \code{boxplot.stats}. A higher coefficient means a higher outlier detection
limit (so for the same dataset, generally less upper or lower outliers will be detected).\\
<<>>=
boxplot.stats(x,coef=2)$out
@

The box-and-whisker method can be visualized with the box-and-whisker plot,
where the box indicates the interquartile range and the median, the whiskers
are represented at the ends of the box-and-whisker plots and outliers are
indicated with as separate points above or below the whiskers.

The box-and-whisker method fails when data are distributed skewly, as in an
exponential or log-normal distribution for example. In that case one can
attempt to transform the data, for example with a logarithm or square root
transformation.  Another option is to use method that takes the skewness into
account. 
%
\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-1cm}
\begin{center}
<<echo=FALSE>>=
par(mar=c(0.1,3,0.1,0.1),oma=c(0,0,0,0))
@
<<echo=FALSE>>=
boxplot(x,cex=2,cex.axis=2)
@
\caption{A box-and-whisker plot, produced with the \code{boxplot} function.}
\end{center}
\end{wrapfigure}
%
A particularly easy-to-implement example is the method of Hiridoglou
and Berthelot\cite{hiridoglou:1986} for positive observations.  Icommonn this method,
an observation is an outlier when
\begin{equation}
h(x) = \max\left(
\frac{x}{x^*},\frac{x^*}{x} 
\right)
\geq r,\textrm{ and } x>0.
\end{equation}
Here, $r$ is a user-defined reference value and $x^*$ is usually the median
observation, although other measures of centrality may be chosen. Here, the
score function $h(x)$ grows as $1/x$ as $x$ approaches zero and grows linearly
with $x$ when it is larger than $x^*$. It is therefore appropriate for finding
both outliers on both sides of the distribution. Moreover, because of the
different behaviour for small and large $x$-values, it is appropriate for
skewed (long-tailed) distributions. An implementation of this method in \R{}
does not seem available but it is implemented simple enough as follows.\\
<<tidy=FALSE>>=
hboutlier <- function(x,r){
  x <- x[is.finite(x)]
  stopifnot(
    length(x) > 0
    , all(x>0)
  )
  xref <- median(x)
  if (xref <= sqrt(.Machine$double.eps))
    warning("Reference value close to zero: results may be inaccurate")
  pmax(x/xref, xref/x) > r
}
@
The above function returns a \code{logical} vector indicating which elements of 
$x$ are outliers.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Obvious inconsistencies}
An obvious inconsistency occurs when a record contains a value or combination
of values that cannot correspond to a real-world situation. For example, a
person's age cannot be negative, a man cannot be pregnant and an under-aged person cannot
possess a drivers license.

Such knowledge can be expressed as \emph{rules} or constraints. In data editing
literature these rules are referred to as \emph{edit rules} or \emph{edits}, in short.
Checking for obvious inconsistencies can be done straightforwardly in \R{} using
logical indices and recycling. For example, to check which elements of
\code{x} obey the rule `\code{x} must be non negative' one simply uses the following.\\
<<eval=FALSE>>=
x_nonnegative <- x >= 0
@
However, as the number of variables increases, the number of rules may increase
rapidly and it may be beneficial to manage the rules separate from the data.
Moreover, since multivariate rules may be interconnected by common variables,
deciding which variable or variables in a record cause an inconsistency may not
be straightforward.

The \code{editrules} package\cite{jonge:2011} allows one to define rules on
categorical, numerical or mixed-type data sets which each record must obey.
Furthermore, \code{editrules} can check which rules are obeyed or not and
allows one to find the minimal set of variables to adapt so that all rules can
be obeyed.  The package also implements a number of basic rule operations
allowing users to test rule sets for contradictions and certain redundancies.

As an example, we will work with a small file containing the following data.
\begin{center}
\lstinputlisting[
    xleftmargin=1cm,
    numbers=left,
    basicstyle=\ttfamily\small
  ]{files/people.txt}
\end{center}
%
We read this data into a variable called \code{people}
and define some restrictions on \emph{age} using \code{editset}.\\
<<>>=
people <- read.csv("files/people.txt")
library(editrules)
(E <- editset(c("age >=0", "age <= 150")))
@
The \code{editset} function parses the textual rules and stores them in an
\code{editset} object. Each rule is assigned a name according to it's type
(\code{num}eric, \code{cat}egorical, or \code{mix}ed) and a number.  The data
can be checked against these rules with the \code{violatedEdits} function.
Record 4 contains an error according to one of the rules: an \code{age} of
\Sexpr{person$age[4]} is not allowed. \\
%
<<>>=
violatedEdits(E, people)
@
\code{violatedEdits} returns a \code{logical} array indicating  for each row of
the data, which rules are violated.

The number and type of rules applying to a data set usually quickly grow with
the number of variables. With \code{editrules}, users may read rules, specified
in a limited \R{}-syntax, directly from a text file using the \code{editfile}
function. As an example consider the contents of the following text file. 
\begin{center}
\lstinputlisting[
    xleftmargin=1cm,
    numbers=left,
    basicstyle=\ttfamily\small
  ]{files/edits.txt}
\end{center}
There are rules pertaining to purely numerical, purely categorical and rules
pertaining to both data types. Moreover, there are univariate as well as multivariate
rules. Comments are written behind the usual \code{\#} character. The rule set can
be read as follows.\\
<<cache=FALSE>>=
E <- editfile("files/edits.txt")
@
As the number of rules grows, looking at the full array produced by
\code{violatedEdits} becomes cumbersome. For this reason, \code{editrules}
offers methods to summarize or visualize the result.\\
<<>>=
ve <- violatedEdits(E,people)
summary(ve)
plot(ve)
@
Here, the edit labeled \code{cat5} is violated by two records (20\% of all
records).  Violated edits are sorted from most to least often violated. The
plot visualizes the same information.


Since rules may pertain to multiple variables, and variables may occur in
several rules (\emph{e.g.} the \emph{age} variable in the current example),
there is a dependency between rules and variables. It can be informative
to show these dependencies in a graph using the \code{plot} function, as in
Figure 3.\\
%
<<echo=FALSE>>=
captxt = paste(
"A graph plot, created with \\code{plot(E)}, showing the interconnection of restrictions.",
"Blue circles represent variables and yellow boxes represent restrictions.",
"The lines indicate which restrictions pertain to what variables."
)
set.seed(1)
par(mar=rep(0,4),oma=rep(0,4))
@
<<echo=FALSE, width=3.5, fig.cap=captxt>>=
plot(E)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Error localization}
\label{sect:errloc}
The interconnectivity of edits is what make error localization a hard problem.
For example, the graph in Figure 3 shows that a record violating edit
\code{num4} may contain an error in \emph{age} and/or \emph{years married}.
Suppose that we alter \emph{age} so that \code{num4} is not violated anymore. We then
run the risk of violating up to \emph{six} other edits containing \emph{age}.

If we have no other information available but the edit violations, it makes
sense to minimize the number of fields being altered. This principle, commonly
referred to as the principle of Fellegi and Holt\cite{fellegi:1976}, is based
on the idea that errors occur relatively few times and when they do, they occur
randomly across variables. Over the years several algorithms have been
developed to solve this minimization problem\cite{waal:2011} of which two have
been implemented in \code{editrules}. The \code{localizeErrors} function
provides access to this functionality.

As an example we take two records from the \code{people} dataset from the
previous subsection.\\
<<>>=
id <- c(2,5)
people[id,]
violatedEdits(E,people[id,])
@
Record 2 violates \code{mix6} while record 5 violates edits \code{num2},
\code{cat5} and \code{mix8}. We use \code{localizeErrors}, with a mixed-integer programming
approach to find the minimal set of variables to adapt.\\
<<>>=
le <- localizeErrors(E, people[id,],method='mip')
le$adapt
@
Here, the \code{le} object contains some processing metadata and a logical
array labeled \code{adapt} which indicates the minimal set of variables to be
altered in each record. It can be used in correction and imputation
procedures for filling in valid values. Such procedures are not part of
\code{editrules}, but for demonstration purposes we will manually fill
in new values showing that the solution computed by \code{localizeErrors}
indeed allows one to repair records to full compliance with all edit rules.\\
<<results='hide'>>=
people[2,'status'] <- 'single'
people[5,'height'] <- 7
people[5,'agegroup'] <- 'adult'
summary(violatedEdits(E,people[id,]))
@
%
The behavior of \code{localizeErrors} can be tuned with various options. It is
possible to supply a confidence weight for each variable allowing for fine
grained control on which values should be adapted. It is also possible to
choose a branch-and-bound based solver (instead of the MIP solver used here),
which is typically slower but allows for more control. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Correction}
\label{sect:correction}
Correction methods aim to fix inconsistent observations by altering invalid
values in a record based on information from valid values. Depending on the
method this is either a single-step procedure or a two-step procedure where
first, an error localization method is used to empty certain fields, followed
by an imputation step.

In some cases, the cause of errors in data can be determined with enough
certainty so that the solution is almost automatically known. In recent years,
several such methods have been developed and implemented in the
\code{deducorrect} package\cite{loo:2011}. In this section we give a quick
overview of the possibilities of \code{deducorrect}. \\
<<echo=FALSE, result=FALSE>>=
library(deducorrect)
@

\subsubsection{Automated ad hoc correction}
In practice, data cleaning procedures involve a lot of \emph{ad-hoc}
transformations. This may lead to long scripts where one selects parts of
the data, changes some variables, selects another part, changes some more
variables, etc. When such scripts are neatly written and commented, they can
almost be treated as a log of the actions performed by the analyst. However, as
scripts get longer it is better to store the transformation rules separately
and log which rule is executed on what record.  The \code{deducorrect} package
offers functionality for this. Consider as an example the following
(fictitious) dataset listing the body length of some brothers.\\
<<>>=
(marx <- read.csv("files/marx.csv",stringsAsFactors=FALSE))
@
The task here is to standardize the lengths and express all of them in
meters. The obvious way would be to use the indexing techniques summarized
in Section \ref{sect:indexing}, which would look something like this.\\
<<>>=
marx_m <- marx
I <- marx$unit == "cm"
marx_m[I,'height'] <- marx$height[I]/100
I <- marx$unit == "inch"
marx_m[I,'inch'] <- marx$height[I]/39.37
I <- marx$unit == "ft"
marx_m[I,'ft'] <- marx$height[I]/3.28
marx_m$unit <- "m"
@
Such operations quickly become cumbersome. Of course, in this case one could
write a for-loop but that would hardly save any code. Moreover, if you want to
check afterwards which values have been converted and for what reason, there
will be a significant administrative overhead.

The deducorrect package takes all this overhead of your hands with the 
\code{correctionRules} functionality. For example, to perform the above 
task, one first specifies a file with correction rules as follows.\\
<<echo=FALSE>>=
read_chunk("files/conversions.txt")
@
%<<conversions,eval=FALSE,tidy=FALSE>>=
%@
\begin{center}
\lstinputlisting[
    xleftmargin=1cm,
    numbers=left,
    basicstyle=\ttfamily\small
  ]{files/conversions.txt}
\end{center}
%
With \code{deducorrect} we can read these rules, apply them to the data and
obtain a log of all actual changes as follows.\\
<<>>=
# read the conversion rules.
R <- correctionRules("files/conversions.txt")
R
@
\code{correctionRules} has parsed the rules and stored them in a \code{correctionRules}
object. We may now apply them to the data.\\
<<>>=
cor <- correctWithRules(R,marx)
@
The returned value, \code{cor}, is a \code{list} containing the
corrected data\\
<<>>=
cor$corrected
@
as well as a also a log of applied corrections.\\
<<>>=
cor$corrections[1:4]
@
The log lists for each row, what variable was changed, what the old value was
and what the new value is. Furthermore, the fifth column of
\code{cor\$corrections} shows the corrections that were applied (not shown
above for formatting reasons)\\
<<>>=
cor$corrections[5]
@
So here, with just two commands, the data is processed and all actions logged
in a \code{data.frame} which may be can stored or analyzed. The rules that may
be applied with \code{deducorrect} are rules that can be executed
record-by-record. 

By design, there are some limitations to which rules can be applied with
\code{correctWithRules}.  The processing rules should be executable
record-by-record. That is, it is not permitted to use functions line
\code{mean} or \code{sd}. The symbols that may be used can be listed as
follows.\\
<<>>=
getOption('allowedSymbols')
@
When the rules are read by \code{correctionRules}, it checks whether any symbol
occurs that is not in the list of allowed symbols and returns an error message
when such a symbol is found as in the following example.\\
<<>>=
correctionRules(expression(x <- mean(x)))
@
%
Finally, it is currently not possible to add new variables using
\code{correctionRules} although such a feature will likely be added in the
future.

\subsubsection{Deductive correction}
\label{sect:deductivecorrection}
When the data you are analyzing is generated by people rather than machines or
measurement devices, certain typical human-generated errors are likely to
occur. Given that data has to obey certain edit rules, the occurrence of such
errors can sometimes be detected from raw data with (almost) certainty.
Examples of errors that can be detected are typing errors in numbers (under linear
restrictions) rounding errors in numbers and sign errors or variable swaps\cite{scholtus:2011}.
The \code{deducorrect} package has a number of functions available that can correct
such errors. Below we give some examples, every time with just a single edit rule.
The functions can handle larger sets of edits however.

With \code{correctRoundings} deviations of the size of one or two measurement
units can be repaired. The function randomly selects one variable to alter such that
the rule violation(s) are nullified while no new violations are generated.\\
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 100,y=101,z=200)
cor <- correctRounding(e,d)
cor$corrected
cor$corrections
@

The function \code{correctSigns} is able to detect and repair sign errors.  It
does this by trying combinations of variable swaps on variables that occur in
violated edits.\\
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 100, y = -100, z=200)
cor <- correctSigns(e,d)
cor$corrected
cor$corrections
@

Finally, the function \code{correctTypos} is capable of detecting and correcting
typographic errors in numbers. It does this by computing candidate solutions and
checking whether those candidates are less than a certain string distance (see Section 
\ref{sect:appstringmatching}) removed from the original.\\
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 123, y = 132, z = 246)
cor <- correctTypos(e,d)
cor$corrected
cor$corrections
@
Indeed, swapping the $3$ and the $2$ in $y=132$ solves the edit violation.

\begin{tip}{Tip}
Every \code{correct-} function in \code{deducorrect} is an object of class
\code{deducorrect}. When printed, it doesn't show the whole contents (the
corrected data and logging information) but a summary of what happened with
th data. A \code{deducorrect} object also contains data on timing, user,
and so on. See \code{?"deducorrect-object"} for a full explanation.
\end{tip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Deterministic imputation}

In some cases a missing value can be determined because the observed values
combined with their constraints force a unique solution.  As an example,
consider a record with variables listing the costs for \emph{staff}
\emph{cleaning}, \emph{housing} and the total \emph{total}.  We have the
following rules.
\begin{equation}
\begin{array}{l}
  \textrm{\em staff} + \textrm{\em cleaning} + \textrm{\em housing} = \textrm{\em total}\\
  \textrm{\em staff} \geq 0\\
  \textrm{\em housing} \geq 0\\
  \textrm{\em cleaning} \geq 0
\end{array}
\end{equation}
In general, if one of the variables is missing the value can clearly be derived
by solving it for the first rule (providing that the solution doesn't violate
the last rule). However, there are other cases where unique solutions exist.
Suppose that we have $staff = total$. Assuming that these values are correct,
the only possible values for the other two variables  is $housing=cleaning=0$.
The \code{deducorrect} function \code{deduImpute} is able to recognize such
cases and compute the unique imputations.\\
<<tidy=FALSE>>=
E <- editmatrix(expression(
  staff + cleaning + housing == total,
  staff    >= 0,
  housing  >= 0,
  cleaning >= 0
))
dat <- data.frame(
  staff = c(100,100,100),
  housing = c(NA,50,NA),
  cleaning = c(NA,NA,NA),
  total = c(100,180,NA)
)
dat
cor <- deduImpute(E,dat)
cor$corrected
@
Note that \code{deduImpute} only imputes those values that can be derived
with absolute certainty (uniquely) from the rules. In the example, there are
many possible solutions to impute the last record and hence \code{deduImpute}
leaves it untouched.

Similar situations exist for categorical data, which are handled by \code{deduImpute} as well.\\
<<tidy=FALSE>>=
E <- editarray(expression(
  age %in% c("adult","under-aged"),
  driverslicense %in% c(TRUE, FALSE),
  if ( age == "under-aged" ) !driverslicense
))
dat <- data.frame(
  age = NA,
  driverslicense = TRUE
)
dat
cor <- deduImpute(E,dat)
cor$corrected
@
Here, \code{deduImpute} uses automated logic to derive from the conditional
rules that if someone has a drivers license, he has to be an adult.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Imputation}
\label{sect:mbimputation}
Imputation is the process of estimating or deriving values for fields where
data is missing.  There is a vast body of literature on imputation methods and
it goes beyond the scope of this tutorial to discuss all of them. In stead, we
present in Table \ref{tab:imputation} an overview of packages that offer some
kind of imputation method and list them against a number of popular model-based
imputation methods. We note that the list of packages and methods are somewhat
biased towards applications at Statistics Netherlands, as the table was
originally produced during a study for internal purposes\cite{broek:2012}.
Nevertheless we feel that this overview can be a quite useful place to start.
De Waal, Pannekoek and Scholtus\cite{waal:2011} (Chpt.\ 7) give a consice overview of many 
of established imputation methods.  

The packages \code{Amelia}, \code{deducorrect} and \code{mix} do not implement
any of the methods mentioned in the table. That is because \code{Amelia}
implements a multiple imputation method which was out of scope for the study 
mentioned above. The \code{deducorrect} package
implements deductive and deterministic methods, and we choose not to call this
model-based in this tutorial. The \code{mix} package implements a Bayesian
estimation method based on an Markov Chain Monte Carlo algorithm.

There is no one single best imputation method that works in all cases. The
imputation model of choice depends on what auxiliary information is available
and whether there are (multivariate) edit restrictions on the data to be
imputed. The availability of \R{} software for imputation under edit
restrictions is, to our best knowledge, limited. However, a viable strategy for
imputing numerical data is to first impute missing values without restrictions,
and then minimally adjust the imputed values so that the restrictions are
obeyed.  Separately, these methods are available in \R{}.

\begin{table}[!t]
\begin{adjustwidth}{-2mm}{}
\begin{threeparttable}
\caption{An overview of imputation functionality offered by some  \R{}
packages.  reg: regression, rand: random, seq: sequential, NN: nearest
neighbor, pmm: predictive mean matching, kNN: $k$-nearest-neighbors, int:
interpolation, lo/no last observation carried forward / next observation
carried backward, LS: method of Little and Su.  }
\label{tab:imputation}
\begin{tabular}{|l|ccc|ccc|c|ccc|}
\hline
\multicolumn{1}{|l}{} & 
\multicolumn{3}{|c|}{Numeric} &
\multicolumn{3}{|c|}{Hot deck}&
\multicolumn{1}{|c|}{}&
\multicolumn{3}{|c|}{Longitudinal}\\

Package                             & 
\multicolumn{1}{|c}{mean}  & 
\multicolumn{1}{c}{ratio}  & 
\multicolumn{1}{c|}{reg.}   & 
\multicolumn{1}{c}{rand}  & 
\multicolumn{1}{c}{seq}   & 
\multicolumn{1}{c|}{pmm}   & 
\multicolumn{1}{c}{kNN}    & 
\multicolumn{1}{|c}{int}   & 
\multicolumn{1}{c}{lo/no}  & 
\multicolumn{1}{c|}{LS}     \\ \hline
\code{Amelia}\cite{honaker:2011}       &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{BaBoon}\cite{meinfelder:2011}    &\xmark & \xmark &\xmark           &\xmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{cat}\cite{harding:2012}          &\xmark & \xmark &\xmark           &\cmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\  
\code{deducorrect}\cite{loo:2011}      &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ \hline
\code{e1071}\cite{meyer:2012}          &\cmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{ForImp}                          &\cmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\cmark$^\ddagger$ &\xmark &\xmark & \xmark \\ 
\code{Hmisc}\cite{harrel:2013}         &\cmark & \xmark &\xmark           &\cmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{imputation}\cite{wong:2013}      &\cmark & \xmark &\cmark$^\dagger$ &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ \hline
\code{impute}\cite{hastie}             &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{mi}\cite{su:2011}                &\xmark & \xmark &\cmark$^*$       &\cmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{mice}\cite{buren:2011}           &\cmark & \xmark &\cmark$^*$       &\cmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{mix}\cite{schafer:2010}          &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ \hline
\code{norm}\cite{alvaro:2013}          &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{robCompositions}\cite{templ:2011}&\xmark & \xmark &\cmark$^*$       &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{rrcovNA}\cite{todorov:2012}      &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{StatMatch}\cite{orazio:2012}     &\xmark & \xmark &\xmark           &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ \hline
\code{VIM}\cite{templ:2012}            &\xmark & \xmark &\cmark$^*$       &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{yaImpute}\cite{crookston:2007}   &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{zoo}\cite{zeileis:2005}          &\cmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\cmark &\cmark & \xmark \\ 
\hline                                                                                                             
\end{tabular}
{\scriptsize ${^*}$Methods are ultimately based on some form of regression, but are more involved than simple linear regression.\\
${^\dagger}$Uses a non-informative auxiliary variable (row number).\\
${^\ddagger}$Uses nearest neighbor as part of a more involved imputation scheme.
}
\end{threeparttable}
\end{adjustwidth}
\end{table}
%
In the following subsections we discuss three types of imputation
models and give some pointers to how to implement them using \R{}.
The next section (\ref{sect:rspa}) is devoted to value adjustment.

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Basic numeric imputation models}
\label{sect:numericimputation}
Here, we distinguish three imputation models. The first is imputation of the mean:
\begin{equation}
\hat{x}_i = \bar{x},
\end{equation}
where the $\hat{x}_i$ is the imputation value and the mean is taken over the
observed values. The usability of this model is limited since it obviously
causes a bias in measures of spread, estimated from the sample after
imputation. However, in base \R{} it is implemented simply enough.\\
<<eval=FALSE>>=
x[is.na(x)] <- mean(x,na.rm=TRUE)
@
In principle one can use other measures of centrality, and in fact, the \code{Hmisc}
package has a convenient wrapper function allowing you to specify what function is
used to compute imputed values from the non-missing. For example, imputation of 
the mean or median can be done as follows. \\
<<eval=FALSE>>=
library(Hmisc)
x <- impute(x, fun=mean)     # mean imputation
x <- impute(x, fun=median)   # median imputation
@
An nice feature of the \code{impute} function is that the resulting
vector ``remembers'' what values were imputed. This information
may be requested with \code{is.imputed} as in the example below.\\
<<echo=FALSE,message=FALSE>>=
library(Hmisc)
@
<<>>=
x <- 1:5   # create a vector...
x[2] <- NA # ...with an empty value
x <- impute(x,mean)
x
is.imputed(x)
@
Note also that imputed values are printed with a post-fixed asterix.

The second imputation model we discuss is ratio imputation. Here, the
imputation estimate $\hat{x}_i$ is given by 
\begin{equation}
\hat{x}_i = \hat{R} y_i,
\end{equation}
Where $y_i$ is a covariate and $\hat{R}$ is an estimate of the average ratio
between $x$ and $y$. Often this will be given by the sum of observed $x$ values
divided by the sum of corresponding $y$ values although variants are possible.
Ratio imputation has the property that the estimated value equals $\hat{x}=0$
when $y=0$, which is in general not guaranteed in linear regression. Ratio
imputation may be a good model to use when restrictions like $x\geq 0$ and/or
$y\geq0$ apply. There is no package directly implementing ratio imputation,
unless it is regarded a special case of regression imputation. It is easily
implemented using plain \R{} though. Below, we suppose that \code{x} and \code{y}
are numeric vectors of equal length, \code{x} contains missing values and \code{y}
is complete.\\
<<eval=FALSE>>=
I <- is.na(x)
R <- sum(x[!I])/sum(y[!I])
x[I] <- R*y[I] 
@
Unfortunately, it is not possible to simply wrap the above in a function and
pass this to \code{HMisc}'s \code{impute} function. There seem to be no packages
that implement ratio imputation in a single function.


The third, and last numerical model we treat are (generalized) linear
regression models. In such models, missing values are imputed
as follows
\begin{equation}
\hat{x}_i = \hat{\beta}_0 + \hat{\beta}_1y_{1,i} + \cdots + \hat{\beta}_ky_{k,i},
\end{equation}
where the $\hat{\beta}_0,\hat{\beta}_1\ldots\hat{\beta}_k$ are estimated linear
regression coefficients for each of the auxiliary variables
$y_1,y_2\ldots,y_k$. Estimating linear models is easy enough using \code{lm}
and \code{predict} in standard \R{}. Here, we use the built-in \code{iris}
data set as an example.\\
<<>>=
data(iris)
iris$Sepal.Length[1:10] <- NA
model <- lm(Sepal.Length ~ Sepal.Width + Petal.Width,data=iris)
I <- is.na(iris$Sepal.Length)
iris$Sepal.Length[I] <- predict(model, newdata=iris[I,])
@
Above, \code{lm} automatically omits rows with empty values and estimates the
model based on the remaining records. Next, we used \code{predict} to estimate
the missing values. 

It should be noted that although \code{Hmisc}, \code{VIM}, \code{mi} and
\code{mice} all implement imputation methods that ultimately use some form of
regression, they do not include a simple interface for the case described
above.  The more advanced methods in those packages are aimed to be more
precise and/or robust against outliers than standard (generalized) linear
regression.  Moreover, both \code{mice} and \code{mi} implement methods for
multiple imputation, which allows for estimation of the imputation variance.
Such methods are beyond the scope of the current text. The \code{imputation}
package has a function \code{lmImpute} but is uses the row number as auxiliary
variable which limits its practical usability to cases where the row number is
directly related to a covariate value.


\subsubsection{Hot deck imputation}
\label{sect:hotdeck}
In hot deck imputation, missing values are imputed by copying values from
similar records in the same dataset. Or, in notation:
\begin{equation}
\hat{x}_i = x_j, 
\end{equation}
where $x_j$ is taken from the observed values.  Hot-deck imputation can be
applied to numerical as well as categorical data but is only viable when enough
donor records are available. 

The main question in hot-deck imputation is
how to choose the replacement value $x_j$ from the observed values.  Here, we
shortly discuss four well-known flavors.

In \emph{random} hot-deck imputation, a value is chosen randomly, and uniformly
from the same data set. When meaningful, random hot deck methods can be
applied per stratum. For example, one may apply random hot-deck imputation of
body height for male and female respondents separately. Random hot-deck on a
single vector can be applied with the \code{impute} function of the
\code{Hmisc} package.\
<<>>=
data(women)
# add some missingess
height <- women$height
height[c(6,9)] <- NA
height
(height <- impute(height,"random"))
@
Note that the outcome of this imputation is very likely tot be different each
time it is executed. If you want the results to be random, but repeatable (e.g.
for testing purposes) you can use \code{set.seed(<a number>)} prior to calling
\code{impute}. 

In \emph{sequential} hot deck imputation, the vector containing missing values is
sorted according to one or more auxiliary variables so that records that have
similar auxiliaries occur sequentially in the \code{data.frame}.  Next, each
missing value is imputed with the value from the first following record that
has an observed value. There seems to be no package that implements the
sequential hot deck technique. However, given a vector that is sorted, the
following function offers some basic functionality.\\
<<tidy=FALSE>>=
# x    :  vector to be imputed
# last : value to use if last value of x is empty
seqImpute <- function(x,last){
  n <- length(x)
  x <- c(x,last)
  i <- is.na(x)
  while(any(i)){
    x[i] <- x[which(i) + 1]
    i <- is.na(x)
  }
  x[1:n]
}
@
We will use this function in exercise \ref{ex:seqimpute}. 


\emph{Predictive mean matching} (pmm) is a form of nearest-neighbor hot-deck
imputation with a specific distance function. Suppose that in record $j$, the
$i$th value is missing. One then estimates the value $\hat{x}_{ji}$ using a
prediction model (often some form of regression) for the record with the
missing values as well as for possible donor records $k\not=j$ where the $i$the
value is not missing. One then determines
$k^*=\argmin_{k\not=j}d(\hat{x}_{ki},\hat{x}_{ji})$ with $d$ a distance
function and copies the observed value $x_{k^*i}$ to the $j$th record.

Predictive mean matching imputation has been
implemented in several packages (\code{mi}, \code{mice}, \code{BaBoon}) but
each time either for a specific type of data and/or using a specific prediction
model (\emph{e.g.} \code{Bayesglm}). Moreover, the packages known to us always
use pmm as a part of a more elaborate multiple imputation scheme, which we
deemed out of scope for this tutorial.

\subsubsection{kNN-imputation}
In $k$ \emph{nearest neighbor} imputation one defines a distance
function $d(i,j)$ that computes a measure of dissimilarity between records. A
missing value is then imputed by finding first the $k$ records nearest to the
record with one or more missing values. Next, a value is chosen from or
computed out of the $k$ nearest neighbors. In the case where a value is picked
from the $k$ nearest neighbors, kNN-imputation is a form of hot-deck imputation.

The \code{VIM} package contains a function called \code{kNN} that uses Gowers
distance\cite{gower:1971} to determine the $k$ nearest neighbors. Gower's
distance between two records labeled $i$ and $j$ is defined as
\begin{equation}
d_{\sf g}(i,j) \frac{\sum_k w_{ijk} d_k(i,j)}{\sum_{k}w_{ijk}},
\end{equation}
where the sum runs over all variables in the record and $d_k(i,j)$
is the distance between the value of variable $k$ in record $i$ and record $j$.
For categorical variables, $d_k(i,j) = 0$ when the value for the $k'th$ variable
is the same in record $i$ and $j$ and $1$ otherwise. For numerical variables
the distance is given by $1-(x_i-x_j)/(\max(x)-\min(x))$. The weight $w_{ijk}=0$
when the $k$'th variable is missing in record $i$ or record $j$ and otherwise
$1$.

Here is an example of \code{kNN}.\\
<<message=FALSE>>=
library(VIM)
data(iris)
n <- nrow(iris)
# provide some empty values (10 in each column, randomly)
for ( i in 1:ncol(iris) ){
  iris[sample(1:n,10,replace=FALSE),i] <- NA
}
iris2 <- kNN(iris)
@
The \code{kNN} function determines the $k$ (default: 5) nearest neighbors of a
record with missing values. For numerical variables the median of the $k$
nearest neighbors is used as imputation value, for categorical variables the
category that most often occurs in the $k$ nearest neighbors is used. Both the
these functions may be replaced by the user. Finally, \code{kNN} prints (negatively)
the time it took to complete the imputation. We encourage the reader to execute
the above code and experiment further. 



\subsubsection{Minimal value adjustment}
\label{sect:rspa}
Once missing numerical values have been adequately imputed, there is a good
chance that the resulting records violate a number of edit rules.  The obvious
reason is that there are not many methods that can perform imputation under
arbitrary edit restrictions. One viable option is therefore to minimally
adjust imputed values such that after adjustment the record passes every edit
check within a certain tolerance. 

We need to specify more clearly what \emph{minimal} adjustment means here.
The \code{rspa} package\cite{loo:2012} is able to take a numerical record
$\bs{x}^0$ and replace it with a record $\bs{x}$, such that
the weighted Euclidean distance
\begin{equation}
\sum_iw_i(x_i-x_i^{0})^2,
\end{equation}
is minimized and $\bs{x}$ obeys a given set of (in)equality restrictions
\begin{equation}
\bs{Ax} \leq \bs{b}.
\end{equation}

As practical example consider the following script.\\
<<>>=
library(editrules)
library(rspa)
E <- editmatrix(expression(
  x + y == z,
  x >= 0,
  y >= 0
))
d <- data.frame(x = 10, y=10, z=21)
d1 <- adjustRecords(E,d)
d1$adjusted
@
The function \code{adjustRecords} adjusts every record minimally to obey the restrictions
in \code{E}. 
Indeed, we may check that the adjusted data now obeys every rule within \code{adjustRecords}'
default tolerance of $0.01$.\\
<<>>=
violatedEdits(E,d1$adjusted,tol=0.01)
@
By default, all variables in the input \code{data.frame} are adjusted with
equal amounts. However, suppose that $z$ is an imputed value while $x$ and $y$
are observed, original values. We can tell \code{adjustRecords} to only adjust
\code{z} as follows.\\
<<>>=
A <- array(c(x=FALSE,y=FALSE,z=TRUE),dim=c(1,3))
A
d2 <- adjustRecords(E,d, adjust=A)
d2$adjusted
@


\newpage
\subsection*{Exercises}
\addcontentsline{toc}{subsection}{Exercises}
In the following exercises we are going to use the \code{dirty\_iris} dataset.
You can download this dataset from 
\url{https://raw.github.com/edwindj/datacleaning/master/data/dirty_iris.csv}
\begin{quote}
\end{quote}


\begin{exercise}
Reading and manually checking.
\begin{subex}
  \item View the file in a text-editor to determine its format and read the file into \R{}. 
    Make sure that strings are not converted to factor.
  \item Calculate the number and percentage of observations that are complete.
  \item Does the data contain other special values? If it does, replace them with \code{NA}
\end{subex}
\end{exercise}
\begin{exercise}
Checking with rules
  \begin{subex}
  \item Besides missing values, the data set contains errors. We have the following background knowledge:
  \begin{itemize}
    \item \code{Species} should be one of the following values: \code{setosa}, \code{versicolor} or \code{virginica}.
    \item All measured numerical properties of an iris should be positive.
    \item The petal length of an iris is at least 2 times its petal width
    \item The sepal length of an iris cannot exceed 30 cm.
    \item The sepals of an iris are longer than its petals.
  \end{itemize}
    Define these rules in a separate text file and read them into \R{} using \code{editfile} (package \code{editrules}).
    Print the resulting constraint object.
    
  \item Determine how often each rules is broken (\code{violatedEdits}). Also summarize and plot the result.
  \item What percentage of the data has no errors?
  \item Find out which observations have too long petals using the result of \code{violatedEdits}. 
  \item Find outliers in sepal length using \code{boxplot} and \code{boxplot.stats}. Retrieve the corrosponding observations and
   look at the other values. Any ideas what might have happened? Set the outliers to \code{NA} (or a value that you may more    appropiate)
\end{subex}
\end{exercise}

\begin{exercise}
\label{ex:seqcorrection}
Correcting
\begin{subex}
  \item Replace non positive values from \code{Petal.Width} with \code{NA} using \code{correctWithRules} from the library \code{deducorrect}.
  \item Replace all erronous values with \code{NA} using (the result of) \code{localizeErrors}
\end{subex}
\end{exercise}
\begin{exercise}
\label{ex:seqimpute}
Imputing
\begin{subex}
  \item Use \code{kNN} imputation (\code{VIM}) to impute all missing values. 
  \item Use sequential hotdeck imputation to impute \code{Petal.Width} by sorting the dataset on \code{Species}. Compare the imputed \code{Petal.Width} with the sequential hotdeck imputation method. Note the ordering of the data!
  \item Do the same but now by sorting the dataset on \code{Species} and \code{Sepal.Length}.
\end{subex}
\end{exercise}
