% Ideas
% - Define 
% - univariate, multivariate rules
% - rule checking, summarizing, plotting
% - soft rules and outlier detection
% - deterministic corrections with deducorrect
% - 
%

\section{From technically correct data to consistent data}
\label{sect:inputtomicro}

\emph{Consistent data} is \emph{technically correct data} that is fit for
statistical analysis. This means that it should not contain obvious errors,
have special values such as \code{Inf} or \code{NaN}, that obstruct statistical
analysis. Also, suspicious values like outliers should be identified, so they
can either be removed, replaced with better values, or treated appropriately in
the subseqent statistical analysis.

With obvious errors, we mean that data violates expectations based on
knowledge of the subject matter that the data describes. For example, a
person's age cannot be negative, or a person under say, 18 years, can not have
a drivers license. 

In practice, simply throwing out the records which violate one or more rules is
undesirable since it may introduce estimation bias or even leavy you with to
few data to reliably analyse statistically.  Moreover, in many cases an invalid
or incomplete observations can be corrected or imputed by leveraging auxiliary
information either within the same data set or from other sources. 

Below, we first describe detection techniques that will allow you to find
records and fields in records containing erroneous values. After that we go
into deterministic correction techniques that can repair obvious errors based
on formalized or heuristic knowledge. We end with an overview of imputation
techniques, that can generate plausible values for partially missing data.

\subsection{Detection}
Detecting that something is wrong is the first step in data cleaning.  Below we
outline several methods for detecting suspicous or erroneous in either
individual variables or in combinations thereof.


\subsubsection{Detecting extraordinary values}
%<<echo=FALSE>>=
%person <- transform(person, height=as.numeric(as.character(height)))
%@

\R{} contains several functions to deal with special values. We have 
encountered several of them in section \ref{sect:specval}.
 
\begin{tip}{Tip}
It is important not to confuse empty values with a specific value. For example,
if an empty value in a numerical column is to be interpreted as $0$, you should
explicitly impute zeros so this interpretation is at least recorded in the
code. 
\end{tip}

Often, data will contain empty values. Rows (cases) containing \code{NA} can be
detected and removed easily in \R{}, with the functions \code{complete.cases}
which returns a \code{logical} indicating which rows contain no \code{NA}
values. By indexing, this may be used to select the complete rows from a
\code{data.frame}.  However, \R{} also contains the \code{na.omit} function,
which does exactly that.
<<>>=
print(person) 
complete.cases(person)
na.omit(person)
@
The result of the \code{na.omit} function is a \code{data.frame} with the 
incomplete rows
removed, but also contains the names of the removed rows in the attribute 
\code{na.action}.

Numeric columns can have more formalized special values: \code{Inf}, \code{-Inf},
\code{NA} and \code{NaN}. Checking for these values together can be done with 
\code{is.finite}.

For example:
<<>>=
is.finite(c(1, Inf, NaN, NA))
@
\code{is.finite} works on a vector. We can write a utility 
function \code{is.special} that checks a column for the
(non)occurrence of special values

<<tidy=FALSE>>=
is.special <- function(x){
  if (is.numeric(x)) !is.finite(x) else is.na(x)
}
person
sapply(person, is.special)
@
%
The result of \code{is.special} shows which data have special values. Observe
that the \code{height} column is still stored as a \code{character} vector here
(not \code{numeric}), so only the \code{NA} is detected.

\subsubsection{Outliers}
There is a vast body of literature on outlier detection, and several
definitions of \emph{outlier} exist. A general definition by Barnett and
Lewis\cite{barnett:1994} defines an outlier in a data set as \emph{an
observation (or set of observations) which appear to be inconsistent with
that set of data}. Although more precise definitions exist (see e.g. the book
by Hawkins\cite{hawkins:1980}), this definition is sufficient for the current
tutorial. Below we mention a few fairly common graphical and computational
techniques for outlier detection in univariate numerical data.

\subsubsection{Univariate outliers}
For more or less unimodal and symettrically distributed data, 
Tukey's box-and-whisker
method\cite{tukey:1977} for outlier detection is often appropriate. In this
method, an observation is an outlier when it is larger than the so-called
``whiskers'' of the set of observations.  The upper whisker is computed by
adding 1.5 times the interquartile range to the third quartile and rounding to
the nearest lower observation.  The lower whisker is computed likewise.

The base \R{}, installation comes with function \code{boxplot.stats},
which, amongst other things, list the outliers.
<<>>=
x <- c(1:10,20, 30)
boxplot.stats(x)$out
@
Here, $20$ and $50$ are detected as outliers since they are above the uppper
whisker of the observations in \code{x}. The factor $1.5$ used to compute the
whisker is to an extent arbitrary and it can be altered by setting the \code{coef}
option of \code{boxplot.stats}. A higher coefficient means a higher outlier detection
limit (so for the same dataset, generally less upper or lower outliers will be detected).
<<>>=
boxplot.stats(x,coef=2)$out
@
%
\begin{wrapfigure}{l}{0.5\textwidth}
\begin{center}
<<>>=
boxplot(x)
@
\caption{A box-and-whisker plot.}
\end{center}
\end{wrapfigure}
%
The box-and-whisker method can be visualized with the box-and-whisker plot,
where the box indicates the interquartile range and the median, the whiskers
are represented at the ends of the box-and-whisker plots and outliers are
indicated with as separate points above or below the whiskers.

The box-and-whisker method fails when data are distributed skewly, as in an
exponential or lognormal distribution for example. In that case one can either
transform the data for example with a logarithm, or square root transformation.
Another option is to use method that takes the skewness into account. A
particularly easy-to-implement example is the method of Hiridoglou and
Berthelot\cite{hiridoglou:1986} for positive observations.  In this method, an
observation is an outlier when
\begin{equation}
h(x) = \max\left(
\frac{x}{x^*},\frac{x^*}{x} 
\right)
\geq r,\textrm{ and } x>0.
\end{equation}
Here, $r$ is a user-defined reference value and $x^*$ is usually the the median
observation, although other measures of centrality may be chosen. Here, the
score function $h(x)$ grows as $1/x$ as $x$ approaches zero and grows linearly
wiht $x$ when it is larger than $x^*$. It is therefore appropriate for
long-tailed distributions. An implementation in of this method in \R{} does not 
seem available but it is implemented simple enough as the following function.
<<>>=
hboutlier <- function(x,r){
  xref <- median(x, na.rm=TRUE)
  max(x/xref, xref, x, na.rm=TRUE) > r
}
@
The above function returns a \code{logical} vector indicating which elements of 
$x$ are outliers.


\subsection{Checking record-wise rules with the \code{editrules} package.} 
With \R{} you can write scripts that do error detection, by writing code that 
checks each hard rule. However if you have many rules this soon gives many problems
Rules may interact, so the rules should be not seen as separate rules, but as a
set of connected rules. Furthermore your \R{} code quickly obscures the rules you 
try to implement. It would be easier and more clear to define a rule set that all
records must obey, and to check if a record obeys the rules.

The \code{editrules} package allows one to define rules on categorical,
numerical or mixed data sets which each record must obey. Furthermore it can 
use these rules to check data for errors and find erroneous values. It also allows
for manipulating the rule set, for example checking if the set of rules is 
consistent or not. We will briefly discuss the features of \code{editrules}, it
has many options and functions, but these are not needed in this introduction.

The rules you specify in \code{editrules} are valid \R{} syntax. They can 
be specified in a \code{character} vector directly with \code{editset} or stored 
separately in a text file and read in with \code{editfile}. 
The last option makes it a lot easier to maintain a large set of rules.
For pure numerical or categorical restrictions \code{editmatrix} and \code{editarray}
can be used. They are more efficient than the general \code{editset}.

<<echo=FALSE>>=
library(editrules)
library(xtable)
person <- read.csv("files/person.txt")
@

<<echo=FALSE,results="asis">>=
xtable(person, caption="Person data, with lots of errors")
@

Lets first start with a small example with restrictions on age.
<<>>=
(E <- editset(c("age >=0", "age <= 150")))
@
The data can be checked with \code{violatedEdits}. Record 4 contains an error.
<<>>=
violatedEdits(E, person)[4,]
@
\code{violatedEdits} shows for each row of the data, which rule (edit) is violated.

As can be seen from the table, the dataset contains lots of more errors, lets
find some more using a more complex set of rules 

<<cache=FALSE,echo=FALSE>>=
E <- editfile("edits.txt")
read_chunk('edits.txt')
@
<<rules, eval=FALSE>>=
@

The result of \code{violatedEdits} shows which rows are invalid according to
what rules. Note that if a multivariate rule is violated, it is not clear which
variable is erroneous.
<<width=7>>=
(ve <- violatedEdits(E, person))
@
It is also possible to do some statistics on the violated edits. For large
dataset this can be very revealing: it may show problems in the collection of
the data. It may also show that some rules are never violated.  The result of
violatedEdits can be plotted.
%
<<>>=
plot(ve)
@
i%
\code{editrules} makes it easy to specify rules that data must obey. It allows 
for easy checking if observations/cases/rows contain valid data or not.

Restrictions pertaining to multiple variables introduce dependencies between
those variables. Also, variables that occur in more than one restriction form a
connection between those restrictions. These dependencies can be visualized in
a dependency graph as follows (See Figure 3).
<<echo=FALSE>>=
captxt = paste(
"A graph plot, showing the interconnection of restrictions.",
"Blue circles represent varaibles and yellow boxes represent restrictions.",
"The lines indicate which restrictions pertain to what variables."
)
set.seed(1)
@
<<width=7, fig.cap=captxt>>=
plot(E)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Error localization}
\label{sect:errloc}
When using \code{is.finite}, \code{special} and univariate rules, it is clear
what values are invalid. However in case that there are many restrictions, this
may not be clear. e.g. if an observation has the variables \code{age},
\code{agegroup}, \code{status}, \code{yearsmarried} are all connected.
\code{editrules} can localize values that need to be adjusted. It takes the
Fellegi-Holt~\cite{Fellegi1976} principle which says: change the values of a minimal (weighted)
number of variables so that it complies to the rules specified. The function in
\code{editrules} that does this, is called \code{localizeErrors}.  It takes a
set of rules and a \code{data.frame} and returns the found solutions.
<<>>=
person[2,]
le <- localizeErrors(E, person[2,])
le$status
le$adapt
@

\code{\$adapt} is a \code{logical} that signifies which variables are considered
erroneous. It can be used in the correction and imputation procedures for filling in valid
values. \code{localizeErrors} can be tuned with various options. It is possible
to supply a confidence weight for each variable allowing for 
fine grained control on
which values should be adapted. It is also possible to use a mixed integer programming
solver, which is typically very fast. For more information see the reference manual
of \code{editrules}.

\subsection{Correction}
\subsubsection{Deterministic and deductive methods}
In some cases, the cause of errors in data can be determined with enough
certainty so that the solution is almost automatically known. In recent years,
several such methods have been developed and implemented in the
\code{deducorrect} package\cite{loo:2011}. In this section we give a quick
overview of the possibilities of \code{deducorrect}. 

To follow the examples load the deducorrect package.
<<result=FALSE>>=
library(deducorrect)
@

\subsubsection{Automated correction using deterministic rules}
Data cleaning often involves applying a lot of \emph{ad-hoc} transformations
and derivations of new variables. This may lead to large scripts, where
you select parts of the data, change some variables, select another part,
change some more variables, etc. When such scripts are neatly written and
commented, they can almost be treated as a log of the actions performed
by the analyst. However, as scripts get longer it is nicer to 
store the transformation rules separately and log which rule is executed
on what record. The \code{deducorrect} package offers functionality for 
this. Consider as an example the following (ficticious) dataset listing
the body length of some brothers.
<<>>=
(marx <- read.csv("files/marx.csv",stringsAsFactors=FALSE))
@
The task here is to standardize the lengths and express all of them in
meters. The obvious way would be to use the indexing techniques summarized
in Section \ref{sect:indexing}, which would look something like this.
<<>>=
marx_m <- marx
I <- marx$unit == "cm"
marx_m[I,'height'] <- marx$height[I]/100
I <- marx$unit == "inch"
marx_m[I,'inch'] <- marx$height[I]/39.37
I <- marx$unit == "ft"
marx_m[I,'ft'] <- marx$height[I]/3.28
marx_m$unit <- "m"
@
Such operations quickly become cumbersome. Of course, in this case one could
write a for-loop but that would hardly save any code. Moreover, if you want to
check afterwards which values have been converted and for what reason, there
will be a significant administrative overhead.

The deducorrect package takes all this overhead of your hands with the 
\code{correctionRules} functionality. For example, to perform the above 
task, one first specifies a file with simple correction rules as follows.
<<echo=FALSE>>=
read_chunk("files/conversions.txt")
@
<<conversions,eval=FALSE,tidy=FALSE>>=
@
With \code{deducorrect} we can read these rules, apply them to the data and
obtain a log of all actual changes as follows.
<<>>=
# read the conversion rules.
R <- correctionRules("files/conversions.txt")
R
@
\code{correctionRules} has parsed the rules and stored them in a \code{correctionRules}
object. We may now apply them to our data
<<>>=
cor <- correctWithRules(R,marx)
@
as simple as that. The returned value, \code{cor}, is a \code{list} containing the
corrected data
<<>>=
cor$corrected
@
but also a log of applied corrections.
<<>>=
cor$corrections[1:4]
@
The log lists for each row, what variable was changed, what the old value was
and what the new value is. Furthermore, the fifth column of
\code{cor\$corrections} shows the corrections that were applied (not shown
above for formatting reasons)
<<>>=
cor$corrections[5]
@
So here, with just two commands, the data is processed and all actions logged
in a \code{data.frame} which you can store or analyze. The rules that may be
applied with \code{deducorrect} are rules that can be executed
record-by-record. 

By design, there are some limitations to which rules can be applied with \code{correctWithRules}.
The processing rules should be executable record-by-record. That is, it is not permitted
to use functions line \code{mean} or \code{sd}. The symbols that may be used can be listed
as follows.
<<>>=
getOption('allowedSymbols')
@
When the rules are read by \code{correctionRules}, it checks whether any symbol
occurs that is not in the list of allowed symbols and returns an error message
when such a symbol is found. For example.
<<>>=
correctionRules(expression(x <- mean(x)))
@
%
Finally, it is currently not possible to add new variables using correctionrules
although such a feature will likely be added in the future.


\subsubsection{Correction of cases where the cause of error can be deduced}
When the data you are analyzing is generated by people rather than machines or
measurement devices, certain typical human-generated errors are likely to
occur. Given that data has to obey certain edit rules, the occurrence of such
errors can sometimes be detected from raw data with (almost) certainty.
Examples of errors that can be detected are typing errors in numbers (under linear
restrictions) rounding errors in numbers and sign errors or variable swaps\cite{scholtus:2011}.
The \code{deducorrect} package has a number of functions available that can correct
such errors. Below we give some examples, every time with just a single edit rule.
The functions can handle larger sets of edits however.

With \code{correctRoundings} deviations of the size of one or two measurement
units can be repaired. The function chooses randomly one variable to alter such that
the rule violation(s) are nullified while no new violations are generated.
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 100,y=101,z=200)
cor <- correctRounding(e,d)
cor$corrected
cor$corrections
@

The function \code{correctSigns} is able to detect and repair sign errors. 
It does this by trying combinations of variable swaps on variables that
occur in violated edits.
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 100, y = -100, z=200)
cor <- correctSigns(e,d)
cor$corrected
cor$corrections
@

Finally, the function \code{correctTypos} is capable of detecting and correcting
typographic errors in numbers. It does this by computing candidate solutions and
checking whether those candidates are less than a certain string distance (see Section 
\ref{sect:appstringmatching}) removed from the original.
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 123, y = 132, z = 246)
cor <- correctTypos(e,d)
cor$corrected
cor$corrections
@
Indeed, swapping the $3$ and the $2$ in $y=132$ solves the edit violation.


\begin{tip}{Tip}
Every \code{correct-} function in \code{deducorrect} is an object of class
\code{deducorrect}. When printed, it doesn't show the whole contents (the
corrected data and logging information) but a summary of what happened with
your data. A \code{deducorrect} object also contains data on timing, user,
and so on. See \code{?"deducorrect-object"} for a full explanation.
\end{tip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Deterministic imputation methods}

Under some circumstances, the rules that your data have to obey are
so restrictive that there's just a single solution for a missing value.
As an example, consider a record with variables listing the costs
for \emph{staff} \emph{cleaning}, \emph{housing} and the total \emph{total}.
We have the following rules.
\begin{equation}
\begin{array}{l}
  \textrm{\em staff} + \textrm{\em cleaning} + \textrm{\em housing} = \textrm{\em total}\\
  \textrm{\em staff} \geq 0\\
  \textrm{\em housing} \geq 0\\
  \textrm{\em cleaning} \geq 0
\end{array}
\end{equation}
In general, if one of the variables is missing the value can clearly be derived
by solving it for the first rule (providing that the solution doesn't violate
the last rule). However, there are other cases where unique solutions exist.
Suppose that we have $staff = total$. Assuming that these values are correct,
the only possible values for the other two variables  is $housing=cleaning=0$.
The \code{deducorrect} function \code{deduImpute} is able to recognize such
cases and compute the unique imputations.
<<tidy=FALSE>>=
E <- editmatrix(expression(
  staff + cleaning + housing == total,
  staff    >= 0,
  housing  >= 0,
  cleaning >= 0
))
dat <- data.frame(
  staff = c(100,100,100),
  housing = c(NA,50,NA),
  cleaning = c(NA,NA,NA),
  total = c(100,180,NA)
)
dat
cor <- deduImpute(E,dat)
cor$corrected
@
Note that \code{deduImpute} only imputes those values that can be derived
with absolute certainty (uniquely) from the rules. In the example, there are
many possible solutions fo impute the last record and hence \code{deduImpute}
leaves it untouched.

Similar situations exist for categorical data, and purely categorical data
is handled by \code{deduImpute} as well.
<<tidy=FALSE>>=
E <- editarray(expression(
  age %in% c("adult","under-aged"),
  driverslicense %in% c(TRUE, FALSE),
  if ( age == "under-aged" ) !driverslicense
))
dat <- data.frame(
  age = NA,
  driverslicense = TRUE
)
dat
cor <- deduImpute(E,dat)
cor$corrected
@
Here, \code{deduImpute} uses automated logic to derive from the conditional
rules that if someone has a drivers license, he has to be an adult.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model-based imputation}
\label{sect:mbimputation}
When deterministic or deductive corrections and imputations are not sufficient
to replace missing values, one may either correct for the missing data using
advances weight models or by estimating the actual missing values using
statistical models. There is a vast body of literature on imputation methods
and is goes beyond the scope of this tutorial to discuss them all. In stead, we
present in Table \ref{tab:imputation} an overview of packages that offer some
kind of imputation method and list them against a number of popular model-based
imputation methods. We note that the list of packages and methods are somewhat
arbitrary as they result from an investigation conducted at Statistics
Netherlands \cite{broek:2012} for internal purposes. Nevertheless we feel that
this overview is a quite useful place to start. See for example the paper by
Kalton and Kasprzyk\cite{kalton:1986} for an overview of established imputation
methods.  The packages \code{Amelia}, \code{deducorrect} and \code{mix} do not
implement any of the methods mentioned in the table. That is because
\code{Amelia} implements a multiple imputation method based on the assumption
of a conditional multinormal variable distribution. The \code{deducorrect}
package implements deductive and deterministic method, and we choose not to
call this model-based in this tutorial. The \code{mix} package implements a
Bayesian estimation method based on an Markov Chain Monte Carlo algorithm.

There is no one single best imputation method that works in all cases. The
imputation model of choice depends on what auxiliary information is available
and whether there are (multivariate) edit restrictions on the data to be
imputed. The availability of \R{} software for imputation under edit
restrictions is, to our best knowledge, limited. However, a viable strategy for
imputing numerical data is to first impute missing values without restrictions,
and then minimally adjust the imputed values so that the restrictions are
obeyed.  Separately, these methods are available in \R{}.

\begin{table}[!t]
\begin{adjustwidth}{-2cm}{}
\begin{threeparttable}
\caption{An overview of imputation functionality offered by some  \R{} packages.
reg: regression, rand: random, seq: sequential, NN: nearest neighbor, pmm:
predictive mean matching, kNN: $k$-nearest-neighbours, int: interpolation,
lo/no last observation carried forward / next observation carried backward,
LS: method of Little and Su.
}
\label{tab:imputation}
\begin{tabular}{|l|ccc|ccc|c|ccc|}
\hline
\multicolumn{1}{|l}{} & 
\multicolumn{3}{|c|}{Numeric} &
\multicolumn{3}{|c|}{Hot deck}&
\multicolumn{1}{|c|}{}&
\multicolumn{3}{|c|}{Longitudinal}\\

Package                             & 
\multicolumn{1}{|c}{mean}  & 
\multicolumn{1}{c}{ratio}  & 
\multicolumn{1}{c|}{reg.}   & 
\multicolumn{1}{c}{rand}  & 
\multicolumn{1}{c}{seq}   & 
\multicolumn{1}{c|}{pmm}   & 
\multicolumn{1}{c}{kNN}    & 
\multicolumn{1}{|c}{int}   & 
\multicolumn{1}{c}{lo/no}  & 
\multicolumn{1}{c|}{LS}     \\ \hline
\code{Amelia}\cite{honaker:2011}       &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{BaBoon}\cite{meinfelder:2011}    &\xmark & \xmark &\xmark           &\xmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{cat}\cite{harding:2012}          &\xmark & \xmark &\xmark           &\cmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\  
\code{deducorrect}\cite{loo:2011}      &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ \hline
\code{e1071}\cite{meyer:2012}          &\cmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{ForImp}                          &\cmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\cmark$^\ddagger$ &\xmark &\xmark & \xmark \\ 
\code{Hmisc}\cite{harrel:2013}         &\cmark & \xmark &\xmark           &\cmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{imputation}\cite{wong:2013}      &\cmark & \xmark &\cmark$^\dagger$ &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ \hline
\code{impute}\cite{hastie}             &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{mi}\cite{su:2011}                &\xmark & \xmark &\cmark$^*$       &\cmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{mice}\cite{buren:2011}           &\cmark & \xmark &\cmark$^*$       &\cmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{mix}\cite{schafer:2010}          &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ \hline
\code{norm}\cite{alvaro:2013}          &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{robCompositions}\cite{templ:2011}&\xmark & \xmark &\cmark$^*$       &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{rrcovNA}\cite{todorov:2012}      &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{StatMatch}\cite{orazio:2012}     &\xmark & \xmark &\xmark           &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ \hline
\code{VIM}\cite{templ:2012}            &\xmark & \xmark &\cmark$^*$       &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{yaImpute}\cite{crookston:2007}   &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{zoo}\cite{zeileis:2005}          &\cmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\cmark &\cmark & \xmark \\ 
\hline                                                                                                             
\end{tabular}
{\scriptsize ${^*}$Methods are ultimately based on some form of regression, but are more involved than simple linear regression.\\
${^\dagger}$Uses a non-informative auxiliary variable (row number).\\
${^\ddagger}$Uses nearest neighbor as part of a more involved imputation scheme.
}
\end{threeparttable}
\end{adjustwidth}
\end{table}
%
In the following subsections we discuss three types of imputation
models and give some pointers to how to implement them using \R{}.
The next section (\ref{sect:rspa}) is devoted to value adjustment.

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Basic numeric imputation models}
\label{sect:numericimputation}
Here, we distinguish three imputation models. The first is imputation of the mean:
\begin{equation}
\hat{x}_i = \bar{x},
\end{equation}
where the mean is taken over the observed values. The usability of this model
is limited since it obviously causes a bias in measures of spread, estimated
from the sample after imputation. However, in base \R{} it is implemented
simply enough.
<<eval=FALSE>>=
x[is.na(x)] <- mean(x,na.rm=TRUE)
@
In principle one can use other measures of centrality, and in fact, the \code{Hmisc}
package has a convenient wrapper function allowing you to specify what function is
used to compute imputed values from the non-missing. For example, imputation of 
the mean or median can be done as follows. 
<<eval=FALSE>>=
library(Hmisc)
x <- impute(x, fun=mean)     # mean imputation
x <- impute(x, fun=median)   # median imputation
@
An nice feature of the \code{impute} function is that the resulting
vector ``remembers'' what values were imputed. This information
may be requested with \code{is.imputed} as in the example below.
<<echo=FALSE,message=FALSE>>=
library(Hmisc)
@
<<>>=
x <- 1:5   # create a vector...
x[2] <- NA # ...with an empty value
x <- impute(x,mean)
x
is.imputed(x)
@
Note also that imputed values are printed with a post-fixed asterix.

The second imputation model we discuss is ratio imputation. Here, the
imputation estimate $\hat{x}_i$ is given by 
\begin{equation}
\hat{x}_i = \hat{R} y_i,
\end{equation}
Where $y_i$ is a covariate and $\hat{R}$ is an estimate of the average ratio
between $x$ and $y$. Often this will be given by the sum of observed $x$ values
divided by the sum of corresponding $y$ values although variants are possible.
Ratio imputation has the property that the estimated value equals $\hat{x}=0$
when $y=0$, which is in general not guaranteed in linear regression. Ratio
imputation may be a good model to use when restrictions like $x\geq 0$ and/or
$y\geq0$ apply. There is no package directly implementing ratio imputation,
unless it is regarded a special case of regression imputation. It is easily
implemented using plain \R{} though. Below, we suppose that \code{x} and \code{y}
are numeric vectors of equal length, \code{x} contains missing values and \code{y}
is complete.
<<eval=FALSE>>=
I <- is.na(x)
R <- sum(x[!I])/sum(y[!I])
x[I] <- R*y[I] 
@
Unfortunately, it is not possible to simply wrap the above in a function and
pass this to \code{HMisc}'s \code{impute} function. There seem to be no packages
that implement ratio imputation in a single function.


The third, and last numerical model we treat are (generalized) linear
regression models. In such models, missing values are imputed
using as follows
\begin{equation}
\hat{x}_i = \hat{\beta}_0 + \hat{\beta}_1y_{1,i} + \cdots + \hat{\beta}_ky_{k,i},
\end{equation}
where the $\hat{\beta}_0,\hat{\beta}_1\ldots\hat{\beta}_k$ are estimated linear
regression coefficients for each of the auxiliary variables
$y_1,y_2\ldots,y_k$. Estimating linear models is easy enough using \code{lm}
and \code{predict} in standard \R{}. Here, we use the built-in \code{iris}
data set as an example.
<<>>=
data(iris)
iris$Sepal.Length[1:10] <- NA
model <- lm(Sepal.Length ~ Sepal.Width + Petal.Width,data=iris)
I <- is.na(iris$Sepal.Length)
iris$Sepal.Length[I] <- predict(model, newdata=iris[I,])
@
Above, \code{lm} automatically omits rows with empty values and estimates the
model based on the remaining records. Next, we used \code{predict} to estimate
the missing values. 

It should be noted that although \code{Hmisc}, \code{VIM}, \code{mi} and
\code{mice} all implement imputation methods that ultimately use some form of
regression, they do not include a simple interface for the case described
above.  The more advanced methods in those packages are aimed to be more
precise and/or robust against outliers than standard (generalized) linear
regression.  Moreover, both \code{mice} and \code{mi} implement methods for
multiple imputation, which allows for estimation of the imputation variance.
Such methods are beyond the scope of the current text. The \code{imputation}
package has a function \code{lmImpute} but is uses a the row number as
auxiliary variable which limits the practical usability.


\subsubsection{Hot deck imputation}
\label{sect:hotdeck}
In hot deck imputation, missing values are imputed by copying values from
similar records in the same dataset. Or, in notation:
\begin{equation}
\hat{x}_i = x_j, 
\end{equation}
where $x_j$ is taken from the observed values.  Hot-deck imputation can be
applied to numerical as well as categorical data but is only viable when enough
donor records are available. 

The main question in hot-deck imputation is
how to choose the replacement value $x_j$ from the observed values.  Here, we
shortly discuss four well-known flavors.

In \emph{random} hot-deck imputation, a value is chosen randomly, and uniformly
from the same data set. When meaningful, random hot deck methods can be
applied per stratum. For example, one may apply random hot-deck imputation of
body height for male and female respondents separately. Random hot-deck on a
single vector can be applied with the \code{impute} function of the
\code{Hmisc} package.
<<>>=
data(women)
# add some missingess
height <- women$height
height[c(6,9)] <- NA
height
(height <- impute(height,"random"))
@
Note that the outcome of this imputation is very likely tot be different each
time it is executed. If you want the results to be random, but repeatable (e.g.
for testing purposes) you can use \code{set.seed(<a number>)} prior to calling
\code{impute}. 

In \emph{sequential} hot deck imputation, the vector containing missing values is
sorted according to one or more auxiliary variables so that records that have
similar auxiliaries occur sequentially in the \code{data.frame}.  Next, each
missing value is imputed with the value from the first following record that
has an observed value. There seems to be no package that implements the
sequential hot deck technique. However, given a vector that is sorted, the
following function offers some basic functionality.
<<tidy=FALSE>>=
# x    :  vector to be imputed
# last : value to use if last value of x is empty
seqImpute <- function(x,last){
  n <- length(x)
  x <- c(x,last)
  i <- is.na(x)
  while(any(i)){
    x[i] <- x[which(i) + 1]
    i <- is.na(x)
  }
  x[1:n]
}
@
We will use this function in exercise \ref{ex:seqimpute}. 


\emph{Predictive mean matching} (pmm) is a form of nearest-neighbor hot-deck
imputation with a specific distance function. In predictive mean matching one
first estimates values for empty fields, for example with a regression method.
Empty values are then imputed with observed values which are the most similar
to the predicted values. Predictive mean matching imputation has been
implemented in several packages (\code{mi}, \code{mice}, \code{BaBoon}) but
each time either for a specific type of data and/or using a specific prediction
model (\emph{e.g.} \code{Bayesglm}). Moreover, the packages known to us always
use pmm as a part of a more elaborate multiple imputation scheme, which we
deemed out of scope for this tutorial.

\subsubsection{kNN-imputation}
In $k$ \emph{nearest neighbor} imputation one defines a distance
function $d(i,j)$ that computes a measure of dissimilarity between records. A
missing value is then imputed by finding first the $k$ records nearest to the
record with one or more missing values. Next, a value is chosen from or
computed out of the $k$ nearest neighbors. In the case where a value is picked
from the $k$ nearest neighbors, kNN-imputation is a form of hot-deck imputation.

The \code{VIM} package contains a function called \code{kNN} that uses Gowers
distance\cite{gower:1971} to determine the $k$ nearest neighbors. Gower's
distance between two records labeled $i$ and $j$ is defined as
\begin{equation}
d_{\sf g}(i,j) \frac{\sum_k w_{ijk} d_k(i,j)}{\sum_{k}w_{ijk}},
\end{equation}
where the sum runs over all variables in the record and $d_k(i,j)$
is the distance between the value of variable $k$ in record $i$ and record $j$.
For categorical variables, $d_k(i,j) = 0$ when the value for the $k'th$ variable
is the same in record $i$ and $j$ and $1$ otherwise. For numerical variables
the distance is given by $1-(x_i-x_j)/(\max(x)-\min(x))$. The weight $w_{ijk}=0$
when the $k$'th variable is missing in record $i$ or record $j$ and otherwise
$1$.

Here is an example of \code{kNN}.
<<message=FALSE>>=
library(VIM)
data(iris)
n <- nrow(iris)
# provide some empty values (10 in each row, randomly)
for ( i in 1:ncol(iris) ){
  iris[sample(1:n,10,replace=FALSE),i] <- NA
}
iris2 <- kNN(iris)
@
The \code{kNN} function determines the $k$ (default: 5) nearest neighbors of a
record with missing values. For numerical variables the median of the $k$
nearest neighbors is used as imputation value, for categorical variables the
category that most often occurs in the $k$ nearest neighbors is used. Both the
these functions may be replaced by the user. Finally, \code{kNN} prints (negatively)
the time it took to complete the imputation. We encourage the reader to execute
the above code and experiment further. 



\subsection{Minimal value adjustment}
\label{sect:rspa}
Once missing numerical values have been adequately imputed, there is a good
chance that the resulting records violate a number of edit rules.  The obvious
reason is that there are not many methods that can perform imputation under
arbitrary edit restrictions. One viable option is therefore to minimally
adjust imputed values such that after adjustment the record passes every edit
check within a certain tolerance. 

We need to specify more clearly what \emph{minimal} adjustment means here.
The \code{rspa} package\cite{loo:2012} is able to take a numerical record
$\bs{x}^0$ and replace it with a record $\bs{x}$, such that
the weighted Euclidean distance
\begin{equation}
\sum_iw_i(x_i-x_i^{0})^2,
\end{equation}
is minimized and $\bs{x}$ obeys a given set of (in)equality restrictions
\begin{equation}
\bs{Ax} \leq \bs{b}.
\end{equation}

As practical example consider the following script.
<<>>=
library(editrules)
library(rspa)
E <- editmatrix(expression(
  x + y == z,
  x >= 0,
  y >= 0
))
d <- data.frame(x = 10, y=10, z=21)
d1 <- adjustRecords(E,d)
d1$adjusted
@
The function \code{adjustRecords} adjusts every record minimally to obey the restrictions
in \code{E}. 
Indeed, we may check that the adjusted data now obeys every rule within \code{adjustRecords}'
default tolerance of $0.01$.
<<>>=
violatedEdits(E,d1$adjusted,tol=0.01)
@
By default, all variables in the input \code{data.frame} are adjusted with
equal amounts. However, suppose that $z$ is an imputed value while $x$ and $y$
are observed, original values. We can tell \code{adjustRecords} to only adjust
\code{z} as follows.
<<>>=
A <- array(c(x=FALSE,y=FALSE,z=TRUE),dim=c(1,3))
A
d2 <- adjustRecords(E,d, adjust=A)
d2$adjusted
@



\subsection*{Exercises}
In the following exercises we are going to use the \code{dirty\_iris} dataset.
You can download this dataset from 
\begin{quote}
\end{quote}


\begin{exercise}
Reading and checking.
\begin{subex}
  \item View the file in a text-editor to determine its format and read the file into \R{}. 
    Make sure that strings are not converted to factor.
  \item Calculate the number and percentage of observations that are complete.
  \item Does the data contain other special values? If it does, replace them with \code{NA}
  \item Besides missing values, the data set contains errors. We have the following background knowledge:
  \begin{itemize}
    \item \code{Species} should be one of the following values: \code{setosa}, \code{versicolor} or \code{virginica}.
    \item All measured numerical properties of an iris should be positive.
    \item The petal length of an iris is at least 2 times its petal width
    \item The sepal length of an iris cannot exceed 30 cm.
    \item The sepals of an iris are longer than its petals.
  \end{itemize}
    Define these rules in a separate text file and read them into \R{} using \code{editfile} (package \code{editrules}).
    Print the resulting constraint object.
    
  \item Determine how often each rules is broken (\code{violatedEdits}). Also summarize and plot the result.
  \item What percentage of the data has no errors?
  \item Find out which observations have too long petals using the result of \code{violatedEdits}. 
  \item Find outliers in sepal length. Look at the observation. Any ideas what might have happened?
\end{subex}
\end{exercise}

\begin{exercise}
\label{ex:seqimpute}
Correcting and imputing

\end{exercise}


