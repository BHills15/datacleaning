% Ideas
% - Define 
% - univariate, multivariate rules
% - rule checking, summarizing, plotting
% - soft rules and outlier detection
% - deterministic corrections with deducorrect
% - 
%

\section{From technically correct data to consistent data}
\label{sect:inputtomicro}
With \emph{consistent data} we mean that the data obeys all expectations you
have based on subject matter knowledge. Such expectations are usually expressed
in terms of rules wich may be interpreted as being obligatory (\emph{hard
rules}), or indicative (\emph{soft rules}). 





\subsection{Checking record-wise rules with the \code{editrules} package.}
The \code{editrules} package allows one to define rules on categorical,
numerical or mixed data sets which each record must obey. Restrictions can
be defined by typing them in a file and reading them into \R{} with
the \code{editfile} function.

<<cache=FALSE,echo=FALSE>>=
read_chunk('edits.txt')
@
<<rules,eval=FALSE>>=
@

\subsection{Deterministic and deductive methods}
In some cases, the cause of errers in data can be determined with enough
certainty so that the solution is almost automatically known. In recent years,
several such methods have been developed and implemented in the
\code{deducorrect} package\cite{loo:2011}. In this section we give a quick
overview of the possibilities of \code{deducorrect}. 

To follow the examples load the deducorrect package.
<<result=FALSE>>=
library(deducorrect)
@


\subsubsection{Automated correction using deterministic rules}
Data cleaning often involves applying a lot of \emph{ad-hoc} transformations
and derivations of new variables. This may lead to large scripts, where
you selects parts of the data, change some variables, select another part,
change some more variables, etc. When such scripts are neatly written and
commented, they can almost be treated as a log of the actions performed
by the analist. However, as scripts get longer it is nicer to 
store the transformation rules separately and log which rule is executed
on what record. The \code{deducorrect} package offers functionality for 
this. Consider as an example the following (fantasized) dataset listing
the body lengtgh of some brothers.
<<>>=
(marx <- read.csv("files/marx.csv",stringsAsFactors=FALSE))
@
The task here is to standardize the lengths and express all of them in
meters. The obvious way would be to use the indexing techniques summarized
in Section \ref{sect:indexing}, which would look something like this.
<<>>=
marx_m <- marx
I <- marx$unit == "cm"
marx_m[I,'height'] <- marx$height[I]/100
I <- marx$unit == "inch"
marx_m[I,'inch'] <- marx$height[I]/39.37
I <- marx$unit == "ft"
marx_m[I,'ft'] <- marx$height[I]/3.28
marx_m$unit <- "m"
@
Such operations quickly become cumbersome. Of course, in this case one could
write a for-loop but that would hardly save any code. Moreover, if you want to
check afterwards which values have been converted and for what reason, there
will be a significant administrative overhead.

The deducorrect package takes all this overhead of your hands with the 
\code{correctionRules} functionality. For example, to perform the above 
task, one first specifies a file with simple correction rules as follows.
<<echo=FALSE>>=
read_chunk("files/conversions.txt")
@
<<conversions,eval=FALSE,tidy=FALSE>>=
@
With \code{deducorrect} we can read these rules, apply them to the data and
obtain a log of all actual changes as follows.
<<>>=
# read the conversion rules.
R <- correctionRules("files/conversions.txt")
R
@
\code{correctionRules} has parsed the rules and stored them in a \code{correctionRules}
object. We may now apply them to our data
<<>>=
cor <- correctWithRules(R,marx)
@
as simple as that. The returned value, \code{cor}, is a \code{list} containing the
corrected data
<<>>=
cor$corrected
@
but also a log of applied corrections.
<<>>=
cor$corrections[1:4]
@
The log lists for each row, what variable was changed, what the old value was
and what the new value is. Furthermore, the fifth column of
\code{cor\$corrections} shows the corrections that were applied (not shown
above for formatting reasons)
<<>>=
cor$corrections[5]
@
So here, with just two commands, the data is processed and all actions logged
in a \code{data.frame} which you can store or analyze. The rules that may be
applied with \code{deducorrect} are rules that can be executed
record-by-record. 

By design, there are some limitations to which rules can be applied with \code{correctWithRules}.
The processing rules should be executable record-by-record. That is, it is not permitted
to use functions line \code{mean} or \code{sd}. The symbols that may be used can be listed
as follows.
<<>>=
getOption('allowedSymbols')
@
When the rules are read by \code{correctionRules}, it checks wether any symbol
occurs that is not in the list of allowed symbols and returns an error message
when such a symbol is found. For example.
<<>>=
correctionRules(expression(x <- mean(x)))
@
%
Finally, it is currently not possible to add new variables using correctionrules
although such afeature will likely be added in the future.


\subsubsection{Correction of cases where the cause of error can be deduced}
When the data you are analyzing is generated by people rather than machines or
measurement devices, certain typical human-generated errors are likely to
occur. Given that data has to obey certain edit rules, the occurrence of such
errors can sometimes be detected from raw data with (almost) certainty.
Examples of errors that can be detected are typing errors in numbers (under linear
restrictions) rounding errors in numbers and sign errors or variable swaps\cite{scholtus:2011}.
The \code{deducorrect} package has a number of functions available that can correct
such errors. Below we give some examples, every time with just a single edit rule.
The functions can handle larger sets of edits however.

With \code{correctRoundings} deviations of the size of one or two measurement
units can be repaired. The function chooses randomly one variable to alter such that
the rule violation(s) are nullified while no new violations are generated.
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 100,y=101,z=200)
cor <- correctRounding(e,d)
cor$corrected
cor$corrections
@

The function \code{correctSigns} is able to detect and repair sign errors. 
It does this by trying combinations of variable swaps on variables that
occur in violated edits.
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 100, y = -100, z=200)
cor <- correctSigns(e,d)
cor$corrected
cor$corrections
@

Finally, the function \code{correctTypos} is capable of detecting and correcting
typographic errors in numbers. It does this by computing candidate solutions and
checking whether those candidates are less than a certain string distance (see Section 
\ref{sect:appstringmatching}) removed from the original.
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 123, y = 132, z = 246)
cor <- correctTypos(e,d)
cor$corrected
cor$corrections
@
Indeed, swapping the $3$ and the $2$ in $y=132$ solves the edit violation.


\begin{tip}{Tip}
Every \code{correct-} function in \code{deducorrect} is an object of class
\code{deducorrect}. When printed, it doesn't show the whole contents (the
corrected data and logging information) but a summary of what happened with
your data. A \code{deducorrect} object also contains data on timing, user,
and so on. See \code{?"deducorrect-object"} for a full explanation.
\end{tip}


\subsection{Imputation of missing data}

\subsubsection{Deriving imputation values with certainty}
Under some circumstances, the rules that your data have to obey are
so restrictive that there's just a single solution for a missing value.
As an example, consider a record with variables listing the costs
for \emph{staff} \emph{cleaning}, \emph{housing} and the total \emph{total}.
We have the following rules.
\begin{equation}
\begin{array}{l}
  \textrm{\em staff} + \textrm{\em cleaning} + \textrm{\em housing} = \textrm{\em total}\\
  \textrm{\em staff} \geq 0\\
  \textrm{\em housing} \geq 0\\
  \textrm{\em cleaning} \geq 0
\end{array}
\end{equation}
In general, if one of the variables is missing the value can clearly be derived
by solving it for the first rule (providing that the solution doesn't violate
the last rule). However, there are other cases where unique solutions exist.
Suppose that we have $staff = total$. Assuming that these values are correct,
the only possible values for the other two variables  is $housing=cleaning=0$.
The \code{deducorrect} function \code{deduImpute} is able to recognize such
cases and compute the unique imputations.
<<tidy=FALSE>>=
E <- editmatrix(expression(
  staff + cleaning + housing == total,
  staff    >= 0,
  housing  >= 0,
  cleaning >= 0
))
dat <- data.frame(
  staff = c(100,100,100),
  housing = c(NA,50,NA),
  cleaning = c(NA,NA,NA),
  total = c(100,180,NA)
)
dat
cor <- deduImpute(E,dat)
cor$corrected
@
Note that \code{deduImpute} only imputes those values that can be derived
with absolute certainty (uniquely) from the rules. In the example, there are
many possible solutions fo impute the last record and hence \code{deduImpute}
leaves it untouched.

Similar sitiations exist for categorical data, and purely categorical data
is handled by \code{deduImpute} as well.
<<tidy=FALSE>>=
E <- editarray(expression(
  age %in% c("adult","under-aged"),
  driverslicense %in% c(TRUE, FALSE),
  if ( age == "under-aged" ) !driverslicense
))
dat <- data.frame(
  age = NA,
  driverslicense = TRUE
)
dat
cor <- deduImpute(E,dat)
cor$corrected
@
Here, \code{deduImpute} uses automated logic to derive from the conditional
rules that if someone has a driverslicense, he has to be an adult.




\subsection{Adjusting values with \code{rspa}}


