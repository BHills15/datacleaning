% Ideas
% - Define 
% - univariate, multivariate rules
% - rule checking, summarizing, plotting
% - soft rules and outlier detection
% - deterministic corrections with deducorrect
% - 
%

\section{From technically correct data to consistent data}
\label{sect:inputtomicro}
With \emph{consistent data} 
we mean that the data obeys all expectations you
have based on subject matter knowledge. Such expectations are usually expressed
in terms of rules wich may be interpreted as being obligatory (\emph{hard
rules}), or indicative (\emph{soft rules}). 



\subsection{Checking record-wise rules with the \code{editrules} package.}
The \code{editrules} package allows one to define rules on categorical,
numerical or mixed data sets which each record must obey. Restrictions can
be defined by typing them in a file and reading them into \R{} with
the \code{editfile} function.

<<cache=FALSE,echo=FALSE>>=
read_chunk('edits.txt')
@
<<rules,eval=FALSE>>=
@

\subsection{Deterministic and deductive methods}
In some cases, the cause of errers in data can be determined with enough
certainty so that the solution is almost automatically known. In recent years,
several such methods have been developed and implemented in the
\code{deducorrect} package\cite{loo:2011}. In this section we give a quick
overview of the possibilities of \code{deducorrect}. 

To follow the examples load the deducorrect package.
<<result=FALSE>>=
library(deducorrect)
@


\subsubsection{Automated correction using deterministic rules}
Data cleaning often involves applying a lot of \emph{ad-hoc} transformations
and derivations of new variables. This may lead to large scripts, where
you selects parts of the data, change some variables, select another part,
change some more variables, etc. When such scripts are neatly written and
commented, they can almost be treated as a log of the actions performed
by the analist. However, as scripts get longer it is nicer to 
store the transformation rules separately and log which rule is executed
on what record. The \code{deducorrect} package offers functionality for 
this. Consider as an example the following (fantasized) dataset listing
the body lengtgh of some brothers.
<<>>=
(marx <- read.csv("files/marx.csv",stringsAsFactors=FALSE))
@
The task here is to standardize the lengths and express all of them in
meters. The obvious way would be to use the indexing techniques summarized
in Section \ref{sect:indexing}, which would look something like this.
<<>>=
marx_m <- marx
I <- marx$unit == "cm"
marx_m[I,'height'] <- marx$height[I]/100
I <- marx$unit == "inch"
marx_m[I,'inch'] <- marx$height[I]/39.37
I <- marx$unit == "ft"
marx_m[I,'ft'] <- marx$height[I]/3.28
marx_m$unit <- "m"
@
Such operations quickly become cumbersome. Of course, in this case one could
write a for-loop but that would hardly save any code. Moreover, if you want to
check afterwards which values have been converted and for what reason, there
will be a significant administrative overhead.

The deducorrect package takes all this overhead of your hands with the 
\code{correctionRules} functionality. For example, to perform the above 
task, one first specifies a file with simple correction rules as follows.
<<echo=FALSE>>=
read_chunk("files/conversions.txt")
@
<<conversions,eval=FALSE,tidy=FALSE>>=
@
With \code{deducorrect} we can read these rules, apply them to the data and
obtain a log of all actual changes as follows.
<<>>=
# read the conversion rules.
R <- correctionRules("files/conversions.txt")
R
@
\code{correctionRules} has parsed the rules and stored them in a \code{correctionRules}
object. We may now apply them to our data
<<>>=
cor <- correctWithRules(R,marx)
@
as simple as that. The returned value, \code{cor}, is a \code{list} containing the
corrected data
<<>>=
cor$corrected
@
but also a log of applied corrections.
<<>>=
cor$corrections[1:4]
@
The log lists for each row, what variable was changed, what the old value was
and what the new value is. Furthermore, the fifth column of
\code{cor\$corrections} shows the corrections that were applied (not shown
above for formatting reasons)
<<>>=
cor$corrections[5]
@
So here, with just two commands, the data is processed and all actions logged
in a \code{data.frame} which you can store or analyze. The rules that may be
applied with \code{deducorrect} are rules that can be executed
record-by-record. 

By design, there are some limitations to which rules can be applied with \code{correctWithRules}.
The processing rules should be executable record-by-record. That is, it is not permitted
to use functions line \code{mean} or \code{sd}. The symbols that may be used can be listed
as follows.
<<>>=
getOption('allowedSymbols')
@
When the rules are read by \code{correctionRules}, it checks wether any symbol
occurs that is not in the list of allowed symbols and returns an error message
when such a symbol is found. For example.
<<>>=
correctionRules(expression(x <- mean(x)))
@
%
Finally, it is currently not possible to add new variables using correctionrules
although such afeature will likely be added in the future.


\subsubsection{Correction of cases where the cause of error can be deduced}
When the data you are analyzing is generated by people rather than machines or
measurement devices, certain typical human-generated errors are likely to
occur. Given that data has to obey certain edit rules, the occurrence of such
errors can sometimes be detected from raw data with (almost) certainty.
Examples of errors that can be detected are typing errors in numbers (under linear
restrictions) rounding errors in numbers and sign errors or variable swaps\cite{scholtus:2011}.
The \code{deducorrect} package has a number of functions available that can correct
such errors. Below we give some examples, every time with just a single edit rule.
The functions can handle larger sets of edits however.

With \code{correctRoundings} deviations of the size of one or two measurement
units can be repaired. The function chooses randomly one variable to alter such that
the rule violation(s) are nullified while no new violations are generated.
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 100,y=101,z=200)
cor <- correctRounding(e,d)
cor$corrected
cor$corrections
@

The function \code{correctSigns} is able to detect and repair sign errors. 
It does this by trying combinations of variable swaps on variables that
occur in violated edits.
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 100, y = -100, z=200)
cor <- correctSigns(e,d)
cor$corrected
cor$corrections
@

Finally, the function \code{correctTypos} is capable of detecting and correcting
typographic errors in numbers. It does this by computing candidate solutions and
checking whether those candidates are less than a certain string distance (see Section 
\ref{sect:appstringmatching}) removed from the original.
<<>>=
e <- editmatrix("x + y == z")
d <- data.frame(x = 123, y = 132, z = 246)
cor <- correctTypos(e,d)
cor$corrected
cor$corrections
@
Indeed, swapping the $3$ and the $2$ in $y=132$ solves the edit violation.


\begin{tip}{Tip}
Every \code{correct-} function in \code{deducorrect} is an object of class
\code{deducorrect}. When printed, it doesn't show the whole contents (the
corrected data and logging information) but a summary of what happened with
your data. A \code{deducorrect} object also contains data on timing, user,
and so on. See \code{?"deducorrect-object"} for a full explanation.
\end{tip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Imputation of missing data}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Deriving imputation values with (near) certainty}
Under some circumstances, the rules that your data have to obey are
so restrictive that there's just a single solution for a missing value.
As an example, consider a record with variables listing the costs
for \emph{staff} \emph{cleaning}, \emph{housing} and the total \emph{total}.
We have the following rules.
\begin{equation}
\begin{array}{l}
  \textrm{\em staff} + \textrm{\em cleaning} + \textrm{\em housing} = \textrm{\em total}\\
  \textrm{\em staff} \geq 0\\
  \textrm{\em housing} \geq 0\\
  \textrm{\em cleaning} \geq 0
\end{array}
\end{equation}
In general, if one of the variables is missing the value can clearly be derived
by solving it for the first rule (providing that the solution doesn't violate
the last rule). However, there are other cases where unique solutions exist.
Suppose that we have $staff = total$. Assuming that these values are correct,
the only possible values for the other two variables  is $housing=cleaning=0$.
The \code{deducorrect} function \code{deduImpute} is able to recognize such
cases and compute the unique imputations.
<<tidy=FALSE>>=
E <- editmatrix(expression(
  staff + cleaning + housing == total,
  staff    >= 0,
  housing  >= 0,
  cleaning >= 0
))
dat <- data.frame(
  staff = c(100,100,100),
  housing = c(NA,50,NA),
  cleaning = c(NA,NA,NA),
  total = c(100,180,NA)
)
dat
cor <- deduImpute(E,dat)
cor$corrected
@
Note that \code{deduImpute} only imputes those values that can be derived
with absolute certainty (uniquely) from the rules. In the example, there are
many possible solutions fo impute the last record and hence \code{deduImpute}
leaves it untouched.

Similar sitiations exist for categorical data, and purely categorical data
is handled by \code{deduImpute} as well.
<<tidy=FALSE>>=
E <- editarray(expression(
  age %in% c("adult","under-aged"),
  driverslicense %in% c(TRUE, FALSE),
  if ( age == "under-aged" ) !driverslicense
))
dat <- data.frame(
  age = NA,
  driverslicense = TRUE
)
dat
cor <- deduImpute(E,dat)
cor$corrected
@
Here, \code{deduImpute} uses automated logic to derive from the conditional
rules that if someone has a driverslicense, he has to be an adult.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model-based imputation}
\label{sect:mbimputation}
When deterministic or deductive corrections and imputations are not sufficient
to replace missing values, one may either correct for the missing data using
advances weight models or by estimating the actual missing values using
statistical models. There is a vast body of literature on imputation methods
and is goes beyond the scope of this tutorial to discuss them all. In stead, we
present in Table \ref{tab:imputation} an overview of packages that offer some
kind of imputation method and list them against a number of popular model-based
imputation methods. We note that the list of packages and methods are somewhat
arbitrary as they result from an investigation conducted at Statistics
Netherlands \cite{broek:2012} for internal purposes. Nevertheless we feel that
this overview is a quite useful place to start. The packages \code{Amelia},
\code{deducorrect} and \code{mix} do not implement any of the methods mentioned
in the table. That is because \code{Amelia} implements a multiple imputation
method based on the assumption of a conditional multinormal variable
distribution. The \code{deducorrect} package implements deductive and
deterministic method, and we choose not to call this model-based in this
tutorial. The \code{mix} package implements a Bayesian estimation method based
on an Markov Chain Monte Carlo algorithm.

There is no one single best imputation method that works in all cases. The
imputation model of choice depends on what auxiliary information is available
and whether there are (multivariate) edit restrictions on the data to be
imputed. The availability of \R{} software for imputation under edit
restrictions is, to our best knowledge, limited. However, a viable strategy for
imputing numerical data is to first impute missing values without restrictions,
and then minimally adjust the imputed values so that the restrictions are
obeyed.  Separately, these methods are available in \R{}.

\begin{table}[!t]
\begin{adjustwidth}{-2cm}{}
\begin{threeparttable}
\caption{An overview of imputation funcitonality offered by some  \R{} packages.
reg: regression, rand: random, seq: sequential, NN: nearest neighbour, pmm:
predictive mean matching, kNN: $K$-nearest-neighbours, int: interpolation,
lo/no last observation carried forward / next observation carried backward,
LS: method of Little and Su.
}
\label{tab:imputation}
\begin{tabular}{|l|ccc|cccc|c|ccc|}
\hline
\multicolumn{1}{|l}{} & 
\multicolumn{3}{|c|}{Numeric} &
\multicolumn{4}{|c|}{Hot deck}&
\multicolumn{1}{|c|}{}&
\multicolumn{3}{|c|}{Longitudinal}\\

Package                             & 
\multicolumn{1}{|c}{mean}  & 
\multicolumn{1}{c}{ratio}  & 
\multicolumn{1}{c|}{reg.}   & 
\multicolumn{1}{c}{rand}  & 
\multicolumn{1}{c}{seq}   & 
\multicolumn{1}{c}{NN}    & 
\multicolumn{1}{c|}{pmm}   & 
\multicolumn{1}{c}{kNN}   & 
\multicolumn{1}{|c}{int}   & 
\multicolumn{1}{c}{lo/no}  & 
\multicolumn{1}{c|}{LS}     \\ \hline
\code{Amelia}\cite{honaker:2011}    &\xmark & \xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{BaBoon}\cite{meinfelder:2011} &\xmark & \xmark &\xmark &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{cat}\cite{harding:2012}       &\xmark & \xmark &\xmark &\cmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\  
\code{deducorrect}\cite{loo:2011}   &\xmark & \xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ \hline
\code{e1071}\cite{meyer:2012}       &\cmark & \xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{ForImp}                       &\cmark & \xmark &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{Hmisc}\cite{harrel:2013}      &\cmark & \xmark &\xmark &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{imputation}\cite{wong:2013}   &\xmark & \xmark &\xmark &\xmark &\xmark &\cmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ \hline
\code{impute}\cite{hastie}          &\xmark & \xmark &\xmark &\xmark &\xmark &\cmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{mi}\cite{su:2011}             &\xmark & \xmark &\cmark &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{mice}\cite{buren:2011}        &\cmark & \xmark &\cmark &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{mix}\cite{schafer:2010}       &\xmark & \xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ \hline
\code{norm}\cite{alvaro:2013}       &\xmark & \xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{robCompositions}\cite{templ:2011}&\xmark & \xmark &\cmark &\cmark &\xmark &\cmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{rrcovNA}\cite{todorov:2012}      &\xmark & \xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
\code{StatMatch}\cite{orazio:2012}     &\xmark & \xmark &\xmark &\cmark &\xmark &\cmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ \hline
\code{VIM}\cite{templ:2012}            &\xmark & \xmark &\cmark &\cmark &\xmark &\cmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{yaImpute}\cite{crookston:2007}   &\xmark & \xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
\code{zoo}\cite{zeileis:2005}          &\cmark & \xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark &\cmark &\cmark & \xmark \\ 
\hline                                                                                                             
\end{tabular}
\end{threeparttable}
\end{adjustwidth}
\end{table}
%
In the following subsections we discuss three types of imputation
models and give some pointers to how to implement them using \R{}.
The next section (\ref{sect:rspa}) is devoted to value adjustment.

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Basic numeric imputation models}
\label{sect:numericimputation}
Here, we distinguish three imputation models. The first is imputation of the mean:
\begin{equation}
\hat{x}_i = \bar{x},
\end{equation}
where the mean is taken over the observed values. The usability of this model
is limited since it obviously causes a bias in measures of spread, estimated
from the sample after imputation. However, in base \R{} it is implemented
simply enough.
<<eval=FALSE>>=
x[is.na(x)] <- mean(x,na.rm=TRUE)
@
In principle one can use other measures of centrality, and in fact, the \code{Hmisc}
package has a convenient wrapper function allowing you to specify what function is
used to compute imputed values from the non-missing. For example, imputation of 
the mean or median can be done as follows. 
<<eval=FALSE>>=
library(Hmisc)
x <- impute(x, fun=mean)     # mean imputation
x <- impute(x, fun=median)   # median imputation
@
An nice feature of the \code{impute} function is that the resulting
vector ``remembers'' what values were imputed. This information
may be requested with \code{is.imputed} as in the example below.
<<echo=FALSE,message=FALSE>>=
library(Hmisc)
@
<<>>=
x <- 1:5   # create a vector...
x[2] <- NA # ...with an empty value
x <- impute(x,mean)
x
is.imputed(x)
@
Note also that imputed values are printed with a post-fixed asterix.

The second imputation model we discuss is ratio imputation. Here, the
imputation estimate $\hat{x}_i$ is given by 
\begin{equation}
\hat{x}_i = \hat{R} y_i,
\end{equation}
Where $y_i$ is a covariate and $\hat{R}$ is an estimate of the average ratio
between $x$ and $y$. Often this will be given by the sum of observed $x$ values
divided by the sum of corresponding $y$ values although variants are possible.
Ratio imputation has the property that the estimated value equals $\hat{x}=0$
when $y=0$, which is in general not guaranteed in linear regression. Ratio
imputation may be a good model to use when restrictions like $x\geq 0$ and/or
$y\geq0$ apply. There is no package directly implementing ratio imputation,
unless it is regarded a special case of regression imputation. It is easily
implemented using plain \R{} though. Below, we suppose that \code{x} and \code{y}
are numeric vectors of equal length, \code{x} contains missing values and \code{y}
is complete.
<<eval=FALSE>>=
I <- is.na(x)
R <- sum(x[!I])/sum(y[!I])
x[I] <- R*y[I] 
@
Unfortunately, it is not possible to simply wrap the above in a function and
pass this to \code{HMisc}'s \code{impute} function. There seem to be no packages
that implement ratio imputation in a single function.


The third, and last numerical model we treat are (generalized) linear
regression models. In such models, missing values are imputed
using as follows
\begin{equation}
\hat{x}_i = \hat{\beta}_0 + \hat{\beta}_1y_{1,i} + \ldots + \hat{\beta}_ky_{k,i},
\end{equation}
where the $\hat{\beta}_0,\hat{\beta}_1\ldots\hat{\beta}_k$ are estimated linear
regression coefficients for each of the auxiliary variables
$y_1,y_2\ldots,y_k$. Estimating linear models is easy enough using \code{lm}
and \code{predict} in standard \R{}. Here, we use the built-in \code{iris}
data set as an example.
<<>>=
data(iris)
iris$Sepal.Length[1:10] <- NA
model <- lm(Sepal.Length ~ Sepal.Width + Petal.Width,data=iris)
I <- is.na(iris$Sepal.Length)
iris$Sepal.Length[I] <- predict(model, newdata=iris[I,])
@
Above, \code{lm} automatically omits rows with empty values and estimates the
model based on the remaining records. Next, we used \code{predict} to estimate
the missing values. 

It should be noted that although \code{Hmisc}, \code{VIM}, \code{mi} and
\code{mice} all implement imputation methods that ultimately use some form of
regression, none of them include the simple case described above.


\subsubsection{Hot deck imputation}
\label{sect:hotdeck}



\subsubsection{Longitudinal imputation}
\label{sect:logitudinal}




\subsection{Adjusting values with \code{rspa}}
\label{sect:rspa}

