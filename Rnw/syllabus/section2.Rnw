
\section{From raw data to technically correct data}
\label{sect:rawtoinput}

% Some ideas: [x] = todo,[-]= in progress, [v] = finished, [c] = cancelled. 
% 
% -[v]  Character encoding issues
% -[x]  regels overslaan, gebruiken van readLines
% -[x]  regels overslaan, gebruiken van scan
% -[x]  Special values, and how are they stored (NA, NULL,...)
% -[x]  labels van categoriale variabelen
% -[v]  spelfouten in tekstvariabelen, normaliseren
% -[x]  indexing skills
% -[x]  factors, relevel
% -[-]  exercises

\subsection{Technically correct}

We use the following simple definition for {\em technically correct} in \R{}:
\begin{itemize}
\item The data to be analyzed are stored in a \code{data.frame} with suitable columns names.
\item Each column of the \code{data.frame} is of correct \R{} type that is needed for analysis.
\end{itemize}

This definition is simple, yet has important details in practice.
The data you collect for your statistical analysis in {\em raw} format, which is typically not
a \R{} data.frame. 

If your data are stored in a database or an other binary structured form, 
retrieving the data within R does give you a \code{data.frame}. However even in these cases it is advisable to
store your data in an open text format (e.g. Comma Separated Values) to make your analysis
reproducible. Furthermore, many times your imported data contains (text) values that need to be processed
into the right \R{} type: for {\em technically correct} data the columns of the data.frame need to have the right types.

Typically raw data are in a text file, which has several favorable properties:
\begin{itemize}
\item It is a format that can be read by human beings. This means that you can check data easily.
\item In many cases it is in an open format that can be read by computer software
\item Text is very permissive in the values that are stored. For many experimental data
this is desirable, because data can be annotated or explained. It is also problematic 
because it make automatic conversion into a technical correct \code{data.frame} more difficult.
\end{itemize}

\begin{tip}{Best practice}
Whenever you need to read data from a foreign file format, like a spreadsheet or
commercial statistical software, make that software responsible for exporting the 
data to an open format that can be read by \R{}.
\end{tip}


So without further a due we assume that our data is in text format and needs to be processed into
a data.frame.

The steps for transforming raw data into technical correct data can be summerized as:
\begin{enumerate}
\item Process the text file into a \code{data.frame}. How the processing can be done depends on
the structure of the raw data
\item Extract and convert columns, \code{character} columns often need processing to extract the right
values from the raw data.Conversion of vectors into the right type can be tricky and can also 
remove valid data from your \code{data.frame}.
\end{enumerate}

\subsection{Reading data into a \R{} \code{data.frame}}

If the data you are trying to read into \R{} is formatted nicely, for example
as a \code{csv} or \code{tab}-delimited file reading data into a \code{data.frame} 
in general will succeed. However without using the right parameters the resulting data.frame
is often not technically correct.

Things become more difficult however when the storage format is not constant over the whole file. For example, in
one column, some numbers may have a period (\code{.}) as decimal separator
while others have a comma (\code{,}). Some lines may contain comments
instead of data, or have a different number of columns. In such cases \R{}'s
high-level data-reading functions such as \code{read.table} are not always capable
of reading the data correctly in a single go.

In this section we point out techniques to read normal and badly formatted text files
into \R{}. This tutorial is limited to rectangular data sets, although some
of the techniques mentioned here will be more broadly applicable.

\subsubsection{\code{read.table} and its cousins}
If you are able to read in your data with any of the following functions
\begin{center}
\begin{tabular}{ll}
\code{read.delim} & \code{read.delim2}\\
\code{read.csv}   & \code{read.csv2}\\
\code{read.table}
\end{tabular}
\end{center}

the result of these functions is a \code{data.frame}. If your are lucky the data is ``technically correct''
However, there are some tips and tricks we'll point out here to help you read those almost-standard
files.

\begin{tip}{Best practice}
Whenever you need to read data into\R{}, check the resulting \code{data.frame} with the \R{}
functions \code{str} and \code{summary}. This helps you to detect 
\end{tip}

The \code{read.table} function is the most flexible function to read tabular data
that is stored in a textual format. In fact the other \code{read}-functions mentioned
earlier are just calls to \code{read.table} with some arguments already fixed. 
Specifically
\begin{center}
\begin{tabular}{lp{0.7\textwidth}}
\code{read.csv}  & for comma separated values with period as decimal separator.\\
\code{read.csv2} & for semicolon separated values with comma as decimal separator.\\
\code{read.delim}& tab-delimited files with period as decimal separator. \\
\code{read.delim2}& tab-delimited files with comma as decimal separator.
\end{tabular}
\end{center}
%
%
%

\begin{tabular}{lcl}
  \code{header}           & FALSE & Does the first line contain column names?\\
  \code{na.string}        &  "NA" & Which strings should be considered \code{NA}? \\
  \code{colClasses}       &  NA   & \code{character} vector with the types of columns. Will coerce the columns to the specified types.\\
  \code{stringsAsFactors} & TRUE  & If TRUE converts all \code{character} vectors into \code{factor} vectors.
\end{tabular}

All these functions, except for \code{read.table}, expect that the first line of the file contains column
headers by default. If your heading is missing you should set the names of the \code{data.frame} with
\code{colnames}

<<>>=
# first line is interpreted as column names
(person <- read.csv("files//unnamed.txt"))
(person <- read.csv("files//unnamed.txt", header=FALSE))
colnames(person) <- c("age", "height")
print(person)
@

Without specifying \code{colClasses} \code{read.table} tries to detect the type of the column
automatically. This works quite well, but in case a numeric column contains a non numeric value
the column is read in as a \code{character} vector. Specifying \code{colClasses} speeds up reading the data considerably.
By default \code{character} vectors are converted into \code{factors}. With \code{stringsAsFactors=FALSE} 
this can be switched off.
Note that since \code{person\$height} contains the value \code{5.9*}, \code{height} was read in as a character and coerced into a factor.
<<>>=
str(person)
summary(person)
@

Other arguments worth mentioning
here are \code{na.strings}: a \code{character} vector indicating which codes
are to be interpreted as \code{NA}, \code{skip}, telling \code{read.table}
how many lines to skip at the beginning of the file and \code{comment.char}
indicating how to recognize lines of comment in the file.

\subsubsection{Reading data with \code{scan}}

It offers a little more freedom then \code{read.table}, but if your file cannot be processed by \code{read.table}
you will probably will need  \code{readLines}.

\subsubsection{Reading data with \code{readLines}}
When the rows in a data file are not uniformly formatted you can consider
reading in the text line-by-line and transforming the data to a rectangular set
yourself.  With \code{readLines} you can exercise precise control over how each line is
interpreted and transformed into fields in a rectangular data set. Table
\ref{tab:transform} gives an overview of the steps to be taken. Below, each step is
discussed in more detail.

\textbf{Step 1. Reading data.} Reading in data with \code{readLines} is simple
enough. It accepts filename as argument and returns a \code{character} vector
containing one element for each line in the file.  \code{readLines} detects
both the end-of-line and carriage return characters so lines are detected
regardless of whether the file was created under \code{DOS}, \code{UNIX} or
\code{MAC} (each OS has traditionally had different ways of marking an
end-of-line). Here's an example (the file can be obtained from our
\href{https://github.com/edwindj/datacleaning/tree/master/Rnw/syllabus/files}{Github}
page.
<<>>=
(txt <- readLines("files/daltons.txt")) 
@
The variable \code{txt} has \Sexpr{length(txt)} elements, equal to the number
of lines in the textfile.
%
\begin{table}
\begin{threeparttable}
\caption{Steps to take when converting lines in a raw text file to a \code{data.frame}
with correctly typed columns.}
\label{tab:transform}
\begin{tabular}{lll}
\hline
& \textbf{Step} & \textbf{result} \\
\hline
1 & Read the data with \code{readLines} & \code{character}\\
2 & Select lines containing data        & \code{character}\\
3 & Split lines into separate fields    & \code{list} of \code{character} vectors     \\
4 & Standardize rows                    & \code{list} of equivalent vectors      \\
5 & Transform to \code{data.frame}      & \code{data.frame}\\
6 & Normalize and coerce to correct type& \code{data.frame}\\
\hline
\end{tabular}
\end{threeparttable}
\end{table}

\textbf{Step 2. Selecting lines containing data.} This is generally done by
throwing out lines containing comments or otherwise lines that do not contain
any data fields. You can use \code{grep} or \code{grepl}
to detect such lines. \code{grep} and \code{grepl} are discussed in more detail in
section \ref{sect:appstringmatching}), but below is an example of how to remove
lines that start with a \code{\%}-sign.
<<>>=
# detect lines starting with a percentage sign..
I <- grepl("^%",txt)
# and throw them out
(dat <- txt[!I])
@

\textbf{Step 3. Split lines into separate fields.} This can be done with
\code{strsplit}. This function accepts a \code{character} vector and a \code{split}
argument which tells \code{strsplit} how to split a string into substrings. The
result is a \code{list} of \code{character} vectors.
<<>>=
(fieldList <- strsplit(dat,split=","))
@
Here, \code{split} is a single character or sequence of characters that are to be
interpreted as field separators. By default, \code{split} is interpreted as a
regular expression (see Section \ref{sect:appstringmatching}) which means you
need to be careful when the split argument contains any of the special characters
listed on page \pageref{lab:regexp}. The meaning of these special characters can
be ignored by passing \code{fixed=TRUE} as extra parameter. Compare as an example
the following results.
<<>>=
strsplit("a.b",split=".")
strsplit("a.b",split=".",fixed=TRUE)
@
The reason for the first result is that the period (\code{.}) is used as a
wild card character in regular expressions.

Note that str


\textbf{Step 4. Standardize rows.} The goal of this step is to make sure that
1) every row has the same number of fields and 2) the fields are in the right
order. This step involves a loop over all elements of \code{fieldList}. You
can either use a \code{for}-loop or the \code{lapply} function to treat
all elements of a \code{list}. In the running example we need to 
replace every vector that is shorter than the number of columns in the data file
with a vector with the correct length. Extra fields should be set to \code{NA}.
<<>>=
standardFields <- list(length(fieldList))
for ( i in 1:length(fieldList)){
  x <- fieldList[[i]]
  length(x) <- 3
  standardFields[[i]] <- x
}
standardFields
@
By setting the \code{length} of each element to 3, every element with less 
elements is concatenated with enough \code{NA}'s to reach length 3. 

The above code snippet does the job, but we can do a little better than that.
The \R{} function \code{lapply} offers a convenient way to loop over all
elements in a list. \code{lapply} accepts as arguments a \code{list} and a
function. The function will be fed the elements of the \code{list} one-by-one
and the results are gathered in a \code{list} again (optionally, extra
parameters to the function can be passed as extra named arguments). The
function \code{sapply} does exactly the same as \code{lapply}, but it tries to
simplify the results to a simpler structure than a list. We'll use
\code{sapply} to get the maximum number of fields in \code{fieldList}.
<<>>=
nrows = max(sapply(fieldList,length))
@
Here, \code{sapply} applies \code{length} to each vector in \code{fieldList} and gathers
the result in a \code{numeric} vector. \code{max} returns its largest value. We now
use \code{lapply} to change the length of each field.
<<tidy=FALSE>>=
standardFields <- lapply(fieldList,
  function(x){
    length(x) <- nrows
    x
  }
)
standardFields
@
Note that here, it is not necessary to first define a \code{standardFields}
list to assign values to.

\begin{tip}{Tip}
Another advantage of \code{lapply} is that it is really easy to parallelize
list-wise operations using the \code{parallel} package. For example, on a quadcore
computer you can do the following.
<<eval=FALSE,tidy=FALSE>>=
library(parallel)
cluster <- makeCluster(4)
standardFields <- parLapply(cl=cluster, fieldList, 
  function(x){
    x
  }
)
stopCluster(cl)
@
Of course, parallelization only makes sense when you have a fairly long list to process,
since there is some overhead in setting up and running the cluster.
\end{tip}



\textbf{Step 5. Transform to \code{data.frame}.}
There are several ways to transform a list to a \code{data.frame} object.
Here we first copy all our elements in a \code{matrix} which we'll then coerce
into a \code{data.frame}.
<<>>=
(M <- matrix(unlist(standardFields),nrow=nrows,byrow=TRUE))
colnames(M) <- c("name","birth","death")
(daltons <- as.data.frame(M, stringsAsFactors=FALSE))
@
The function \code{unlist} concatenates all vectors in a \code{list} into one large
character vector. We then use that vector to fill a \code{matrix} of class \code{character}.
However, the \code{matrix} function usually fills up a matrix column by column. Here, our
data is stored with rows concatenated, so we need to add the argument \code{byrow=TRUE}.
Finally, we add column names and coerce the matrix to a \code{data.frame}. We use
\code{stringsAsFactors=FALSE} since we have not started interpreting the values yet.

\textbf{Step 6. Normalize and coerce to correct types.}

This step consists of preparing the \code{character} columns of our \code{data.frame}
for coercion and translating numbers into \code{numeric} vectors and possibly
\code{character} vectors to \code{factor} variables. String normalisation is
the subject of section \ref{sect:stringnormalisation} and type conversion is
discussed in some more detail in the next section. However, in our example
we can suffice with the following statements.
<<>>=
daltons$birth <- as.numeric(daltons$birth)
daltons$death <- as.numeric(daltons$death)
daltons
@
Or using the underappreciated function \code{transform}:
<<tidy=FALSE>>=
daltons = transform( daltons
                   , birth = as.numeric(birth)
                   , death = as.numeric(death)
                   )
@
This last example is a case of converting types in \R{}.

\subsection{Type conversion}

Converting a variable from one type to another is called \emph{coercion}.
The reader is probably familiar with \R{}'s basic
coercion functions, but as a reference they are listed here.
\begin{center}
\begin{tabular}{ll}
  \code{as.numeric}     & \code{as.logical}\\
  \code{as.integer}     & \code{as.factor} \\
  \code{as.character}   & \code{as.ordered} 
\end{tabular}
\end{center}
Each of these functions takes an \R{} object and tries to convert it to the
class specified behind the ``\code{as.}''.

Note that Values that cannot be converted to the specified type will be 
converted into a \code{NA} value.
<<>>=
as.numeric(c("7","7*","7.0","7,0"))
@

\begin{tip}{Best practice}
We advise against \code{as.factor} since \code{factor} give  control over 
creating a factor, while \code{as.factor} does not.
\end{tip}

Converting a \code{factor} or \code{ordered} with \code{as.numeric} however is
counterintuitive. The reason for this has to do with differences between
\emph{classes} and \emph{types} in \R{}. An example will be worked out
in Exercise \ref{ex:factorconversion}. Below, we  introduce \R{}'s typing and
storage system and explain, amongst others, the difference between \R{}
\emph{types} and \emph{classes}. After that we discuss date conversion.

\subsubsection{Introduction to \R{}'s typing system}
Everything in \R{} is an object\cite{chambers:2008}. An object is a container
of data endowed with a \emph{class} label describing the data. Objects can
be created, destroyed or overwritten on-the-fly by the user.

The function \code{class} returns the class label of an \R{} object.
<<>>=
class(c('abc','def'))
class(1:10)
class(c(pi,exp(1)))
class(factor(c("abc","def")))
@
%
\begin{tip}{Tip}
Here's a quick way to retrieve the classes of all columns in a \code{data.frame} called \code{dat}.
<<eval=FALSE>>=
sapply(dat,class)
@
\vspace{-1cm}
\end{tip}

For the user of \R{} these class labels are usually enough to handle \R{} objects
in \R{} scripts. Under the hood, the basic \R{} objects are stored as \C{}
structures as \C{} is the language in which \R{} itself has been written. The type
of \C{} structure that is used to store a basic type can be found with the
\code{typeof} function. Compare the results below with those in the previous
code snippet.
<<>>=
typeof(c('abc','def'))
typeof(1:10)
typeof(c(pi,exp(1)))
typeof(factor(c("abc","def")))
@
Note that the \code{type} of an \R{} object of class \code{numeric} is \code{double}.
The term \code{double} refers to \code{double precision}, which is a standard
way for lower-level computer languages such as \C{} to store approximations of real numbers. 

Summarizing, one can regard the \emph{class} of an object the object's type from
the user's point of view while the \emph{type} of an object is the way \R{} looks
at the object. It is important to realize that \R{}'s coercion functions are fundamentally
functions that change the underlying \code{type} of an object and that class changes
are a consequence of the type changes. 

Confusingly, \R{} objects also have a \emph{mode} (and \emph{storage.mode})
which can be retrieved or set using functions of the same name. Both
\code{mode} and \code{storage.mode} differ slightly from \code{typeof}, and are
only there for backwards compatibility with \R{}'s precursor language:
\code{S}. We therefore advise the user to avoid using these functions to
retrieve or modify an object's type. 

\subsubsection{Recoding factors}
Often data contains categorical or ordinal data. In \R{} these are stored in a
\code{factor} or \code{ordered}, which is a special form of a \code{factor}.

A \code{factor} is a dressed up \code{integer} with an extra attribute \code{levels}.
Every integer value \code{i} in a \code{factor} corresponds to \code{levels[i]}.
<<>>=
f <- factor(c("a","b","a","a","c"))
levels(f)
levels(f)[f] # same as as.character(f)
@

\begin{tip}{Bad practice}
Allthough it is very easy to manipulate the \code{levels} of a \code{factor}, it is often better and more clear
to use \code{factor} for this functionality.
\end{tip}

<<>>=
# dirty recoding, avoid it!
levels(f) <- c("A","B", NA)
f
@

Categorical data are in many data files stored as text values. If a categorical data column
is not neatly specified, we will need \code{character} manipulation 
(see \ref{sssect:charactermanipulation}). If you use data of others, you
will often need to recode factors.

Even when the specification is clear, we might want to recode the used factor into 
more meaningful codes, with a specific ordering.
Suppose in our dataset \code{gender} is encoded with \code{1} for female, \code{2} for male and \code{0} for 
unknown. With \code{factor} this can be recoded.
<<>>=
gender <- c(2,1,1,2,0,1,1)

#recode
codes <- c("2"="M", "1"="F")
(gender <- factor(gender, levels=names(codes), labels=codes))
@
Note that \code{levels} and \code{labels} need to have the same order.

In case we have an ordinal column, we need to specify the order the categories have.
If we don't it will take the alphabetical order of the \code{levels}, which is in the given example
just plain wrong.
<<tidy=FALSE>>=
agegroup <- c("elderly", "child", "adult", "child", "elderly")
(agegroup <- factor( agegroup
                   , levels=c("child", "adult", "elderly")
                   , order=TRUE)
                   )
@


In some case you may want calculate the order of the levels depending on another
variable. This can be done with \code{reorder}:
<<>>=
age <- c(72, 10, 23, 6, 67)
agegroup <- c("elderly", "child", "adult", "child", "elderly")
(agegroup <- reorder(agegroup, age, order=TRUE))
@
\code{reorder} uses the mean of the \code{age} to calculate the order. 

Or you might want make one of the levels a reference level, making it the first level.
<<>>=
(colors <- factor(c("red","green", "blue", "blue")))
relevel(colors, ref="green")
@

\subsubsection{Converting dates}
The base \R{} installation has three types of objects to store a time instance
\code{Date}, \code{POSIXlt} and \code{POSIXct}. The \code{Date} object
can only be used to store dates, the other two store date and/or time. Here, we
focus on converting text to \code{POSIXct} objects since this is probably the
most portable way to store such information.

Under the hood, a \code{POSIXct} object stores the number of seconds that have
passed since January 1, 1970 \mbox{00:00}. This makes it possible to compute
duration by simply subtracting two \code{POSIXct} objects. When a \code{POSIXct}
object is printed, \code{R} shows it in a human-readable calender format. For example,
the command \code{Sys.time} returns the system time provided by the operating system
in \code{POSIXct} format.
<<>>=
current_time <- Sys.time()
class(current_time)
current_time
@ 
Here, \code{Sys.time} uses the time zone that is stored in the \code{locale} settings
of the machine running \R{}. 

Converting from a calender time to \code{POSIXct} and back is not entirely
trivial, since there are many idiosyncrasies to handle in calender systems like
leap days, leap seconds, daylight saving times, time zones and so on.  It is
possible to add information on time zone to a \code{POSIXct} object when it is
created. 
%
Converting from text to \code{POSIXct} is further complicated by the many
textual conventions of time/date denotation. For example, both \code{28
September 1976} and \code{1976/09/28} both indicate the same day of the same
year. Moreover, the name of the month (or weekday) is language-dependent. In
this section we will not go into problems caused by leap days and seconds but
we will limit ourselves to converting the textual representation of a date to a
\code{POSIXct} object.

The \code{lubridate} package\cite{grolemund:2011} contains a number of
functions facilitating the conversion of text to \code{POSIXct} dates.
%
<<message=FALSE>>=
library(lubridate)
dates <- c("15/02/2013","15 Feb  13","It happened on 15 02 '13")
dmy(dates)
@
Here, the function \code{dmy} assumes that dates are denoted in the order
day-month-year and tries to extract valid dates. There are similar functions
for all permutations of \code{d}, \code{m} and \code{y}. Explicitly, all of the
following functions exist.
\begin{center}
\begin{tabular}{ccc}
\code{dmy} & \code{myd} &\code{ydm}\\
\code{mdy} & \code{dym} &\code{ymd}\\
\end{tabular}
\end{center}
So once it is known in what order days, months and years are denoted,
extraction is very easy. 

\begin{tip}{Note}
Years, indicated with two numbers are always interpreted as years in the 21st century.
<<>>=
dmy("01 01 13")
@
\vspace{-1cm}
\end{tip}


It should be noted that \code{lubridate} (as well as \R{}'s base functionality) is
only capable of converting certain standard notations. For example, the following
notation does not convert.
<<>>=
dmy("15 Febr. 2013")
@
The standard notations that can be recognized by \R{}, either using \code{lubridate}
or \R{}'s built-in functionality are shown in Table \ref{tab:dateformats}.
%
\begin{table}
\begin{threeparttable}
  \caption{Day, month and year formats recognized by \R{}.}
  \label{tab:dateformats}
  \begin{tabular}{lll}
    \hline
    Code       & description\cite{}                             & Example  \\
    \hline
    \code{\%a} & Abbreviated weekday name in the current locale.& \code{Mon}  \\
    \code{\%A} & Full weekday name in the current locale.       & \code{Monday}\\
    \code{\%b} & Abbreviated month name in the current locale.  & \code{Sep} \\
    \code{\%B} & Full month name in the current locale.         & \code{September}\\
    \code{\%m} & Month number (01-12)                           & \code{09}\\
    \code{\%d} & Day of the month as decimal number (01-31).    & \code{28} \\
    \code{\%y} & Year without century (00-99)                   & \code{13}\\
    \code{\%Y} & Year including century.                        & \code{2013}\\
    \hline
  \end{tabular}
\end{threeparttable}
\end{table}
%
%
Here, the names of (abbreviated) week or month names that are sought for in the
text depend on the locale settings of the machine that is running \R{}. For example,
on a PC running under a Dutch locale, ``\code{maandag}'' will be recognized as the
first day of the week while in English locales ``\code{Monday}'' will be recognized. If
the machine running \R{} has multiple locales installed you may add the argument
\code{locale} to one of the \code{dmy}-like functions. In \code{linux}-alike systems
you can use the command \code{locale -a} in \code{bash} terminal to see the list of
installed locales. In \code{Windows} you can find available locale settings under
``language and regional settings'', under the configuration screen.

If you know the textual format that is used to describe a date in the input, you may
want to use \R{}'s core functionality to convert from text to \code{POSIXct}. This
can be done with the \code{as.POSIXct} function. It takes as arguments a \code{character}
vector with time/date strings and a string describing the format. 
<<>>=
dates <- c("15-9-2009","16-07-2008","17 12-2007","29-02-2011")
as.POSIXct(dates,format="%d-%m-%Y")
@
In the format string, date and time fields are indicated by a letter preceded
by a percent sign (\code{\%}). Basically, such a \code{\%}-code tells \R{} to
look for a range of substrings. For example, the \code{\%d} indicator makes
\R{} look for numbers \code{1-31} where precursor zeros are allowed, so
\code{01}, \code{02},$\ldots$\code{31} are recognized as well. Table
\ref{tab:dateformats} shows which date-codes are recognized by \R{}. The
complete list can be found by typing \code{?strptime} in the \R{} console.
Strings that are not in the exact format specified by the \code{format}
argument (like the third string in the above example) will not be converted by
\code{as.POSIXct}.  Impossible dates, such as the leap day in the fourth date
above are also not converted.

Finally, to convert dates from \code{POSIXct} back to character, one may use
the \code{format} function that comes with base \R{}. It accepts a \code{POSIXct}
date/time object and an output format string.
<<>>=
mybirth <- dmy("28 Sep 1976")
format(mybirth,format="I was born on %B %d, %Y")
@


\subsection{\code{character} manipulation}
\label{sssect:charactermanipulation}

Character data can be notoriously hard to process. For example, consider the
following excerpt of a data set with a \code{gender} variable.
<<echo=2>>=
data.frame(gender=c("M","male ","Female","fem."))
@
If this would be treated as a factor variable without any preprocessing,
obviously four, not two classes would be stored. The job at hand is therefore
to automatically recognize from the above data whether each elements pertains
to \code{male} or \code{female}. In statistical contexts, classifying such
``messy'' text strings into a number of fixed categories is often referred to
as \emph{coding}.

Below we discuss two complementary approaches to string coding: \emph{string
normalization} and \emph{approximate text matching}. In particular, the
following topics are discussed.
\begin{itemize}[itemsep=0pt]
\item Remove prepending or trailing white spaces.
\item Pad strings to a certain width.
\item Transform to upper/lower case.
\item Search for strings containing simple patterns (substrings).
\item Simple approximate matching procedures based on string distances.
\end{itemize}

\subsubsection{String normalization}
\label{sect:stringnormalisation}
%
String normalization techniques are aimed at transforming a variety of strings
to a (possibly) smaller set of string values which are more easily processed.
By default, \R{} comes with extensive string manipulation functionality that is
based on the two basic string operations: \emph{finding} a pattern in a string
and \emph{replacing} one patterns with another. We will deal with \R{}'s
generic functions below but start by pointing out some common string cleaning
operations.
  
The \code{stringr} package\cite{wickham:2009} offers a number of functions that
make a some string manipulation tasks a lot easier. For example, extra white
spaces at the beginning or end of a string can be removed using
\code{str\_trim}.
<<>>=
library(stringr)
str_trim("  hello world ")
str_trim("  hello world ",side="left")
str_trim("  hello world ",side="right")
@
Conversely, strings can be padded with spaces or other characters with \code{str\_pad} to
a certain width. For example, numerical codes are often represented with prepending zeros.
<<>>=
str_pad(112,width=6,side='left', pad=0)
@
Both \code{str\_trim} and \code{str\_pad} accept a \code{side} argument to indicate whether
trimming or padding should occur at the beginning (\code{left}), end (\code{right}) or both
sides of the string. 

Converting strings to complete upper or lower case can be done with \R{}'s built-in
\code{toupper} and \code{tolower} functions.
<<>>=
toupper("Hello world")
tolower("Hello World")
@


\subsubsection{Approximate string matching}
\label{sect:appstringmatching}
There are two forms of string matching. The first consists of determining
whether a (range of) substring(s) occurs within another string. In this case
one needs to specify a range of substrings (called a \emph{pattern}) to search
for in another string. In the second form one defines a distance metric between
strings that measures how ``different'' two strings are.  Below we will discuss
a short introduction to pattern matching and string distances with \R{}.


There are several pattern matching functions that come with base \R{}. The most
used are probably \code{grep} and \code{grepl}. Both functions take a pattern
and a \code{character} vector as input. The output only differs in that
\code{grepl} returns a logical index, indicating which element of the input
\code{character} vector contains the pattern, while \code{grep} returns a
numerical index. You may think of \code{grep(...)} as \code{which(grepl(...))}.

%
%
In the most simple case, the pattern to look for is a simple substring. For
example, using the data of the example on page
\pageref{sect:stringnormalisation}.
<<>>=
gender <- c("M","male ","Female","fem.")
grepl('m',gender)
grep('m',gender)
@
Note that the result is case sensitive: the capital \code{M} in the first
element of \code{gender} does not match the lower case \code{m}. There are
several ways to circumvent this case sensitivity.  Either by case normalization
or by the optional argument \code{ignore.case}.
<<>>=
grepl('m',gender,ignore.case=TRUE)
grepl('m',tolower(gender))
@
Obviously, looking for the occurrence of \code{m} or \code{M} in the
\code{gender} vector does not allow us to determine which strings pertain to
\code{male} and which not. Preferably we would like to search for strings that
start with an \code{m} or \code{M}.  Fortunately, the search patterns that
\code{grep} accepts allow for such searches. The beginning of a string is
indicated with a caret (\code{\^}).
<<>>=
grepl('^m',gender,ignore.case=TRUE)
@
Indeed, the \code{grepl} function now finds only the first two elements of
\code{gender}.  The caret is an example of a so-called \emph{meta-character}.
That is, it does not indicate the caret itself but something else, namely the
beginning of a string. The search patterns that \code{grep}, \code{grepl} (and
\code{sub}
and \code{gsub}) understand have more of these meta-characters, namely: 
\begin{verbatim}
   . \ | ( ) [ {  ^ $ * + ? 
\end{verbatim}
\label{lab:regexp}
If you need to search a string for any of these characters, you can use the
option \code{fixed=TRUE}.
<<>>=
grepl('^',gender,fixed=TRUE)
@
This will make \code{grepl} ignore any meta-characters in the search string.

Search patterns using meta-characters are called \emph{regular expressions}.
Regular expressions offer powerful and flexible ways to search (and alter)
text.  A discussion of regular expressions is beyond the scope of these lecture
notes. However, a concise description of regular expressions allowed by \R{}'s
built-in string processing functions can be found by typing \code{?regex} at
the \R{} command line. The books by Fitzgerald\cite{fitzgerald:2012} or
Friedl\cite{friedl:2006} provide a thorough introduction to the subject of
regular expression. If you frequently have to deal with ``messy'' text
variables, learning to work with regular expressions is a worthwhile
investment. Moreover, since many popular programming languages support some
dialect of regexps, it is an investment that could pay off several times.

We now turn our attention to the second method of approximate matching, namely
string distances.  A string distance is an algorithm or equation that indicates
how much two strings differ from each other.  An important distance measure is
implemented by the \R{}'s native \code{adist} function. This function counts
how many basic operations are needed to turn one string into another. These
operations include insertion, deletion or substitution of a single
character\cite{levenshtein:1966}.  For example
<<>>=
adist("abc","bac")
@
The result equals two since turning \code{"abc"} into \code{"bac"} involves two
character substitutions. Using \code{adist}, we can compare fuzzy text strings
to a list of known codes. For example:
<<>>=
codes <- c('male','female')
D <- adist(gender,codes)
colnames(D) <- codes
rownames(D) <- gender
D
@
Here, \code{adist} returns the distance matrix between our vector of fixed
codes and the input data. For readability we added row- and column names
accordingly. Now, to find out which code matches best with our raw data, we
need to find the index of the smallest distance for each row of \code{D}. This
can be done as follows.
<<>>=
i <- apply(D,1,which.min)
data.frame(
  rawtext = gender,
  coded   = codes[i]
)
@
We use \code{apply} to apply \code{which.min} to every row of \code{D}. Note
that in the case of multiple minima, the first match will be returned.

Finally, we mention two more functions based on string distances. First, the
\R{}-built-in function \code{agrep} is similar to grep, but it allows one to
specify a maximum Levenshtein distance between the input pattern and the found
substring. 

Secondly, the functions \code{stringdist} and \code{stringdistmatrix} of the
\code{stringdist} package\cite{loo:2013} offer an interfaces to a variety of
string distance metrics. For example, \code{agrep} does not allow for character
transpositions, which is quite common in typing errors. Using the \emph{optimal
string alignment distance} (the default choice for \code{stringdist}) we get
<<message=FALSE>>=
library(stringdist)
stringdist("abc","bac")
@
The answer is now 1 (not 2), since the optimal string alignment distance allows
for transpositions of adjacent characters.

\subsection{Character encoding issues}
\label{sect:encoding}
A \emph{character encoding system} is a system that defines how to translate
each character of a given alphabet into a computer byte or sequence of
bytes\footnote{In fact, the definition can be more general, for example to
include Morse code. However for this tutorial it is sufficient to limit
ourselves to computerized character encodings.}. For example, \code{ASCII} is
an encoding system that prescribes how to translate 127 characters into single
bytes (where the first bit of each byte is necessarily 0). The \code{ASCII} characters
include the upper and lower case letters of the Latin alphabet (\code{a-z},
\code{A-Z}), Arabic numbers (\code{0}-\code{9}), a number of punctuation
characters and a number of invisible so-called control characters such as
newline and carriage return. 


Although it is widely used, \code{ASCII} is obviously incapable of encoding
characters outside the Latin alphabet, so you can say ``hello'', but not
``$\gamma\varepsilon\iota\alpha$ $\sigma\alpha\varsigma$'' in this encoding.
For this reason, a number of character encoding systems have been developed
that extend \code{ASCII} or replace it all together. Some well-known schemes
include \code{UTF-8} and \code{latin1}. The character encoding scheme that is
used by default by your operating system is defined in your \code{locale}
settings. Most Unix-alikes use \code{UTF-8} by default while \code{Windows}
often uses (a \code{Windows}-specific variant of) \code{latin1}. The
\code{UTF-8} encoding standard is also widely used on the web.

For \R{} to be able to correctly read in a textfile, it must understand which
character encoding scheme was used to store it. By default, \R{} assumes that a
textfile is stored in the encoding scheme defined by the operating system's
\code{locale} setting.  This may fail when the file was not generated on the
same computer that \R{} is running on but was obtained from the web, for
example.  To make things worse, it is impossible to determine automatically
with certainty from a file what encoding scheme has been used. This means that
you may run into situations where you have to tell \R{} literally in which
encoding a file has been stored. Once a file has been read into \R{}, a
character vector will internally be stored in either \code{UTF-8} or
\code{latin1}. 

The \code{fileEncoding} argument of \code{read.table} and its relatives tells
\R{} what encoding scheme was used to store the file. For \code{readLines} the
file encoding must be specified when the file is opened, before calling
\code{readLines}, as in the example below.
<<eval=FALSE>>=
# 1. open a connection to your file, specifying its encoding
  f <- file('myUTF16file.txt',encoding='UTF-16')
# 2. Read the data with readLines. 
#    Text read from the file is converted to uft8 or latin1
  input <- readLines(f)
# close the file connection.
  close(f)
@
%
%
When reading the file, \R{} will not translate the encoding to \code{UTF-8} or
\code{latin1} by itself, but instead relies on libraries offered by the
operating system.  \R{}'s \code{iconv} function forms an interface to the
OS-specific character encoding conversion libraries. With \code{iconvlist()}
you can check what encodings can be translated by your operating system.


\section*{Exercises}

\begin{exercise}
In this exercise we'll use \code{readLines} to read in an irregular textfile.
The file looks like this (without numbering).
\begin{center}
\lstinputlisting[
    xleftmargin=1cm,
    numbers=left,
    basicstyle=\ttfamily\scriptsize
  ]{files/example.txt}
\end{center}
You may copy the text from this \code{pdf} file in a textfile called
\code{example.txt} or download the file from our
\href{https://github.com/edwindj/datacleaning/blob/master/Rnw/syllabus/files}{Github
page}.

\begin{subex}
  \item Read the complete file using \code{readLines}.
  \item Separate the \code{list} of lines into a \code{list} containing
    comments and a \code{list} containing the data.  Hint: use \code{grepl}.
  \item Extract the date from the first comment line.
  \item Read the data into a \code{matrix} as follows.
  \begin{enumerate}
    \item Split the character vectors in the vector containing \code{data}
      lines by semicolon (\code{;}) using \code{strsplit}.
    \item Find the maximum number of fields retrieved by \code{split}. Append
      rows that are shorter with \code{NA}'s.
    \item Use \code{unlist} and \code{matrix} to transform the data to
      row-column format.
  \end{enumerate}
  \item From comment lines \code{2-4}, extract the names of the fields. Set
    these as \code{colnames} for the \code{matrix} you just created.
\end{subex}
\end{exercise}

\begin{exercise}
We will coerce the columns of the data of the previous exercise to a structured data set.
\begin{subex}
  \item Coerce the matrix to a \code{data.frame}, making sure all columns are
    \code{character} columns.
  \item Use a string distance technique to transform the \code{Gender} column into
    a \code{factor} variable with labels \code{man} and \code{woman}.
  \item Coerce the \code{Age} column to \code{integer}.
  \item Coerce the \code{weight} column to \code{numeric}. Hint: use \code{gsub} to replace
   comma's with a period.
\end{subex}
\end{exercise}


\begin{exercise}
More on type conversions.

  \begin{subex}
    \item Load the buildtin \code{warpbreaks} data set .
          Find out, in a single command, which columns of \code{warpbreaks} are either \code{numeric} or \code{integer}.
    \item Is \code{numeric} a logical data type for the columns which are stored as such? Convert to integer when necessary.
          (See also \code{?warpbreaks} for an explanation of the data).
    \item Error messages in \R{} sometimes report the underlying \emph{type} of an object rather than the user-level \emph{class}.
          Derive from the following code and error message what the underlying type of an \R{} function is.
          <<>>=
          mean[1]
          @
    Confirm your answer using \code{typeof}.
\end{subex}
\end{exercise}

\begin{exercise}
Create a factor variable by extracting the wool type from the \code{warpbreaks}
data set of the previous exercise.
<<>>=
myfac <- warpbreaks$wool 
@

\begin{subex}
  \item Take a look at the values of \code{myfac}, and convert it to \code{character} using \code{as.character}.
  \item Investigate the \code{class} and underlying \code{type} of \code{myfac}. Explain what happened in the previous subexercise.
  \item Now convert \code{myfac} to character, making sure that the labels, not the underlying integers are returned in the character vector.
        Tip: You can use \code{labels(myfac)} to get the factor labels.
  \item Use \code{format} to convert \code{myfac} to character vectors (tip: \code{\%s} indicates a text field).
\end{subex}
\label{ex:factorconversion}
\end{exercise}



