
\section{From raw data to technically correct data}
\label{sect:rawtoinput}
A data set is a collection of data that describes attribute values (variables)
of a number of real-world objects (units). With data that is \emph{technically
correct}, we understand a data set that is suitable for automated processing
such that for each unit, the attribute values should be correctly recognized
and read by the same set of computer instructions. Specifically, we demand that
each attribute value is stored in a data type that is suitable to describe the
value domain of the attribute and that the type is constant over units.  In
short: for each unit, a text variable should be stored as text, numeric
variables as numbers, and so on, and all this in a format that is consistent
accross the data set.


\subsection{Technically correct data in \R{}}
The \R{} environment is capable of reading and processing several file and data
formats.  For this tutorial we will limit ourselves to `rectangular'
data sets that are to be read from a text-based format.
In the case of \R{}, we define \emph{technically correct} data as a data set that
\begin{itemize}
\item is stored in a \code{data.frame} with suitable columns names, and
\item each column of the \code{data.frame} is of the \R{} type that adequately represents the
value domain of the variable in the column.
\end{itemize}
%
The second demand implies that numeric data should be stored
as \code{numeric} or \code{integer}, textual data should be stored as \code{character}
and categorical data should be stored as a \code{factor} or \code{ordered} vector,
with the appropriate levels.

Limiting ourselves to textual data formats for this tutorial may have its
drawbacks, but there are several several favorable properties of textual
formats over binary formats:
\begin{itemize}
\item It is human-readable. When you inspect a text-file, make sure to use a
text-reader (\code{more}, \code{less}) or editor (\code{Notepad}, \code{vim})
that uses a fixed-width font. Never use an office application for this purpose 
typesetting clutters the data's structure, for example by  the use of ligature.
\item Text is very permissive in the types values that are stored, allowing for
comments and annotations.
\end{itemize}
%
The task then, is to find ways to read a textfile into \R{} and have it
transformed to a well-typed \code{data.frame} with suitable column names. 

\begin{tip}{Best practice}
Whenever you need to read data from a foreign file format, like a spreadsheet
or proprietary statistical software that uses undisclosed file formats, make
that software responsible for exporting the data to an open format that can be
read by \R{}.
\end{tip}


\subsection{Reading text data into a \R{} \code{data.frame}}
In the following, we assume that the text-files we are reading contain at data
of at most one unit per line. The number of attributes, their format and
separation symbols in lines containing data may differ over the lines. This
includes files in fixed-width or \code{csv}-like format, but excludes
\code{XML}-like storage formats.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\code{read.table} and its cousins}
The following high-level \R{} functions allow you to read in data that is technically
correct, or close to it.
\begin{center}
\begin{tabular}{ll}
\code{read.delim} & \code{read.delim2}\\
\code{read.csv}   & \code{read.csv2}\\
\code{read.table} & \code{read.fwf}
\end{tabular}
\end{center}
%
the return type of all these functions is a \code{data.frame}. If the column names are
stored in the first line, they can automatically be assigned to the \code{names} attribute
of the resulting \code{data.frame}.

\begin{tip}{Best practice}
A freshly read \code{data.frame} should always be inspected with functions like
\code{head}, \code{str}, and \code{summary}.
\end{tip}

The \code{read.table} function is the most flexible function to read tabular
data that is stored in a textual format. In fact, the other
\code{read}-functions mentioned above all eventually use \code{read.table}
with some fixed parameters and possibly after some preprocessing. Specifically
\begin{center}
\begin{tabular}{lp{0.7\textwidth}}
\code{read.csv}  & for comma separated values with period as decimal separator.\\
\code{read.csv2} & for semicolon separated values with comma as decimal separator.\\
\code{read.delim}& tab-delimited files with period as decimal separator. \\
\code{read.delim2}& tab-delimited files with comma as decimal separator. \\
\code{read.fwf}   & data with a predetermined number of bytes per column.
\end{tabular}
\end{center}
%
%

Each of these functions accept, amongst others, the following optional arguments.
%
\begin{center}
\begin{tabular}{lp{0.5\textwidth}}
\hline
  Argument & description\\
  \hline
  \code{header}                 & Does the first line contain column names?\\
  \code{col.names}              & \code{character} vector with column names.\\
  \code{na.string}              & Which strings should be considered \code{NA}? \\
  \code{colClasses}             & \code{character} vector with the types of columns. Will coerce the columns to the specified types.\\
  \code{stringsAsFactors}       & If \code{TRUE}, converts all \code{character} vectors into \code{factor} vectors.\\
  \code{sep}$^\mathrm{\dagger}$ & Field separator. \\
\hline
\end{tabular}\\
${^\mathrm{\dagger}}${\small Used only internally by \code{read.fwf}}\hfill
\end{center}
%
Except for \code{read.table} and \code{read.fwf}, each of the above functions assumes by 
default that the first line in the text file contains column headers. To demonstrate
this, we assume that we have a the following text file stored under \code{files/unnamed.txt}.
\begin{center}
\lstinputlisting[
    xleftmargin=1cm,
    numbers=left,
    basicstyle=\ttfamily\small
  ]{files/unnamed.txt}
\end{center}
Now consider the following script.
<<tidy=FALSE>>=
# first line is erroneously interpreted as column names
(person <- read.csv("files/unnamed.txt"))
# so we better do the following
person <- read.csv(
  file        = "files/unnamed.txt"
  , header    = FALSE
  , col.names = c("age","height") )
person
@
In the first attempt, \code{read.csv} interprets the first line as column
headers and fixes the numeric data to meet \R{}'s variable naming standards by
prepending an \code{X}.


If \code{colClasses} is not specified by the user, \code{read.table} will try
to determine the column types. Although this may seem convenient, it is
noticably slower for larger files (say, larger than a few MB) and it may yield
unexpected results. For example, in the above script, one of the rows contains
a misformed numerical variable (\code{5.9*}), causing \R{} to interpret the
whole column as a text variable. Moreover, by default text variables are
converted to \code{factor}, so we are now stuck with a \emph{height} variable
expressed as levels in a categorical variable:
<<>>=
str(person)
@
%
Using \code{colClasses}, we can force \R{} to either interpret the columns
in the way we want or throw an error when this is not possible.
<<tidy=FALSE>>=
read.csv("files/unnamed.txt",
  header=FALSE,
  colClasses=c('numeric','numeric'))
@
This behaviour is desirable if you need to be strict about how data is offered
to your \R{} script. However, unless you are prepared to write \code{tryCatch}
constructions, a script containing the above code will stop executing
completely when an error is encountered.

As an alternative, columns can be read in as \code{character} by setting
\code{stringsAsFactors=FALSE}. Next, one of the \code{as.}-functions can be
applied to convert to the desired type, as shown below. 
<<tidy=FALSE>>=
dat <- read.csv(
  file              = "files/unnamed.txt"
  , header          = FALSE
  , col.names       = c("age","height")
  , stringsAsFactors=FALSE)
dat$height <- as.numeric(dat$height)
dat
@
Now, everything is read in and the \code{height} column is translated to
numeric, with the exception of the row containing \code{5.7*}. Moreover, since
we now get a warning in stead of an error, a script containing this statement
will continue to run, albeit with less data to analyse then it was supposed to.
It is of course up to the programmer to check for these extra
\code{NA}'s and handle them appropriately.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Reading data with \code{readLines}}
When the rows in a data file are not uniformly formatted you can consider
reading in the text line-by-line and transforming the data to a rectangular set
yourself.  With \code{readLines} you can exercise precise control over how each
line is interpreted and transformed into fields in a rectangular data set.
Table \ref{tab:transform} gives an overview of the steps to be taken. Below,
each step is discussed in more detail. As an example we will use a file called
\code{daltons.txt}. Below, we show the  contents of the file and the actual table
with data as it should appear in \R{}.

\vspace{1ex}
\begin{minipage}{0.5\textwidth}
\emph{Data file:}
\lstinputlisting[
    xleftmargin=1cm,
    numbers=left,
    basicstyle=\ttfamily\small
  ]{files/daltons.txt}
\end{minipage}\begin{minipage}{0.5\textwidth}
\hspace{0.5cm}\emph{Actual table:}\\

\hspace{0.5cm}\begin{tabular}{lll}
Name  & Birth     & Death\\
\hline
Gratt & 1861      & 1892\\ 
Bob   & \code{NA} & 1892\\
Emmet & 1871      & 1937\\
\end{tabular}
\end{minipage}
%

The file has comments on several lines (starting with a \code{\%} sign) and a
missing value in the second row.  Moreover, in the third row the name and birth
date have been swapped.


\textbf{Step 1. Reading data.} The \code{readLines} function accepts filename
as argument and returns a \code{character} vector containing one element for
each line in the file.  \code{readLines} detects both the end-of-line and
carriage return characters so lines are detected regardless of whether the file
was created under \code{DOS}, \code{UNIX} or \code{MAC} (each OS has
traditionally had different ways of marking an end-of-line). Reading in the
Daltons file yields the following.
<<>>=
(txt <- readLines("files/daltons.txt")) 
@
The variable \code{txt} has \Sexpr{length(txt)} elements, equal to the number
of lines in the textfile.

\textbf{Step 2. Selecting lines containing data.} This is generally done by
throwing out lines containing comments or otherwise lines that do not contain
any data fields. You can use \code{grep} or \code{grepl}
to detect such lines. 


<<>>=
# detect lines starting with a percentage sign..
I <- grepl("^%",txt)
# and throw them out
(dat <- txt[!I])
@
Here, the first argument of \code{grepl} is a search pattern, where the caret
(\code{\^}) indicates a start-of-line. The result of \code{grepl} is a
\code{logical} vector that indicates which elements of \code{txt} contain the
pattern 'start-of-line' followed by a percent-sign.  The functionality of
\code{grep} and \code{grepl} will be discussed in more detail in section
\ref{sect:appstringmatching}. 


\textbf{Step 3. Split lines into separate fields.} This can be done with
\code{strsplit}. This function accepts a \code{character} vector and a \code{split}
argument which tells \code{strsplit} how to split a string into substrings. The
result is a \code{list} of \code{character} vectors.
<<>>=
(fieldList <- strsplit(dat,split=","))
@
Here, \code{split} is a single character or sequence of characters that are to be
interpreted as field separators. By default, \code{split} is interpreted as a
regular expression (see Section \ref{sect:appstringmatching}) which means you
need to be careful when the split argument contains any of the special characters
listed on page \pageref{lab:regexp}. The meaning of these special characters can
be ignored by passing \code{fixed=TRUE} as extra parameter. 
%
\begin{table}
\begin{threeparttable}
\caption{Steps to take when converting lines in a raw text file to a \code{data.frame}
with correctly typed columns.}
\label{tab:transform}
\begin{tabular}{lll}
\hline
& \textbf{Step} & \textbf{result} \\
\hline
1 & Read the data with \code{readLines} & \code{character}\\
2 & Select lines containing data        & \code{character}\\
3 & Split lines into separate fields    & \code{list} of \code{character} vectors     \\
4 & Standardize rows                    & \code{list} of equivalent vectors      \\
5 & Transform to \code{data.frame}      & \code{data.frame}\\
6 & Normalize and coerce to correct type& \code{data.frame}\\
\hline
\end{tabular}
\end{threeparttable}
\end{table}



\textbf{Step 4. Standardize rows.} The goal of this step is to make sure that
1) every row has the same number of fields and 2) the fields are in the right
order. In \code{read.table}, lines that contain less fields than the maximum
number of fields detected are appended with \code{NA}. One advantage of the
do-it-yourself approach shown here is that we do not have to make this assumption.
The easiest way to standardize rows is to write a function that takes a single
\code{character} vector as input and assigns the values in the right order.
%
<<tidy=FALSE>>=
assignFields <- function(x){
  out <- character(3)
  # get names
  i <- grepl("[[:alpha:]]",x)
  out[1] <- x[i]
  # get birth date (if any)
  i <- which(as.numeric(x) < 1890)
  out[2] <- ifelse(length(i)>0, x[i], NA)
  # get death date (if any)
  i <- which(as.numeric(x) > 1890)
  out[3] <- ifelse(length(i)>0, x[i], NA)
  out
}
@
The above function accepts a character vector and assigns three values to an
output vector of class \code{character}. The \code{grepl} statement detects
fields containing alphabetical values \code{a-z} or \code{A-Z}. To assign year
of birth and year of death, we use the knowledge that all Dalton brothers were
born before and died after 1890. To retrieve the fields for each row in the example, 
we need to apply this function to every element of \code{fieldList}. 
<<warning=FALSE>>=
standardFields <- lapply(fieldList, assignFields)
standardFields
@
Here, we suppressed the warnings about failed conversions that \R{} generates
in the output.

The advantage of this approach is having greater flexibility than
\code{read.table} offers.  However, since we are interpreting the value of
fields here, it is unavoidable to know about the contents of the dataset which
makes it hard to generalize the field assigner function. Furthermore,
\code{assignFields} function we wrote is still relatively fragile. That is:
it crashes for example when the input vector contains two or more text-fields
or when it contains  more than one numeric value larger than 1890. Again, no
one but the data analyst is probably in a better position to choose how safe
and general the field assigner should be.


\begin{tip}{Tip}
Element-wise operations over lists are easy to parallelize with the
\code{parallel} package that comes with the standard \R{} installation. For
example, on a quadcore computer you can do the following.
<<eval=FALSE,tidy=FALSE>>=
library(parallel)
cluster <- makeCluster(4)
standardFields <- parLapply(cl=cluster, fieldList, assignFields)
stopCluster(cl)
@
Of course, parallelization only makes sense when you have a fairly long list to process,
since there is some overhead in setting up and running the cluster.
\end{tip}



\textbf{Step 5. Transform to \code{data.frame}.}
There are several ways to transform a list to a \code{data.frame} object.
Here, first all elements are copied into a \code{matrix} which is then coerced
into a \code{data.frame}.
<<tidy=FALSE>>=
(M <- matrix(
  unlist(standardFields)
  , nrow=length(standardFields)
  , byrow=TRUE))
colnames(M) <- c("name","birth","death")
(daltons <- as.data.frame(M, stringsAsFactors=FALSE))
@
The function \code{unlist} concatenates all vectors in a \code{list} into one large
character vector. We then use that vector to fill a \code{matrix} of class \code{character}.
However, the \code{matrix} function usually fills up a matrix column by column. Here, our
data is stored with rows concatenated, so we need to add the argument \code{byrow=TRUE}.
Finally, we add column names and coerce the matrix to a \code{data.frame}. We use
\code{stringsAsFactors=FALSE} since we have not started interpreting the values yet.

\textbf{Step 6. Normalize and coerce to correct types.}

This step consists of preparing the \code{character} columns of our \code{data.frame}
for coercion and translating numbers into \code{numeric} vectors and possibly
\code{character} vectors to \code{factor} variables. String normalisation is
the subject of section \ref{sect:stringnormalisation} and type conversion is
discussed in some more detail in the next section. However, in our example
we can suffice with the following statements.
<<>>=
daltons$birth <- as.numeric(daltons$birth)
daltons$death <- as.numeric(daltons$death)
daltons
@
Or, using \code{transform}:
<<tidy=FALSE>>=
daltons = transform( daltons
                   , birth = as.numeric(birth)
                   , death = as.numeric(death)
                   )
@


\subsection{Type conversion}

Converting a variable from one type to another is called \emph{coercion}.
The reader is probably familiar with \R{}'s basic
coercion functions, but as a reference they are listed here.
\begin{center}
\begin{tabular}{ll}
  \code{as.numeric}     & \code{as.logical}\\
  \code{as.integer}     & \code{as.factor} \\
  \code{as.character}   & \code{as.ordered} 
\end{tabular}
\end{center}
Each of these functions takes an \R{} object and tries to convert it to the
class specified behind the ``\code{as.}''.
By default, values that cannot be converted to the specified type will be 
converted to a \code{NA} value while a warning is issued.
<<>>=
as.numeric(c("7","7*","7.0","7,0"))
@
In the remainder of this section we  introduce \R{}'s typing and storage system
and explain the difference between \R{} \emph{types} and \emph{classes}. After
that we discuss date conversion.

\subsubsection{Introduction to \R{}'s typing system}
Everything in \R{} is an object\cite{chambers:2008}. An object is a container
of data endowed with a \emph{class} label describing the data. Objects can
be created, destroyed or overwritten on-the-fly by the user.

The function \code{class} returns the class label of an \R{} object.
<<>>=
class(c('abc','def'))
class(1:10)
class(c(pi,exp(1)))
class(factor(c("abc","def")))
@

\begin{tip}{Tip}
Here's a quick way to retrieve the classes of all columns in a \code{data.frame} called \code{dat}.
<<eval=FALSE>>=
sapply(dat,class)
@
\vspace{-1cm}
\end{tip}

For the user of \R{} these class labels are usually enough to handle \R{} objects
in \R{} scripts. Under the hood, the basic \R{} objects are stored as \Clang{}
structures as \Clang{} is the language in which \R{} itself has been written. The type
of \Clang{} structure that is used to store a basic type can be found with the
\code{typeof} function. Compare the results below with those in the previous
code snippet.
<<>>=
typeof(c('abc','def'))
typeof(1:10)
typeof(c(pi,exp(1)))
typeof(factor(c("abc","def")))
@
Note that the \code{type} of an \R{} object of class \code{numeric} is
\code{double}.  The term \code{double} refers to \code{double precision}, which
is a standard way for lower-level computer languages such as \Clang{} to store
approximations of real numbers.  Also, the \code{type} of an object of class
\code{factor} is \code{integer}. The reason is that \R{} saves memory (and
computational time!) by storing factor values as integers, while a translation
table between factor and integers are kept in memory. Normally, a user should not
have to worry about these subtleties, but there are exceptions. An example of this is 
the subject of Exercise \ref{ex:factor}.

In short, one may regard the \emph{class} of an object the object's type from
the user's point of view while the \emph{type} of an object is the way \R{}
looks at the object. It is important to realize that \R{}'s coercion functions
are fundamentally functions that change the underlying \code{type} of an object
and that class changes are a consequence of the type changes. 

Confusingly, \R{} objects also have a \emph{mode} (and \emph{storage.mode})
which can be retrieved or set using functions of the same name. Both
\code{mode} and \code{storage.mode} differ slightly from \code{typeof}, and are
only there for backwards compatibility with \R{}'s precursor language:
\code{S}. We therefore advise the user to avoid using these functions to
retrieve or modify an object's type. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Recoding factors}
In \R{}, the value of categorical variables is stored in \code{factor} variables.

A \code{factor} basically an integer vector endowed with a table specifying what integer 
value corresponds to what \code{level}. The values in this translation table can be requested
with the \code{levels} function.
<<>>=
f <- factor(c("a","b","a","a","c"))
levels(f)
@
The use of integers combined with a translation table is not uncommon in statistical sofware,
so chances are that you eventually have to make such a translation by hand. For example,
suppose we read in a vector where 1 stands for `male', 2 stands for `female' and 0 stands for unknown.
Conversion to a factor variable can be done as in the example below. 
<<>>=
# example:
gender <- c(2,1,1,2,0,1,1)
# recoding table, stored in a simple vector
recode <- c(male=1, female=2)
(gender <- factor(gender,levels=recode, labels=names(recode)) )
@ 
Note that we do not explicitly need to set \code{NA} as a label.  Every integer
value that is encountered in the first argument, but not in the \code{levels}
argument will be regarded missing.


Levels in a factor variable have no natural ordering. However in multivariate (regression) analyses
it can be beneficial to fix one of the levels as the reference level. \R{}'s standard multivariate
routines (\code{lm}, \code{glm}) use the first level as reference level. The \code{relevel} function
allows you to determine which level comes first.
%
<<>>=
(gender <- relevel(gender, ref='female'))
@
Levels can also be reordered, depending on the mean value of another variable, for example:
<<>>=
age <- c(27, 52, 65, 34, 89, 45, 68)
(gender <- reorder(gender, age))
@
Here, the means are added as a named vector attribute to \code{gender}. It can be removed
by setting that attribute to \code{NULL}.
<<>>=
attr(gender,'scores') <- NULL
gender
@


%\begin{tip}{Bad practice}
%\marginpar{Ik begrijp uit de tekst niet waarom?}
%Although it is very easy to manipulate the \code{levels} of a \code{factor}, it is often better and more clear
%to use \code{factor} for this functionality.
%<<>>=
%# dirty recoding, avoid it!
%levels(f) <- c("A","B", NA)
%f
%@
%\end{tip}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Converting dates}
The base \R{} installation has three types of objects to store a time instance:
\code{Date}, \code{POSIXlt} and \code{POSIXct}. The \code{Date} object
can only be used to store dates, the other two store date and/or time. Here, we
focus on converting text to \code{POSIXct} objects since this is the
most portable way to store such information.

Under the hood, a \code{POSIXct} object stores the number of seconds that have
passed since January 1, 1970 \mbox{00:00}. Such a storage format facilitates the calculation of
durations by subtraction of two \code{POSIXct} objects. 

When a \code{POSIXct} object is printed, \code{R} shows it in a human-readable
calender format. For example, the command \code{Sys.time} returns the system
time provided by the operating system in \code{POSIXct} format.
<<>>=
current_time <- Sys.time()
class(current_time)
current_time
@ 
Here, \code{Sys.time} uses the time zone that is stored in the \code{locale} settings
of the machine running \R{}. 

Converting from a calender time to \code{POSIXct} and back is not entirely
trivial, since there are many idiosyncrasies to handle in calender systems.
These include leap days, leap seconds, daylight saving times, time zones and so
on.  Converting from text to \code{POSIXct} is further complicated by the many
textual conventions of time/date denotation. For example, both \code{28
September 1976} and \code{1976/09/28} indicate the same day of the same year.
Moreover, the name of the month (or weekday) is language-dependent, where the
language is again defined in the operating system's locale settings.


The \code{lubridate} package\cite{grolemund:2011} contains a number of
functions facilitating the conversion of text to \code{POSIXct} dates.
As an example, consider the following code.
%
<<message=FALSE>>=
library(lubridate)
dates <- c("15/02/2013","15 Feb  13","It happened on 15 02 '13")
dmy(dates)
@
Here, the function \code{dmy} assumes that dates are denoted in the order
day-month-year and tries to extract valid dates. Note that the code above
will only work properly in locale settings where the name of the second month
is abbreviated to \emph{Feb}. 
This holds for English or Dutch locales, but fails
for example in a French locale (\emph{F\'evrier}).

There are similar functions
for all permutations of \code{d}, \code{m} and \code{y}. Explicitly, all of the
following functions exist.
\begin{center}
\begin{tabular}{ccc}
\code{dmy} & \code{myd} &\code{ydm}\\
\code{mdy} & \code{dym} &\code{ymd}\\
\end{tabular}
\end{center}
So once it is known in what order days, months and years are denoted,
extraction is very easy. 

\begin{tip}{Note}
It is not uncommon to indicate years with two numbers, leaving out the indication
of century. In \R{}, 00-68 are interpreted as 2000-2068 and 69-99 as 1969-1999.
<<>>=
dmy("01 01 68")
dmy("01 01 69")
@
This behaviour is according to the 2008 POSIX standard, but one should expect that this
interpretation changes over time.
\end{tip}


It should be noted that \code{lubridate} (as well as \R{}'s base functionality) is
only capable of converting certain standard notations. For example, the following
notation does not convert.
<<>>=
dmy("15 Febr. 2013")
@
The standard notations that can be recognized by \R{}, either using \code{lubridate}
or \R{}'s built-in functionality are shown in Table \ref{tab:dateformats}.
%
\begin{table}
\begin{threeparttable}
  \caption{Day, month and year formats recognized by \R{}.}
  \label{tab:dateformats}
  \begin{tabular}{lll}
    \hline
    Code       & description                                    & Example  \\
    \hline
    \code{\%a} & Abbreviated weekday name in the current locale.& \code{Mon}  \\
    \code{\%A} & Full weekday name in the current locale.       & \code{Monday}\\
    \code{\%b} & Abbreviated month name in the current locale.  & \code{Sep} \\
    \code{\%B} & Full month name in the current locale.         & \code{September}\\
    \code{\%m} & Month number (01-12)                           & \code{09}\\
    \code{\%d} & Day of the month as decimal number (01-31).    & \code{28} \\
    \code{\%y} & Year without century (00-99)                   & \code{13}\\
    \code{\%Y} & Year including century.                        & \code{2013}\\
    \hline
  \end{tabular}
\end{threeparttable}
\end{table}
%
%
Here, the names of (abbreviated) week or month names that are sought for in the
text depend on the locale settings of the machine that is running \R{}. For example,
on a PC running under a Dutch locale, ``\code{maandag}'' will be recognized as the
first day of the week while in English locales ``\code{Monday}'' will be recognized. If
the machine running \R{} has multiple locales installed you may add the argument
\code{locale} to one of the \code{dmy}-like functions. In \code{linux}-alike systems
you can use the command \code{locale -a} in \code{bash} terminal to see the list of
installed locales. In \code{Windows} you can find available locale settings under
``language and regional settings'', under the configuration screen.

If you know the textual format that is used to describe a date in the input, you may
want to use \R{}'s core functionality to convert from text to \code{POSIXct}. This
can be done with the \code{as.POSIXct} function. It takes as arguments a \code{character}
vector with time/date strings and a string describing the format. 
<<>>=
dates <- c("15-9-2009","16-07-2008","17 12-2007","29-02-2011")
as.POSIXct(dates,format="%d-%m-%Y")
@
In the format string, date and time fields are indicated by a letter preceded
by a percent sign (\code{\%}). Basically, such a \code{\%}-code tells \R{} to
look for a range of substrings. For example, the \code{\%d} indicator makes
\R{} look for numbers \code{1-31} where precursor zeros are allowed, so
\code{01}, \code{02},$\ldots$\code{31} are recognized as well. Table
\ref{tab:dateformats} shows which date-codes are recognized by \R{}. The
complete list can be found by typing \code{?strptime} in the \R{} console.
Strings that are not in the exact format specified by the \code{format}
argument (like the third string in the above example) will not be converted by
\code{as.POSIXct}.  Impossible dates, such as the leap day in the fourth date
above are also not converted.

Finally, to convert dates from \code{POSIXct} back to character, one may use
the \code{format} function that comes with base \R{}. It accepts a \code{POSIXct}
date/time object and an output format string.
<<>>=
mybirth <- dmy("28 Sep 1976")
format(mybirth,format="I was born on %B %d, %Y")
@


\subsection{\code{character} manipulation}
\label{sssect:charactermanipulation}

Because of the many ways people can write the same things down, character data
can be difficult to process.  For example, consider the following excerpt of a
data set with a \code{gender} variable.
<<echo=2>>=
data.frame(gender=c("M","male ","Female","fem."))
@
If this would be treated as a factor variable without any preprocessing,
obviously four, not two classes would be stored. The job at hand is therefore
to automatically recognize from the above data whether each elements pertains
to \code{male} or \code{female}. In statistical contexts, classifying such
``messy'' text strings into a number of fixed categories is often referred to
as \emph{coding}.

Below we discuss two complementary approaches to string coding: \emph{string
normalization} and \emph{approximate text matching}. In particular, the
following topics are discussed.
\begin{itemize}[itemsep=0pt]
\item Remove prepending or trailing white spaces.
\item Pad strings to a certain width.
\item Transform to upper/lower case.
\item Search for strings containing simple patterns (substrings).
\item Approximate matching procedures based on string distances.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{String normalization}
\label{sect:stringnormalisation}
%
String normalization techniques are aimed at transforming a variety of strings
to a smaller set of string values which are more easily processed.
By default, \R{} comes with extensive string manipulation functionality that is
based on the two basic string operations: \emph{finding} a pattern in a string
and \emph{replacing} one pattern with another. We will deal with \R{}'s
generic functions below but start by pointing out some common string cleaning
operations.
  
The \code{stringr} package\cite{wickham:2009} offers a number of functions that
make some some string manipulation tasks a lot easier than they would be with
\R{}'s base functions. For example, extra white
spaces at the beginning or end of a string can be removed using
\code{str\_trim}.
<<>>=
library(stringr)
str_trim("  hello world ")
str_trim("  hello world ",side="left")
str_trim("  hello world ",side="right")
@
Conversely, strings can be padded with spaces or other characters with
\code{str\_pad} to a certain width. For example, numerical codes are often
represented with prepending zeros.
<<>>=
str_pad(112,width=6,side='left', pad=0)
@
Both \code{str\_trim} and \code{str\_pad} accept a \code{side} argument to
indicate whether trimming or padding should occur at the beginning
(\code{left}), end (\code{right}) or both sides of the string. 

Converting strings to complete upper or lower case can be done with \R{}'s
built-in \code{toupper} and \code{tolower} functions.
<<>>=
toupper("Hello world")
tolower("Hello World")
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Approximate string matching}
\label{sect:appstringmatching}
There are two forms of string matching. The first consists of determining
whether a (range of) substring(s) occurs within another string. In this case
one needs to specify a range of substrings (called a \emph{pattern}) to search
for in another string. In the second form one defines a distance metric between
strings that measures how ``different'' two strings are.  Below we will give
a short introduction to pattern matching and string distances with \R{}.

There are several pattern matching functions that come with base \R{}. The most
used are probably \code{grep} and \code{grepl}. Both functions take a pattern
and a \code{character} vector as input. The output only differs in that
\code{grepl} returns a logical index, indicating which element of the input
\code{character} vector contains the pattern, while \code{grep} returns a
numerical index. You may think of \code{grep(...)} as \code{which(grepl(...))}.

%
%
In the most simple case, the pattern to look for is a simple substring. For
example, using the data of the example on page
\pageref{sect:stringnormalisation}, we get the following.
<<>>=
gender <- c("M","male ","Female","fem.")
grepl('m',gender)
grep('m',gender)
@
Note that the result is case sensitive: the capital \code{M} in the first
element of \code{gender} does not match the lower case \code{m}. There are
several ways to circumvent this case sensitivity.  Either by case normalization
or by the optional argument \code{ignore.case}.
<<>>=
grepl('m',gender,ignore.case=TRUE)
grepl('m',tolower(gender))
@
Obviously, looking for the occurrence of \code{m} or \code{M} in the
\code{gender} vector does not allow us to determine which strings pertain to
\code{male} and which not. Preferably we would like to search for strings that
start with an \code{m} or \code{M}.  Fortunately, the search patterns that
\code{grep} accepts allow for such searches. The beginning of a string is
indicated with a caret (\code{\^}).
<<>>=
grepl('^m',gender,ignore.case=TRUE)
@
Indeed, the \code{grepl} function now finds only the first two elements of
\code{gender}.  The caret is an example of a so-called \emph{meta-character}.
That is, it does not indicate the caret itself but something else, namely the
beginning of a string. The search patterns that \code{grep}, \code{grepl} (and
\code{sub}
and \code{gsub}) understand have more of these meta-characters, namely: 
\begin{verbatim}
   . \ | ( ) [ {  ^ $ * + ? 
\end{verbatim}
\label{lab:regexp}
If you need to search a string for any of these characters, you can use the
option \code{fixed=TRUE}.
<<>>=
grepl('^',gender,fixed=TRUE)
@
This will make \code{grepl} or \code{grep} ignore any meta-characters in the
search string.

Search patterns using meta-characters are called \emph{regular expressions}.
Regular expressions offer powerful and flexible ways to search (and alter)
text.  A discussion of regular expressions is beyond the scope of these lecture
notes. However, a concise description of regular expressions allowed by \R{}'s
built-in string processing functions can be found by typing \code{?regex} at
the \R{} command line. The books by Fitzgerald\cite{fitzgerald:2012} or
Friedl\cite{friedl:2006} provide a thorough introduction to the subject of
regular expression. If you frequently have to deal with ``messy'' text
variables, learning to work with regular expressions is a worthwhile
investment. Moreover, since many popular programming languages support some
dialect of regexps, it is an investment that could pay off several times.

We now turn our attention to the second method of approximate matching, namely
string distances.  A string distance is an algorithm or equation that indicates
how much two strings differ from each other.  An important distance measure is
implemented by the \R{}'s native \code{adist} function. This function counts
how many basic operations are needed to turn one string into another. These
operations include insertion, deletion or substitution of a single
character\cite{levenshtein:1966}.  For example
<<>>=
adist("abc","bac")
@
The result equals two since turning \code{"abc"} into \code{"bac"} involves two
character substitutions: 
\begin{center}
\code{abc}$\to$\code{bbc}$\to$\code{bac}.
\end{center}

Using \code{adist}, we can compare fuzzy text strings
to a list of known codes. For example:
\label{example:gendercode}
<<>>=
codes <- c('male','female')
D <- adist(gender,codes)
colnames(D) <- codes
rownames(D) <- gender
D
@
Here, \code{adist} returns the distance matrix between our vector of fixed
codes and the input data. For readability we added row- and column names
accordingly. Now, to find out which code matches best with our raw data, we
need to find the index of the smallest distance for each row of \code{D}. This
can be done as follows.
<<>>=
i <- apply(D,1,which.min)
data.frame(
  rawtext = gender,
  coded   = codes[i]
)
@
We use \code{apply} to apply \code{which.min} to every row of \code{D}. Note
that in the case of multiple minima, the first match will be returned. At the
end of this subsection we show how this code can be simplified with the
\code{stringdist} package.

Finally, we mention three more functions based on string distances. First, the
\R{}-built-in function \code{agrep} is similar to grep, but it allows one to
specify a maximum Levenshtein distance between the input pattern and the found
substring. The \code{agrep} function allows for searching for regular
expression patterns, which makes it very flexible.

Secondly, \code{stringdist} package\cite{loo:2013} offers a function called
\code{stringdist} which can compute a variety of string distance metrics, some
of which are likely to provide results that are better than \code{adist}'s.
Most importantly, the distance function used by \code{adist} does not allow for
character transpositions, which is a common typographical error. Using the
\emph{optimal string alignment distance} (the default choice for
\code{stringdist}) we get
<<message=FALSE>>=
library(stringdist)
stringdist("abc","bac")
@
The answer is now 1 (not 2 as with \code{adist}), since the optimal string
alignment distance allows for transpositions of adjacent characters:
\begin{center}
\code{abc}$\to$ \code{bac}.
\end{center}

Thirdly, the \code{stringdist} package provides a function called
\code{amatch}, which mimics the behaviour of \R{}'s \code{match} function:
it returns the an index to the closest match within a maximum distance.
Recall the \code{gender} and \code{code} example of page \pageref{example:gendercode}.
<<tidy=FALSE>>=
# this yields the closest match of 'gender' in 'codes' (within a distance of 4)
(i <- amatch(gender,codes,maxDist=4))

# store results in a data.frame
data.frame(
  rawtext = gender
  , code = codes[i]
  )
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Character encoding issues}
\label{sect:encoding}
A \emph{character encoding system} is a system that defines how to translate
each character of a given alphabet into a computer byte or sequence of
bytes\footnote{In fact, the definition can be more general, for example to
include Morse code. However, we limit ourselves to computerized character
encodings.}. For example, \code{ASCII} is an encoding system that prescribes
how to translate 127 characters into single bytes (where the first bit of each
byte is necessarily 0). The \code{ASCII} characters include the upper and lower
case letters of the Latin alphabet (\code{a-z}, \code{A-Z}), Arabic numbers
(\code{0}-\code{9}), a number of punctuation characters and a number of
invisible so-called control characters such as newline and carriage return. 


Although it is widely used, \code{ASCII} is obviously incapable of encoding
characters outside the Latin alphabet, so you can say ``hello'', but not
``$\gamma\varepsilon\iota\alpha$ $\sigma\alpha\varsigma$'' in this encoding.
For this reason, a number of character encoding systems have been developed
that extend \code{ASCII} or replace it all together. Some well-known schemes
include \code{UTF-8} and \code{latin1}. The character encoding scheme that is
used by default by your operating system is defined in your \code{locale}
settings. Most Unix-alikes use \code{UTF-8} by default while older
\code{Windows} applications, including the \code{Windows} version of \code{R}
use \code{Windows-1252} (a superset of \code{latin1}). The \code{UTF-8}
encoding standard is widely used to encode web pages: according to a frequently
repeated survey of \code{w3techs}\cite{w3techs:2013}, about 75\% of the 10
million most visited web pages are encoded in \code{utf-8}. You can find out
the characer encoding of your system by typing
<<eval=FALSE>>=
Sys.getlocale("LC_CTYPE")
@
at the \R{} commandline.

For \R{} to be able to correctly read in a textfile, it must understand which
character encoding scheme was used to store it. By default, \R{} assumes that a
textfile is stored in the encoding scheme defined by the operating system's
\code{locale} setting.  This may fail when the file was not generated on the
same computer that \R{} is running on but was obtained from the web, for
example.  To make things worse, it is impossible to determine automatically
with certainty from a file what encoding scheme has been used. This means that
you may run into situations where you have to tell \R{} literally in which
encoding a file has been stored. Once a file has been read into \R{}, a
character vector will internally be stored in either \code{UTF-8} or
\code{latin1}. 

The \code{fileEncoding} argument of \code{read.table} and its relatives tells
\R{} what encoding scheme was used to store the file. For \code{readLines} the
file encoding must be specified when the file is opened, before calling
\code{readLines}, as in the example below.
<<eval=FALSE>>=
# 1. open a connection to your file, specifying its encoding
  f <- file('myUTF16file.txt',encoding='UTF-16')
# 2. Read the data with readLines. 
#    Text read from the file is converted to uft8 or latin1
  input <- readLines(f)
# close the file connection.
  close(f)
@
%
%
When reading the file, \R{} will not translate the encoding to \code{UTF-8} or
\code{latin1} by itself, but instead relies on libraries offered by the
operating system.  \R{}'s \code{iconv} function forms an interface to the
OS-specific character encoding conversion libraries. With \code{iconvlist()}
you can check what encodings can be translated by your operating system.

\newpage
\subsection*{Exercises}
\addcontentsline{toc}{subsection}{Exercises}


\begin{exercise}
Type conversions.

  \begin{subex}
    \item Load the buildtin \code{warpbreaks} data set .
          Find out, in a single command, which columns of \code{warpbreaks} are either \code{numeric} or \code{integer}.
    \item Is \code{numeric} a natural data type for the columns which are stored as such? Convert to integer when necessary.
          (See also \code{?warpbreaks} for an explanation of the data).
    \item Error messages in \R{} sometimes report the underlying \emph{type} of an object rather than the user-level \emph{class}.
          Derive from the following code and error message what the underlying type of an \R{} function is.
          <<>>=
          mean[1]
          @
    Confirm your answer using \code{typeof}.
\end{subex}
\end{exercise}

\begin{exercise}
\label{ex:factor}
Type the following code in your \R{} terminal.
<<>>=
v <- factor(c("2","3","5","7","11"))
@
\begin{subex}
  \item Convert \code{v} to character with \code{as.character}. Explain what just happened.
  \item Convert \code{v} to numeric with \code{as.numeric}. Explain what just happened.
  \item How would you convert the values of \code{v} to integers?
\end{subex}

\end{exercise}

\begin{exercise}
In this exercise we'll use \code{readLines} to read in an irregular textfile.
The file looks like this (without numbering).
\begin{center}
\lstinputlisting[
    xleftmargin=1cm,
    numbers=left,
    basicstyle=\ttfamily\scriptsize
  ]{files/example.txt}
\end{center}
You may copy the text from this \code{pdf} file in a textfile called
\code{example.txt} or download the file from our
\href{https://github.com/edwindj/datacleaning/blob/master/Rnw/syllabus/files}{Github
page}.

\begin{subex}
  \item Read the complete file using \code{readLines}.
  \item Separate the \code{vector} of lines into a \code{vector} containing
    comments and a \code{vector} containing the data.  Hint: use \code{grepl}.
  \item Extract the date from the first comment line.
  \item Read the data into a \code{matrix} as follows.
  \begin{enumerate}
    \item Split the character vectors in the vector containing \code{data}
      lines by semicolon (\code{;}) using \code{strsplit}.
    \item Find the maximum number of fields retrieved by \code{split}. Append
      rows that are shorter with \code{NA}'s.
    \item Use \code{unlist} and \code{matrix} to transform the data to
      row-column format.
  \end{enumerate}
  \item From comment lines \code{2-4}, extract the names of the fields. Set
    these as \code{colnames} for the \code{matrix} you just created.
\end{subex}
\end{exercise}

\begin{exercise}
We will coerce the columns of the data of the previous exercise to a structured data set.
\begin{subex}
  \item Coerce the matrix to a \code{data.frame}, making sure all columns are
    \code{character} columns.
  \item Use a string distance technique to transform the \code{Gender} column into
    a \code{factor} variable with labels \code{man} and \code{woman}.
  \item Coerce the \code{Age} column to \code{integer}.
  \item Coerce the \code{weight} column to \code{numeric}. Hint: use \code{gsub} to replace
   comma's with a period.
\end{subex}
\end{exercise}





