\documentclass{beamer}

\usecolortheme{CBS}
\usetheme{CBS}
\title{Part II: from technically correct to consistent data}
\author{\hfill Edwin de Jonge and Mark van der Loo}
\date{\hfill July 09 2013\\ \hfill\emph{useR!2013}}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\R}{\code{R}} %% call as \R{} to avoid extra spaces.

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,decorations.pathreplacing}
  \tikzstyle{statpoint}=[
            fill=blue!10,
            draw, rectangle,
            rounded corners,
            text width=3cm,
            font=\scriptsize\bf\sf,
            node distance=1.4cm,
            align=center]
  \tikzstyle{arr}=[->,thick,>=stealth',color=black]
  \tikzstyle{action}=[right, font=\scriptsize\sf]
  \tikzstyle{file}=[
    fill=green!10,
    draw, rectangle,
    node distance=6cm,
    text width=2.3cm,
    font=\scriptsize\bf\sf,
    align=center
  ]
  \tikzstyle{store}=[->,thick,>=stealth',color=black]
  \tikzstyle{save}=[above,font=\scriptsize\sf]

\begin{document}

<<setup, echo=FALSE, include=FALSE, cache=FALSE>>=
library(knitr)
opts_chunk$set(size='scriptsize')
library(editrules)
@
 
\CBStitleframe
 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Statistical analyses}
\begin{frame}[fragile]
   \begin{tikzpicture}
     \node[statpoint] (raw) {Raw data};
     \node[statpoint, below of=raw] (input)   {Technically correct data};
     \node[statpoint, below of=input] (micro) {Consistent data};
%     \node[statpoint, below of=micro] (stat)  {Statistical results};
     \draw[arr] (raw.south) to node[action]   {type checking, normalizing} (input.north);
     \draw[arr] (input.south) to node[action] {fix and impute} (micro.north);
%     \draw[arr] (micro.south) to node[action] {estimate, analyze} (stat.north);
%     \draw[arr] (stat.south) to node[action]  {tabulate, plot} (output.north);
     \draw[decorate,
       decoration={
         raise=6pt,
         brace,
         amplitude=10pt},
         thick](micro.west) -- 
                 node[sloped,above=0.5cm,font=\scriptsize\sf] {data cleaning}
               (raw.west);
% 
%   \only<2->{
%     \node[file,right of=raw] (pi) {pre-input};
%     \draw[store,dotted] (raw.east) to node[save]{store} (pi.west);
%   }
%   \only<3->{
%     \node[file,right of=input] (ip) {input};
%     \draw[store,dotted] (input.east) to node[save]{store} (ip.west);
%   }
%   \only<4->{
%     \node[file,right of=micro] (mi) {microdata};
%     \draw[store,dotted] (micro.east) to node[save]{store} (mi.west);
%     \node[file,right of=stat] (st) {stats};
%     \draw[store,dotted] (stat.east) to node[save]{store} (st.west);
%     \node[file,right of=output] (rp) {report};
%     \draw[store,dotted] (output.east) to node[save]{store} (rp.west);
%   }
   \end{tikzpicture}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Consistent data}

{\usebackgroundtemplate{
  \hspace{2cm}
  \includegraphics[height=\paperheight]{img/i-love-consistent.jpg}
  }
  \setbeamertemplate{footline}{}
  \begin{frame}
    \begin{block}{Consistent data?}<2->
    \em \emph{Consistent data} is \emph{technically correct data} that is fit for
statistical analysis.
    so it is data without:
    \begin{itemize}
      \item Special values (\code{NA}, \code{NaN}, \code{Inf})
      \item Implausible outliers
      \item Obvious errors
    \end{itemize}
    \end{block}  
  \end{frame}
}

\section{Why consistency?}
\begin{frame}[fragile]
Special values obstruct most statistics:
<<>>=
age <- c(23, 16, Inf, NA)
income <- c(2000, 2500, 1000, NaN)

# Not a useful output...
c(mu_income=mean(income), mu_age=mean(age))

# error!
coef(lm(income ~ age))
@
\end{frame}

\begin{frame}[fragile]
Obvious errors corrupt your statistics:
<<>>=
age <- c(23, 36, 2000, 0)
income <- c(2000, 3000, -100000, 80000)

c(mu_income=mean(income), mu_age=mean(age))
c1 <- coef(lm(income ~ age))
c2 <- coef(lm(head(income,2) ~ head(age,2))) # remove last observations
rbind(c1,c2)
@
\end{frame}

\section{Towards consistency}
\begin{frame}
  \begin{enumerate}
    \item {\bf Detect} errors and special values
    \item {\bf Remove} or {\bf Correct} values.
    \item {\bf Impute} missing values
  \end{enumerate}
  
  \begin{block}{Note}
    \em All these actions have impact on your analysis. 
    They are data operations and should be scripted for reproducability.
    (Did I hear \R{}?)
  \end{block}
\end{frame}

\section{}
{\usebackgroundtemplate{
  \includegraphics[width=\paperwidth]{img/detection.jpg}
  }
  \setbeamertemplate{footline}{}
  \begin{frame}
  \begin{block}{Detect}<2->
    \begin{itemize}
      \item Special values
      \item Outliers
      \item (Multivariate) rule restrictions
    \end{itemize}
  \end{block}
  \end{frame}
}
% 

\section{Detection: \code{NA}}
\begin{frame}[fragile]
For detecting which rows contain \code{NA}, \code{complete.cases} is useful:
<<>>=
complete.cases(data.frame(x=c(1,NA,3), y=c(1,1,NA)))
@

Of course the same can be done with \code{is.na}

<<>>=
sapply(data.frame(x=c(1,NA,3), y=c(1,1,NA)), is.na)
@
\end{frame}

\section{Detection: all special values}
\begin{frame}[fragile]
\code{NA} is not enough. You should check all values:
\code{NA},\code{NaN}, \code{Inf} and \code{-Inf}.

To do this write a utility function \code{is.special}:
<<tidy=FALSE>>=
is.special <- function(x){
  if (is.numeric(x)) !is.finite(x) else is.na(x)
}
dat <- data.frame(x=factor(c(NA, "M","F")), y=c(1,NaN, Inf))
sapply(dat, is.special)
@
\end{frame}

\section{Detection: univariate outliers}
\begin{frame}[fragile]
Outliers can heavily influence the statistical outcome, so they 
should be detected.\footnote{Note that outliers are not necessarily errors}

<<echo=FALSE>>=
(x <- sample(c(rnorm(10), 40.232)))
@

What are the outliers?
\end{frame}

\begin{frame}[fragile]

<<fig.height=3>>=
boxplot(x, horizontal=T)
@

Or better use \code{boxplot.stats}:
\code{boxplot.stats} lists the outliers.
<<fig.height=2>>=
boxplot.stats(x)$out
@
\end{frame}

\begin{frame}[fragile]
  For skewed data, with $x^*$ (usually) the median of $x$
  \begin{equation*}
    h(x) = \max\left(
    \frac{x}{x^*},\frac{x^*}{x} 
    \right)
    \geq r,\textrm{ with } x>0.\footnote{Hiridoglou and Berthelot}
  \end{equation*}
  works better.
  In \R{}:
<<eval=FALSE>>=
hboutlier <- function(x,r){
  xref <- median(x, na.rm=TRUE)
  max(x/xref, xref/x, na.rm=TRUE) > r
}
@
\end{frame}
\section{Detection using rules}
\begin{frame}
  Often you have background knowledge on data:
\begin{itemize}
  \item A person having a drivers license is at least 16 years old
  \item Age cannot be negative
  \item Profit = turnover - cost
\end{itemize}
  This knowledge can be expressed in constraints / rules / edits.
  
  \begin{block}{Hard rules}
    A rule that must be obeyed by a record, is a {\em hard rule}.
    If not it is a {\em soft rule}. Soft rule may indicate suspicious data
  \end{block}
\end{frame}

\begin{frame}
  Often data is dirty: it violates hard rules.
  So first:
  \begin{itemize}
    \item Check which records violate hard rules.
    \item Find out which values are the cause.
    \item Replace this value with \code{NA}, so it can be corrected or imputed.
  \end{itemize}
  Rule checking can be done straight in \R{}, but for set of rules this is problematic.
  Rules interact and it may be difficult to find the causing value.
\end{frame}

\section{Detection: \code{editrules}}
\begin{frame}
  \code{editrules} (de Jonge, van der Loo, 2012) is a toolbox to specify, maintain and check constraints/rules on data.
  It allows:
  \begin{itemize}
  \item Specifying rules in \R{} syntax
  \item Store rules in a separate file (using \code{editfile})
  \item Check for violations: \code{violatedEdits}
  \item Find erronous values: \code{localizeErrors}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
<<tidy=FALSE>>=
library(editrules)
(E <- editset(c( "profit == turnover - cost"
               , "driverlicence %in% c(TRUE,FALSE)"
               , "if (driverlicence) age > 15")
               )
)
@
\end{frame}

\section{\code{editrules::violatedEdits}}
\begin{frame}[fragile]
\code{violatedEdits} finds out which records violate which constraints.
<<tidy=FALSE>>=
E <- editset(c( "driverlicence %in% c(TRUE,FALSE)"
              , "if (driverlicence) age > 15"))
x <- data.frame(driverlicence=c(TRUE), age=c(12,18))
violatedEdits(E, x)
@  
\end{frame}

\section{\code{editrules::localizeErrors}}
\begin{frame}[fragile]
\code{localizeErrors} looks for the minimum number of values that should be changed.
<<tidy=FALSE>>=
E <- editset(c( "driverlicence %in% c(TRUE,FALSE)"
              , "if (driverlicence) age > 15"))
x <- data.frame(driverlicence=c(TRUE), age=c(12,18))
localizeErrors(E, x, method="mip")$adapt
@  
\end{frame}

\section{}
{
  \usebackgroundtemplate{
    \hspace{2cm}
    \includegraphics[height=\paperheight]{img/correction.jpg}
  }
  \setbeamertemplate{footline}{}
  \begin{frame}
  \begin{block}{Correction}<2->
    \begin{itemize}
      \item Remove invalid values
      \item Using (correction) rules
    \end{itemize}
  \end{block}
  \end{frame}
}

\section{Correction: removal}
\begin{frame}[fragile]
  \begin{itemize}
  \item Replace invalid values with \code{NA}\\
  This value still needs to be fixed!
  \item Or remove the observation.\\
  This may leave you with very few data to analyze...
 \end{itemize}
\end{frame}


\section{}
{ \usebackgroundtemplate{
    \includegraphics[width=\paperwidth]{img/imputation.jpg}
  }
  \setbeamertemplate{footline}{}
  \begin{frame}
    \begin{block}{Imputation: Filling in the blanks}<2->
    Imputation fills in the gaps in your data, so that {\em all} of your
    observations can be used for analysis. 
    Other options are:
    \begin{itemize}
      \item Remove all incomplete observations
      \item Remove \code{NA} per statistical operation.
    \end{itemize}
    \end{block}
  \end{frame}
}

\section{Imputation}
\begin{frame}
 There is a vast body of literature on imputation methods
and is goes beyond the scope of this tutorial.
\begin{block}{R packages with imputation methods}
  \code{Amelia}, \code{BaBoon}, \code{cat}, \code{deducorrect},
  \code{e1071}, \code{ForImp}, \code{Hmisc}, \code{imputation},
  \code{impute}, \code{mi}, \code{mice}, \code{mix}, \code{norm},
  \code{robCompositions}, \code{rrcovNA}, \code{StatMatch}, \code{VIM},
  \code{yaImpute}, \code{zoo}
\end{block}

\end{frame}
  
\section{Sequential hot deck imputation}
\begin{frame}
  Imputation methods are based on some sort of regression.
  This can be:
  \begin{itemize}
  \item Mean or glm-type regressions
  \item Multiple Imputation regression
  \item Local methods, e.g. nearest neighbor methods.
  \end{itemize}
\end{frame}

\section{Nearest neighbor}
\begin{frame}
  Fill missing value using a nearby observation.
  So needs:
  \begin{itemize}
    \item $d$ a measure of distance (e.g. euclidean)
    \item $k$ the number of neighbors that should be averaged.
  \end{itemize}
\end{frame}

\section{Sequential hot deck Imputation}
\begin{frame}
   Very simple recipe:
   \begin{itemize}
      \item Sort the data set
      \item Replace each \code{NA} at row \code{i} with value from \code{i+1}
   \end{itemize}
   Works when distance can be approximated by one ordered variable.
   Implementation in syllabus.
\end{frame}

\section{kNN}
\begin{frame}
 k-nearest-neighbor imputation. Implementation in \code{VIM}
 \begin{itemize}
    \item Uses Gower's distance
\begin{equation*}
d_{\sf g}(i,j) \frac{\sum_k w_{ijk} d_k(i,j)}{\sum_{k}w_{ijk}},
\end{equation*}
  \item Uses $k$ neighbors
 \end{itemize}
 
Usage is simple 
<<eval=FALSE>>=
library(VIM)

kNN(data, variable = colnames(data), k = 5)
@

\end{frame}

\section{Start analyzing!}
\begin{frame}
 After data cleaning, the statistical analyzing can begin. 
 However: 
 \begin{itemize}
 \item During analysis you may encounter extraordinary
 values
 \item May be the data
 cleaning process was too thorough. 
 \end{itemize}
 \begin{block}{R}
 Nice thing about \R{}
 you can correct it and rerun the data cleaning process and analysis!
 \end{block}
\end{frame}
\end{document}


