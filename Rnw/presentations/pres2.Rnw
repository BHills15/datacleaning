\documentclass{beamer}

\usecolortheme{CBS}
\usetheme{CBS}
\title{Part II: from technically correct to consistent data}
\author{\hfill Edwin de Jonge and Mark van der Loo}
\date{\hfill July 09 2013\\ \hfill\emph{useR!2013}}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\R}{\code{R}} %% call as \R{} to avoid extra spaces.


\usepackage{tikz}
\usetikzlibrary{shapes,arrows,decorations.pathreplacing}
  \tikzstyle{statpoint}=[
            fill=blue!10,
            draw, rectangle,
            rounded corners,
            text width=2.3cm,
            font=\scriptsize\bf\sf,
            node distance=1.4cm,
            align=center]
  \tikzstyle{arr}=[->,thick,>=stealth',color=black]
  \tikzstyle{action}=[right, font=\scriptsize\sf]
  \tikzstyle{file}=[
    fill=green!10,
    draw, rectangle,
    node distance=6cm,
    text width=2.3cm,
    font=\scriptsize\bf\sf,
    align=center
  ]
  \tikzstyle{store}=[->,thick,>=stealth',color=black]
  \tikzstyle{save}=[above,font=\scriptsize\sf]

\begin{document}

<<setup, echo=FALSE, include=FALSE, cache=FALSE>>=
library(knitr)
opts_chunk$set(size='scriptsize')
@
 
\CBStitleframe
 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Statistical analyses}
\begin{frame}[fragile]
   \begin{tikzpicture}
     \node[statpoint] (raw) {Raw data};
     \node[statpoint, below of=raw] (input)   {Technically correct data};
     \node[statpoint, below of=input] (micro) {Consistent data};
%     \node[statpoint, below of=micro] (stat)  {Statistical results};
     \draw[arr] (raw.south) to node[action]   {type checking, normalizing} (input.north);
     \draw[arr] (input.south) to node[action] {fix and impute} (micro.north);
%     \draw[arr] (micro.south) to node[action] {estimate, analyze} (stat.north);
%     \draw[arr] (stat.south) to node[action]  {tabulate, plot} (output.north);
     \draw[decorate,
       decoration={
         raise=6pt,
         brace,
         amplitude=10pt},
         thick](micro.west) -- 
                 node[sloped,above=0.5cm,font=\scriptsize\sf] {data cleaning}
               (raw.west);
% 
%   \only<2->{
%     \node[file,right of=raw] (pi) {pre-input};
%     \draw[store,dotted] (raw.east) to node[save]{store} (pi.west);
%   }
%   \only<3->{
%     \node[file,right of=input] (ip) {input};
%     \draw[store,dotted] (input.east) to node[save]{store} (ip.west);
%   }
%   \only<4->{
%     \node[file,right of=micro] (mi) {microdata};
%     \draw[store,dotted] (micro.east) to node[save]{store} (mi.west);
%     \node[file,right of=stat] (st) {stats};
%     \draw[store,dotted] (stat.east) to node[save]{store} (st.west);
%     \node[file,right of=output] (rp) {report};
%     \draw[store,dotted] (output.east) to node[save]{store} (rp.west);
%   }
   \end{tikzpicture}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Consistent data}

{\usebackgroundtemplate{
  \hspace{2cm}
  \includegraphics[height=\paperheight]{img/i-love-consistent.jpg}
  }
  \setbeamertemplate{footline}{}
  \begin{frame}
    \begin{block}{Consistent data?}<2->
    \em \emph{Consistent data} is \emph{technically correct data} that is fit for
statistical analysis.
    so it is data without:
    \begin{itemize}
      \item Special values (\code{NA}, \code{NaN}, \code{Inf})
      \item Implausible outliers
      \item Obvious errors
    \end{itemize}
    \end{block}  
  \end{frame}
}

\section{Why consistency?}
\begin{frame}[fragile]
Special values obstruct most statistics:
<<>>=
age <- c(23, 16, Inf, NA)
income <- c(2000, 2500, 1000, NaN)

# Not a useful output...
c(mu_income=mean(income), mu_age=mean(age))

# error!
coef(lm(income ~ age))
@
\end{frame}

\begin{frame}[fragile]
Obvious errors corrupt your statistics:
<<>>=
age <- c(23, 36, 2000, 0)
income <- c(2000, 3000, -100000, 80000)

c(mu_income=mean(income), mu_age=mean(age))
c1 <- coef(lm(income ~ age))
c2 <- coef(lm(head(income,2) ~ head(age,2))) # remove last observations
rbind(c1,c2)
@
\end{frame}

\section{Towards consistency}
\begin{frame}
  \begin{enumerate}
    \item {\bf Detect} errors and special values
    \item {\bf Remove} or {\bf Correct} values.
    \item {\bf Impute} missing values
  \end{enumerate}
  
  \begin{block}{Note}
    \em All these actions have impact on your analysis. 
    They are data operations and should be scripted for reproducability.
    (Did I hear \R{}?)
  \end{block}
\end{frame}

\section{}
{\usebackgroundtemplate{
  \includegraphics[width=\paperwidth]{img/detection.jpg}
  }
  \setbeamertemplate{footline}{}
  \begin{frame}
  \begin{block}{Detect}<2->
    \begin{itemize}
      \item Special values
      \item Outliers
      \item (Multivariate) rule restrictions
    \end{itemize}
  \end{block}
  \end{frame}
}
% 

\section{Detection: special values}
\begin{frame}[fragile]
For detecting which rows contain \code{NA}, \code{complete.cases} is useful:
<<>>=
complete.cases(data.frame(x=c(1,NA,3), y=c(1,1,NA)))
@

or use \code{is.special} to check for all special values
<<tidy=FALSE>>=
is.special <- function(x){
  if (is.numeric(x)) !is.finite(x) else is.na(x)
}
is.special(factor(c(NA, "M","F")))
@
\end{frame}

\section{Detection: univariate outliers}
\begin{frame}[fragile]
\code{boxplot.stats} lists the outliers.
<<eval=FALSE>>=
boxplot.stats(x, coef = 1.5, do.conf = TRUE, do.out = TRUE)
@
<<>>=
(x <- sample(c(rnorm(10), 30)))
boxplot.stats(x)$out
@
\end{frame}
\begin{frame}[fragile]

<<echo=FALSE,fig.height=4.2>>=
boxplot(x)
@
\end{frame}

\begin{frame}[fragile]
  For skewed data, with $x^*$ (usually) the median of $x$
  \begin{equation*}
    h(x) = \max\left(
    \frac{x}{x^*},\frac{x^*}{x} 
    \right)
    \geq r,\textrm{ with } x>0.\footnote{Hiridoglou and Berthelot}
  \end{equation*}
  works better.
  In \R{}:
<<eval=FALSE>>=
hboutlier <- function(x,r){
  xref <- median(x, na.rm=TRUE)
  max(x/xref, xref/x, na.rm=TRUE) > r
}
@
\end{frame}

\section{}
{
  \usebackgroundtemplate{
    \hspace{2cm}
    \includegraphics[height=\paperheight]{img/correction.jpg}
  }
  \setbeamertemplate{footline}{}
  \begin{frame}
  \begin{block}{Correction}<2->
    \begin{itemize}
      \item Remove invalid values
      \item Using (correction) rules
    \end{itemize}
  \end{block}
  \end{frame}
}

\section{Correction: removal}
\begin{frame}[fragile]
  \begin{itemize}
  \item Replace invalid values with \code{NA}\\
  This value still needs to be fixed!
  \item Or remove the observation.\\
  This may leave you with very few data to analyze...
 \end{itemize}
\end{frame}


\section{}
{ \usebackgroundtemplate{
    \includegraphics[width=\paperwidth]{img/imputation.jpg}
  }
  \setbeamertemplate{footline}{}
  \begin{frame}
    \begin{block}{Imputation: Filling in the blanks}<2->
    Imputation fills in the gaps in your data, so that {\em all} of your
    observertions can be used for analysis. 
    Other options are:
    \begin{itemize}
      \item Remove all incomplete observations
      \item Remove \code{NA} per statistical operation.
    \end{itemize}
    \end{block}
  \end{frame}
}

\section{Imputation}
\begin{frame}
 There is a vast body of literature on imputation methods
and is goes beyond the scope of this tutorial.

% \code{Amelia}\cite{honaker:2011}       &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
% \code{BaBoon}\cite{meinfelder:2011}    &\xmark & \xmark &\xmark           &\xmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
% \code{cat}\cite{harding:2012}          &\xmark & \xmark &\xmark           &\cmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\  
% \code{deducorrect}\cite{loo:2011}      &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ \hline
% \code{e1071}\cite{meyer:2012}          &\cmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
% \code{ForImp}                          &\cmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\cmark$^\ddagger$ &\xmark &\xmark & \xmark \\ 
% \code{Hmisc}\cite{harrel:2013}         &\cmark & \xmark &\xmark           &\cmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
% \code{imputation}\cite{wong:2013}      &\cmark & \xmark &\cmark$^\dagger$ &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ \hline
% \code{impute}\cite{hastie}             &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
% \code{mi}\cite{su:2011}                &\xmark & \xmark &\cmark$^*$       &\cmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
% \code{mice}\cite{buren:2011}           &\cmark & \xmark &\cmark$^*$       &\cmark &\xmark &\cmark &\xmark &\xmark &\xmark & \xmark \\ 
% \code{mix}\cite{schafer:2010}          &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ \hline
% \code{norm}\cite{alvaro:2013}          &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
% \code{robCompositions}\cite{templ:2011}&\xmark & \xmark &\cmark$^*$       &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
% \code{rrcovNA}\cite{todorov:2012}      &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\xmark &\xmark & \xmark \\ 
% \code{StatMatch}\cite{orazio:2012}     &\xmark & \xmark &\xmark           &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ \hline
% \code{VIM}\cite{templ:2012}            &\xmark & \xmark &\cmark$^*$       &\cmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
% \code{yaImpute}\cite{crookston:2007}   &\xmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\cmark &\xmark &\xmark & \xmark \\ 
% \code{zoo}\cite{zeileis:2005}          &\cmark & \xmark &\xmark           &\xmark &\xmark &\xmark &\xmark &\cmark &\cmark & \xmark \\ 
% \hline                                                                                                             
% \end{tabular}
% {\scriptsize ${^*}$Methods are ultimately based on some form of regression, but are more involved than simple linear regression.\\
% ${^\dagger}$Uses a non-informative auxiliary variable (row number).\\
% ${^\ddagger}$Uses nearest neighbor as part of a more involved imputation scheme.
% }
% \end{threeparttable}
% \end{table} 
\end{frame}
  
\section{Sequential hot deck imputation}
\begin{frame}
\end{frame}

\section{Imputation:kNN}
\begin{frame}
\end{frame}


\section{Start analyzing!}
\begin{frame}
 After data cleaning, the statistical analyzing can begin. 
 However it is not uncommon that during analysis extra ordinary
 values are encountered. It may also be possible that the data
 cleaning process was too thorough. Nice thing about scripting this
 is that you can correct this and rerun the datacleaning process and analysis.
\end{frame}
\end{document}

